{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026c08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 21 pure strategies.\n",
      "After 50000 iterations:\n",
      "  strategyA = [0.0, 0.0, 0.124, 0.253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.297, 0.006, 0.0, 0.0, 0.0]\n",
      "  strategyB = [0.0, 0.0, 0.0, 0.382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515, 0.0, 0.0, 0.0, 0.0, 0.103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "After 100000 iterations:\n",
      "  strategyA = [0.0, 0.0, 0.009, 0.0, 0.0, 0.0, 0.0, 0.135, 0.0, 0.0, 0.0, 0.169, 0.0, 0.0, 0.0, 0.108, 0.326, 0.253, 0.0, 0.0, 0.0]\n",
      "  strategyB = [0.0, 0.0, 0.337, 0.182, 0.0, 0.0, 0.0, 0.199, 0.0, 0.236, 0.0, 0.0, 0.0, 0.0, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "After 150000 iterations:\n",
      "  strategyA = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.004, 0.0, 0.0, 0.187, 0.164, 0.361, 0.255, 0.0, 0.0, 0.0]\n",
      "  strategyB = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.229, 0.0, 0.402, 0.0, 0.0, 0.207, 0.162, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "After 200000 iterations:\n",
      "  strategyA = [0.0, 0.0, 0.0, 0.041, 0.0, 0.0, 0.0, 0.0, 0.0, 0.203, 0.0, 0.0, 0.0, 0.0, 0.256, 0.061, 0.203, 0.236, 0.0, 0.0, 0.0]\n",
      "  strategyB = [0.0, 0.0, 0.386, 0.0, 0.0, 0.0, 0.0, 0.516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.098, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "train finish\n",
      "the distribution of A in strategy ~ (prob>0.01)\n",
      "  ID=2, strat=(0, 2, 3), prob=0.112\n",
      "  ID=3, strat=(0, 3, 2), prob=0.103\n",
      "  ID=7, strat=(1, 1, 3), prob=0.110\n",
      "  ID=9, strat=(1, 3, 1), prob=0.114\n",
      "  ID=11, strat=(2, 0, 3), prob=0.108\n",
      "  ID=14, strat=(2, 3, 0), prob=0.110\n",
      "  ID=15, strat=(3, 0, 2), prob=0.122\n",
      "  ID=16, strat=(3, 1, 1), prob=0.105\n",
      "  ID=17, strat=(3, 2, 0), prob=0.115\n",
      "the distribution of B in strategy ~ (prob>0.01)\n",
      "  ID=2, strat=(0, 2, 3), prob=0.112\n",
      "  ID=3, strat=(0, 3, 2), prob=0.107\n",
      "  ID=7, strat=(1, 1, 3), prob=0.114\n",
      "  ID=9, strat=(1, 3, 1), prob=0.112\n",
      "  ID=11, strat=(2, 0, 3), prob=0.105\n",
      "  ID=14, strat=(2, 3, 0), prob=0.116\n",
      "  ID=15, strat=(3, 0, 2), prob=0.116\n",
      "  ID=16, strat=(3, 1, 1), prob=0.109\n",
      "  ID=17, strat=(3, 2, 0), prob=0.110\n",
      "\n",
      "Using 10000 random matches:\n",
      "  A total score：5032.0\n",
      "  B total score：4968.0\n",
      "  => A wins more often\n",
      "\n",
      "Now check if (finalA, finalB) is a near NE:\n",
      "\n",
      "=> (p,q) is a near Nash Equilibrium.\n",
      "Conclusion: This is an approximate NE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    get a list of all pure strategies (x1, x2, ..., xN)\n",
    "    with sum(xi)=S, xi>=0\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def back(nowSourceList, leftSource, slots):\n",
    "        if slots == 1:\n",
    "            results.append(tuple(nowSourceList + [leftSource]))\n",
    "            return\n",
    "        for i in range(leftSource+1):\n",
    "            back(nowSourceList + [i], leftSource - i, slots - 1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_A_vs_B(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    Colonel Blotto payoff: if a>b => (A+=1,B+=0);\n",
    "                           if a<b => (A+=0,B+=1);\n",
    "                           if a=b => each +=0.5.\n",
    "    \"\"\"\n",
    "    scoreA = 0\n",
    "    scoreB = 0\n",
    "    for (a, b) in zip(strategyA, strategyB):\n",
    "        if a > b:\n",
    "            scoreA += 1\n",
    "        elif a < b:\n",
    "            scoreB += 1\n",
    "        else:\n",
    "            scoreA += 0.5\n",
    "            scoreB += 0.5\n",
    "    return scoreA, scoreB\n",
    "\n",
    "def regret_matching_update(regretSum, strategies, chosenIndex, oppStrategy):\n",
    "    \"\"\"\n",
    "    External regret update for time t:\n",
    "      chosenIndex => the strategy we actually used\n",
    "      oppStrategy => the opponent's actual strategy (tuple).\n",
    "    \"\"\"\n",
    "    chosenStrat = strategies[chosenIndex]\n",
    "    scoreChosen, _ = payoff_A_vs_B(chosenStrat, oppStrategy)\n",
    "\n",
    "    for i, stratSprime in enumerate(strategies):\n",
    "        score_sprime, _ = payoff_A_vs_B(stratSprime, oppStrategy)\n",
    "        diff = score_sprime - scoreChosen\n",
    "        regretSum[i] += diff\n",
    "    return regretSum\n",
    "\n",
    "def get_mixed_strategy_from_regret(regretSum):\n",
    "    \"\"\"\n",
    "    p[i] = max(0, regret[i]) / sum_of_positives\n",
    "    if all non-positive => uniform.\n",
    "    \"\"\"\n",
    "    positives = [max(0.0, r) for r in regretSum]\n",
    "    totalPos  = sum(positives)\n",
    "    n = len(regretSum)\n",
    "    if totalPos>1e-15:\n",
    "        return [p/totalPos for p in positives]\n",
    "    else:\n",
    "        return [1.0/n]*n\n",
    "\n",
    "def sample_from_distribution(dist):\n",
    "    r = random.random()\n",
    "    cum = 0.0\n",
    "    for i, p in enumerate(dist):\n",
    "        cum += p\n",
    "        if r<cum:\n",
    "            return i\n",
    "    return len(dist)-1  # fallback\n",
    "\n",
    "# ============ NE 检查需要: payoff 矩阵 & 检测函数 ============\n",
    "\n",
    "def build_payoff_matrices(strategies):\n",
    "    \"\"\"\n",
    "    A[i][j] = payoff of row player(A) if A= i, B= j\n",
    "    B[i][j] = payoff of col player(B) if A= i, B= j\n",
    "    \"\"\"\n",
    "    n = len(strategies)\n",
    "    A = [[0.0]*n for _ in range(n)]\n",
    "    B = [[0.0]*n for _ in range(n)]\n",
    "    for i, sa in enumerate(strategies):\n",
    "        for j, sb in enumerate(strategies):\n",
    "            scoreA, scoreB = payoff_A_vs_B(sa, sb)\n",
    "            A[i][j] = scoreA\n",
    "            B[i][j] = scoreB\n",
    "    return A, B\n",
    "\n",
    "def check_NE(A, B, p, q, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    检查( p, q )是否为(近似)混合纳什均衡：\n",
    "    对 A: U_A(p, q) >= U_A(i, q), 对所有纯策略 i\n",
    "          并对于 i in support(p), U_A(i,q) ~ U_A(p,q)\n",
    "    对 B: 同理\n",
    "    A[i][j], B[i][j]: payoff matrices, shape n x n\n",
    "    p, q: length n\n",
    "    \"\"\"\n",
    "    n = len(A)\n",
    "    # 先算 UA(p,q), UB(p,q)\n",
    "    UA_pq = 0.0\n",
    "    UB_pq = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            UA_pq += p[i]*q[j]*A[i][j]\n",
    "            UB_pq += p[i]*q[j]*B[i][j]\n",
    "\n",
    "    # 对A: check pure strategy i\n",
    "    for i in range(n):\n",
    "        # U_A(i,q) = sum_j q[j]*A[i][j]\n",
    "        U_iq = sum(q[j]*A[i][j] for j in range(n))\n",
    "        if U_iq>UA_pq+epsilon:\n",
    "            print(f\"[A can deviate]: pure {i} => payoff={U_iq:.3f} > UA_pq={UA_pq:.3f}\")\n",
    "            return False\n",
    "        # 若 p[i]>0 => check ~equal\n",
    "        if p[i]>epsilon:\n",
    "            if abs(U_iq - UA_pq)>5e-3:\n",
    "                print(f\"[A support mismatch]: i={i}, payoff(i,q)={U_iq:.3f}, UA_pq={UA_pq:.3f}\")\n",
    "                return False\n",
    "\n",
    "    # 对B\n",
    "    for j in range(n):\n",
    "        # U_B(p,j) = sum_i p[i]*B[i][j]\n",
    "        U_pj = sum(p[i]*B[i][j] for i in range(n))\n",
    "        if U_pj>UB_pq+epsilon:\n",
    "            print(f\"[B can deviate]: pure {j} => payoff={U_pj:.3f} > UB_pq={UB_pq:.3f}\")\n",
    "            return False\n",
    "        if q[j]>epsilon:\n",
    "            if abs(U_pj - UB_pq)>5e-3:\n",
    "                print(f\"[B support mismatch]: j={j}, payoff(p,j)={U_pj:.3f}, UB_pq={UB_pq:.3f}\")\n",
    "                return False\n",
    "\n",
    "    # 如果都没找到可偏离 => 近似NE\n",
    "    print(\"=> (p,q) is a near Nash Equilibrium.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# ===============  主函数  =================\n",
    "\n",
    "def main():\n",
    "    random.seed()\n",
    "    S = 5\n",
    "    N = 3\n",
    "    num_iterations = 200000\n",
    "\n",
    "    # 1) 枚举纯策略\n",
    "    allStrategies = get_all_strategies(S, N)\n",
    "    numStrats = len(allStrategies)\n",
    "    print(f\"we have {numStrats} pure strategies.\")\n",
    "    #for idx, st in enumerate(allStrategies):\n",
    "    #    print(f\"  ID={idx:2d} -> {st}\")\n",
    "\n",
    "    # 2) 初始化\n",
    "    regretSumA = [0.0]*numStrats\n",
    "    regretSumB = [0.0]*numStrats\n",
    "    strategyA  = [1.0/numStrats]*numStrats\n",
    "    strategyB  = [1.0/numStrats]*numStrats\n",
    "\n",
    "    # 用于时间平均分布\n",
    "    strategyCountA = [0.0]*numStrats\n",
    "    strategyCountB = [0.0]*numStrats\n",
    "\n",
    "    # 3) 迭代: 后悔匹配\n",
    "    for t in range(1, num_iterations+1):\n",
    "        # a) 抽样得到纯策略 iA, iB\n",
    "        iA = sample_from_distribution(strategyA)\n",
    "        iB = sample_from_distribution(strategyB)\n",
    "        stratA = allStrategies[iA]\n",
    "        stratB = allStrategies[iB]\n",
    "\n",
    "        # b) 更新后悔\n",
    "        regretSumA = regret_matching_update(regretSumA, allStrategies, iA, stratB)\n",
    "        regretSumB = regret_matching_update(regretSumB, allStrategies, iB, stratA)\n",
    "\n",
    "        # c) 得到新的混合策略\n",
    "        strategyA = get_mixed_strategy_from_regret(regretSumA)\n",
    "        strategyB = get_mixed_strategy_from_regret(regretSumB)\n",
    "\n",
    "        # d) accumulate time-average\n",
    "        for i in range(numStrats):\n",
    "            strategyCountA[i] += strategyA[i]\n",
    "            strategyCountB[i] += strategyB[i]\n",
    "\n",
    "        # 观察中间\n",
    "        if t%50000==0:\n",
    "            print(f\"After {t} iterations:\")\n",
    "            print(\"  strategyA =\", [round(x,3) for x in strategyA])\n",
    "            print(\"  strategyB =\", [round(x,3) for x in strategyB])\n",
    "            print()\n",
    "\n",
    "    # 4) 得到“时间平均”混合策略\n",
    "    finalA = [x/num_iterations for x in strategyCountA]\n",
    "    finalB = [x/num_iterations for x in strategyCountB]\n",
    "\n",
    "    print(\"\\ntrain finish\")\n",
    "    print(\"the distribution of A in strategy ~ (prob>0.01)\")\n",
    "    for i, p in enumerate(finalA):\n",
    "        if p>0.01:\n",
    "            print(f\"  ID={i}, strat={allStrategies[i]}, prob={p:.3f}\")\n",
    "    print(\"the distribution of B in strategy ~ (prob>0.01)\")\n",
    "    for i, p in enumerate(finalB):\n",
    "        if p>0.01:\n",
    "            print(f\"  ID={i}, strat={allStrategies[i]}, prob={p:.3f}\")\n",
    "\n",
    "    # 5) 做一次实验对战\n",
    "    test_rounds = 10000\n",
    "    winsA = 0.0\n",
    "    winsB = 0.0\n",
    "    for _ in range(test_rounds):\n",
    "        iA = sample_from_distribution(finalA)\n",
    "        iB = sample_from_distribution(finalB)\n",
    "        (a_score, b_score) = payoff_A_vs_B(allStrategies[iA], allStrategies[iB])\n",
    "        if a_score > b_score:\n",
    "            winsA += 1\n",
    "        elif a_score < b_score:\n",
    "            winsB += 1\n",
    "        else:\n",
    "            winsA += 0.5\n",
    "            winsB += 0.5\n",
    "\n",
    "    print(f\"\\nUsing {test_rounds} random matches:\")\n",
    "    print(f\"  A total score：{winsA:.1f}\")\n",
    "    print(f\"  B total score：{winsB:.1f}\")\n",
    "    if abs(winsA - winsB) < 1e-5:\n",
    "        print(\"  => tie!\")\n",
    "    elif winsA > winsB:\n",
    "        print(\"  => A wins more often\")\n",
    "    else:\n",
    "        print(\"  => B wins more often\")\n",
    "\n",
    "    # ============ NE 检查部分 ===================\n",
    "    # 首先构造 payoff 矩阵\n",
    "    A_mat, B_mat = build_payoff_matrices(allStrategies)\n",
    "    # 然后对 (finalA, finalB) 做 NE 检测\n",
    "    print(\"\\nNow check if (finalA, finalB) is a near NE:\\n\")\n",
    "    is_NE = check_NE(A_mat, B_mat, finalA, finalB, epsilon=1e-1)\n",
    "    if not is_NE:\n",
    "        print(\"Conclusion: Not NE (or fails check_NE).\")\n",
    "    else:\n",
    "        print(\"Conclusion: This is an approximate NE.\\n\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7de1f25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 21 pure strategies for S=5,N=3, in zero-sum version(tie=0).\n",
      "\n",
      "--- Final average regrets (last few) ---\n",
      "  t=1995, avg_regret_A=0.032080\n",
      "  t=1996, avg_regret_A=0.032565\n",
      "  t=1997, avg_regret_A=0.032549\n",
      "  t=1998, avg_regret_A=0.032533\n",
      "  t=1999, avg_regret_A=0.033017\n",
      "  t=2000, avg_regret_A=0.033000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check if empirical jointDist is CE:\n",
      "[A violates CE]: i=2, i'=3, diff=0.118\n",
      "Conclusion: Not CE.\n",
      "\n",
      "Some (i,j) with prob>0.01 in jointDist:\n",
      "   (i=2, j=9), prob=0.013, (A strat=(0, 2, 3), B strat=(1, 3, 1))\n",
      "   (i=2, j=11), prob=0.015, (A strat=(0, 2, 3), B strat=(2, 0, 3))\n",
      "   (i=2, j=14), prob=0.021, (A strat=(0, 2, 3), B strat=(2, 3, 0))\n",
      "   (i=2, j=15), prob=0.018, (A strat=(0, 2, 3), B strat=(3, 0, 2))\n",
      "   (i=2, j=16), prob=0.024, (A strat=(0, 2, 3), B strat=(3, 1, 1))\n",
      "   (i=2, j=17), prob=0.019, (A strat=(0, 2, 3), B strat=(3, 2, 0))\n",
      "   (i=3, j=2), prob=0.013, (A strat=(0, 3, 2), B strat=(0, 2, 3))\n",
      "   (i=3, j=3), prob=0.017, (A strat=(0, 3, 2), B strat=(0, 3, 2))\n",
      "   (i=3, j=9), prob=0.022, (A strat=(0, 3, 2), B strat=(1, 3, 1))\n",
      "   (i=3, j=14), prob=0.014, (A strat=(0, 3, 2), B strat=(2, 3, 0))\n",
      "   (i=7, j=11), prob=0.021, (A strat=(1, 1, 3), B strat=(2, 0, 3))\n",
      "   (i=7, j=14), prob=0.013, (A strat=(1, 1, 3), B strat=(2, 3, 0))\n",
      "   (i=7, j=15), prob=0.021, (A strat=(1, 1, 3), B strat=(3, 0, 2))\n",
      "   (i=7, j=16), prob=0.015, (A strat=(1, 1, 3), B strat=(3, 1, 1))\n",
      "   (i=9, j=2), prob=0.018, (A strat=(1, 3, 1), B strat=(0, 2, 3))\n",
      "   (i=9, j=3), prob=0.023, (A strat=(1, 3, 1), B strat=(0, 3, 2))\n",
      "   (i=9, j=7), prob=0.017, (A strat=(1, 3, 1), B strat=(1, 1, 3))\n",
      "   (i=9, j=15), prob=0.011, (A strat=(1, 3, 1), B strat=(3, 0, 2))\n",
      "   (i=11, j=2), prob=0.015, (A strat=(2, 0, 3), B strat=(0, 2, 3))\n",
      "   (i=11, j=3), prob=0.011, (A strat=(2, 0, 3), B strat=(0, 3, 2))\n",
      "   (i=11, j=11), prob=0.018, (A strat=(2, 0, 3), B strat=(2, 0, 3))\n",
      "   (i=11, j=15), prob=0.019, (A strat=(2, 0, 3), B strat=(3, 0, 2))\n",
      "   (i=14, j=2), prob=0.018, (A strat=(2, 3, 0), B strat=(0, 2, 3))\n",
      "   (i=14, j=3), prob=0.021, (A strat=(2, 3, 0), B strat=(0, 3, 2))\n",
      "   (i=14, j=7), prob=0.016, (A strat=(2, 3, 0), B strat=(1, 1, 3))\n",
      "   (i=14, j=11), prob=0.018, (A strat=(2, 3, 0), B strat=(2, 0, 3))\n",
      "   (i=14, j=15), prob=0.024, (A strat=(2, 3, 0), B strat=(3, 0, 2))\n",
      "   (i=14, j=16), prob=0.011, (A strat=(2, 3, 0), B strat=(3, 1, 1))\n",
      "   (i=14, j=17), prob=0.011, (A strat=(2, 3, 0), B strat=(3, 2, 0))\n",
      "   (i=15, j=2), prob=0.029, (A strat=(3, 0, 2), B strat=(0, 2, 3))\n",
      "   (i=15, j=3), prob=0.021, (A strat=(3, 0, 2), B strat=(0, 3, 2))\n",
      "   (i=15, j=7), prob=0.013, (A strat=(3, 0, 2), B strat=(1, 1, 3))\n",
      "   (i=15, j=9), prob=0.017, (A strat=(3, 0, 2), B strat=(1, 3, 1))\n",
      "   (i=15, j=14), prob=0.017, (A strat=(3, 0, 2), B strat=(2, 3, 0))\n",
      "   (i=15, j=17), prob=0.016, (A strat=(3, 0, 2), B strat=(3, 2, 0))\n",
      "   (i=16, j=2), prob=0.024, (A strat=(3, 1, 1), B strat=(0, 2, 3))\n",
      "   (i=16, j=9), prob=0.017, (A strat=(3, 1, 1), B strat=(1, 3, 1))\n",
      "   (i=16, j=14), prob=0.019, (A strat=(3, 1, 1), B strat=(2, 3, 0))\n",
      "   (i=16, j=17), prob=0.018, (A strat=(3, 1, 1), B strat=(3, 2, 0))\n",
      "   (i=17, j=2), prob=0.013, (A strat=(3, 2, 0), B strat=(0, 2, 3))\n",
      "   (i=17, j=9), prob=0.011, (A strat=(3, 2, 0), B strat=(1, 3, 1))\n",
      "   (i=17, j=14), prob=0.014, (A strat=(3, 2, 0), B strat=(2, 3, 0))\n",
      "   (i=17, j=15), prob=0.012, (A strat=(3, 2, 0), B strat=(3, 0, 2))\n",
      "   (i=17, j=17), prob=0.019, (A strat=(3, 2, 0), B strat=(3, 2, 0))\n"
     ]
    }
   ],
   "source": [
    "#检查CE和后悔值\n",
    "import random\n",
    "import math\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    返回所有将 S 个士兵分配到 N 个战场的纯策略列表。\n",
    "    (x1, x2, ..., xN) with sum(xi)=S, xi>=0.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def back(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            back(now+[i], left-i, slots-1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_A_vs_B_zero_sum(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    零和规则: \n",
    "      if a>b => (A=1,B=-1)\n",
    "      if a<b => (A=-1,B=1)\n",
    "      if a=b => (A=0,B=0)\n",
    "    累加 3 个战场 => (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA = 0\n",
    "    scoreB = 0\n",
    "    for (a, b) in zip(strategyA, strategyB):\n",
    "        if a > b:\n",
    "            scoreA += 1\n",
    "            scoreB -= 1\n",
    "        elif a < b:\n",
    "            scoreA -= 1\n",
    "            scoreB += 1\n",
    "        else:\n",
    "            pass\n",
    "    return scoreA, scoreB\n",
    "\n",
    "def regret_matching_update(regretSum, strategies, chosenIndex, oppStrategy):\n",
    "    \"\"\"\n",
    "    对行玩家(A)或列玩家(B)来说: \n",
    "    chosenIndex => 当前用的纯策略\n",
    "    oppStrategy => 对手用的具体纯策略(元组)\n",
    "    累加 external regret: for each i,\n",
    "      regretSum[i] += (score_i - score_chosen).\n",
    "    \"\"\"\n",
    "    chosenStrat = strategies[chosenIndex]\n",
    "    scoreChosen, _ = payoff_A_vs_B_zero_sum(chosenStrat, oppStrategy)\n",
    "    for i, s_i in enumerate(strategies):\n",
    "        score_i, _ = payoff_A_vs_B_zero_sum(s_i, oppStrategy)\n",
    "        diff = score_i - scoreChosen\n",
    "        regretSum[i] += diff\n",
    "    return regretSum\n",
    "\n",
    "def get_mixed_strategy_from_regret(regretSum):\n",
    "    \"\"\"\n",
    "    p[i] = max(0, regretSum[i])/ sum_of_positives\n",
    "    若 sum_of_positives=0 => 均匀分布\n",
    "    \"\"\"\n",
    "    positives = [max(r,0) for r in regretSum]\n",
    "    totalPos = sum(positives)\n",
    "    n = len(regretSum)\n",
    "    if totalPos>1e-15:\n",
    "        return [p/totalPos for p in positives]\n",
    "    else:\n",
    "        return [1.0/n]*n\n",
    "\n",
    "def sample_from_distribution(dist):\n",
    "    r = random.random()\n",
    "    cum = 0.0\n",
    "    for i, p in enumerate(dist):\n",
    "        cum += p\n",
    "        if r<cum:\n",
    "            return i\n",
    "    return len(dist)-1\n",
    "\n",
    "# ===========  构造 payoff 矩阵 for CE check  ===========\n",
    "\n",
    "def build_payoff_matrices(strategies):\n",
    "    \"\"\"\n",
    "    A[i][j] = 当 A 用 i, B 用 j 时, A的收益\n",
    "    B[i][j] = 当 A 用 i, B 用 j 时, B的收益 = -A[i][j], 因为零和\n",
    "    \"\"\"\n",
    "    n = len(strategies)\n",
    "    A = [[0]*n for _ in range(n)]\n",
    "    B = [[0]*n for _ in range(n)]\n",
    "    for i, sa in enumerate(strategies):\n",
    "        for j, sb in enumerate(strategies):\n",
    "            scoreA, scoreB = payoff_A_vs_B_zero_sum(sa, sb)\n",
    "            A[i][j] = scoreA\n",
    "            B[i][j] = scoreB\n",
    "    return A,B\n",
    "\n",
    "# ===========  CE 检查 ===========\n",
    "\n",
    "def check_CE(A, B, jointDist, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    检查给定(A,B)收益矩阵, jointDist(联合分布) 是否近似CE.\n",
    "    A,B形状均 n x n, jointDist同理, sum=1.\n",
    "    \"\"\"\n",
    "    n = len(A)\n",
    "    # 1) 行玩家(A)条件收益\n",
    "    for i in range(n):\n",
    "        mu_i = sum(jointDist[i][j] for j in range(n))\n",
    "        if mu_i<epsilon: \n",
    "            continue\n",
    "        old_payoff = 0.0\n",
    "        for j in range(n):\n",
    "            old_payoff += jointDist[i][j]*A[i][j]\n",
    "        old_payoff /= mu_i\n",
    "        for i_prime in range(n):\n",
    "            if i_prime==i: \n",
    "                continue\n",
    "            new_payoff=0.0\n",
    "            for j in range(n):\n",
    "                new_payoff += jointDist[i][j]*A[i_prime][j]\n",
    "            new_payoff/=mu_i\n",
    "            if new_payoff>old_payoff+epsilon:\n",
    "                print(f\"[A violates CE]: i={i}, i'={i_prime}, diff={new_payoff-old_payoff:.3f}\")\n",
    "                return False\n",
    "    # 2) 列玩家(B)条件收益\n",
    "    for j in range(n):\n",
    "        mu_j = sum(jointDist[i][j] for i in range(n))\n",
    "        if mu_j<epsilon:\n",
    "            continue\n",
    "        old_payoff = 0.0\n",
    "        for i in range(n):\n",
    "            old_payoff += jointDist[i][j]*B[i][j]\n",
    "        old_payoff/= mu_j\n",
    "        for j_prime in range(n):\n",
    "            if j_prime==j: \n",
    "                continue\n",
    "            new_payoff=0.0\n",
    "            for i in range(n):\n",
    "                new_payoff += jointDist[i][j]*B[i][j_prime]\n",
    "            new_payoff/=mu_j\n",
    "            if new_payoff>old_payoff+epsilon:\n",
    "                print(f\"[B violates CE]: j={j}, j'={j_prime}, diff={new_payoff-old_payoff:.3f}\")\n",
    "                return False\n",
    "    print(\"=> jointDist satisfies (approx) CE condition.\")\n",
    "    return True\n",
    "\n",
    "# ===========  主函数: 无悔学习+收敛速度表  ===========\n",
    "\n",
    "def main():\n",
    "    random.seed(1)\n",
    "    S=5\n",
    "    N=3\n",
    "    allStrats = get_all_strategies(S,N)    # 生成纯策略\n",
    "    m = len(allStrats)                    # ~21\n",
    "    print(f\"We have {m} pure strategies for S={S},N={N}, in zero-sum version(tie=0).\")\n",
    "\n",
    "    # payoff 矩阵\n",
    "    A_mat, B_mat = build_payoff_matrices(allStrats)\n",
    "    # B_mat 应该= -A_mat, 在 build时自动算了 (scoreB= -scoreA or 0)\n",
    "    # 这里是tie=0, a>b =>(1,-1), a<b =>(-1,1).\n",
    "\n",
    "    # 后悔值\n",
    "    regretSumA = [0.0]*m\n",
    "    regretSumB = [0.0]*m\n",
    "    strategyA = [1.0/m]*m\n",
    "    strategyB = [1.0/m]*m\n",
    "\n",
    "    # 记录 (iA,iB) 出现频率 => 检查CE\n",
    "    jointCount = [[0]*m for _ in range(m)]\n",
    "\n",
    "    num_iters = 2000\n",
    "    # 记录收敛速度: average regret(对A)\n",
    "    regret_list = []\n",
    "\n",
    "    for t in range(1, num_iters+1):\n",
    "        # 1) 从当前混合策略采样\n",
    "        iA = sample_from_distribution(strategyA)\n",
    "        iB = sample_from_distribution(strategyB)\n",
    "        # 2) 更新后悔\n",
    "        stratA = allStrats[iA]\n",
    "        stratB = allStrats[iB]\n",
    "        regretSumA = regret_matching_update(regretSumA, allStrats, iA, stratB)\n",
    "        regretSumB = regret_matching_update(regretSumB, allStrats, iB, stratA)\n",
    "        # 3) 得到新的混合\n",
    "        strategyA = get_mixed_strategy_from_regret(regretSumA)\n",
    "        strategyB = get_mixed_strategy_from_regret(regretSumB)\n",
    "        # 4) jointCount\n",
    "        jointCount[iA][iB]+=1\n",
    "\n",
    "        # === 计算 \"平均外部后悔\" for A ===\n",
    "        # external regret for A:  max_{i'} regretSumA[i'] 的正部分\n",
    "        max_regret_A = max(regretSumA)  # 可能有负值\n",
    "        if max_regret_A < 0:\n",
    "            max_regret_A = 0\n",
    "        avg_regret_A = max_regret_A / t\n",
    "        regret_list.append(avg_regret_A)\n",
    "\n",
    "        # 可以每隔一定步数打印一次\n",
    "        if t%5000000==0:\n",
    "            print(f\"Iter={t}, avg_regret_A={avg_regret_A:.5f}\")\n",
    "\n",
    "    # 最终 jointDist\n",
    "    jointDist = [[ jointCount[i][j]/num_iters for j in range(m)] for i in range(m)]\n",
    "\n",
    "    # 打印收敛速度表(或画图)\n",
    "    print(\"\\n--- Final average regrets (last few) ---\")\n",
    "    for step in [num_iters-5, num_iters-4, num_iters-3, num_iters-2, num_iters-1, num_iters]:\n",
    "        print(f\"  t={step}, avg_regret_A={regret_list[step-1]:.6f}\")\n",
    "\n",
    "    # 如果想画图，可加 matplotlib:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(1,num_iters+1), regret_list, label='Average regret(A)')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Average regret(A)')\n",
    "    plt.title('Regret Matching Convergence')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # ========== 最后做 CE 检验 ==========\n",
    "    print(\"\\nCheck if empirical jointDist is CE:\")\n",
    "    is_ce = check_CE(A_mat,B_mat, jointDist, epsilon=1e-2)\n",
    "    if is_ce:\n",
    "        print(\"Conclusion: It's an approximate CE.\\n\")\n",
    "    else:\n",
    "        print(\"Conclusion: Not CE.\\n\")\n",
    "\n",
    "    # 展示几项 jointDist>0.01\n",
    "    print(\"Some (i,j) with prob>0.01 in jointDist:\")\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if jointDist[i][j]>0.01:\n",
    "                print(f\"   (i={i}, j={j}), prob={jointDist[i][j]:.3f}, (A strat={allStrats[i]}, B strat={allStrats[j]})\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b91bf725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 21 pure strategies for S=5,N=3, in zero-sum version(tie=0).\n",
      "Iter=50000, avg_regret_A=0.00318\n",
      "Iter=100000, avg_regret_A=0.00246\n",
      "Iter=150000, avg_regret_A=0.00195\n",
      "Iter=200000, avg_regret_A=0.00208\n",
      "Iter=250000, avg_regret_A=0.00138\n",
      "Iter=300000, avg_regret_A=0.00082\n",
      "\n",
      "--- Final average regrets (last few) ---\n",
      "  t=299995, avg_regret_A=0.000830\n",
      "  t=299996, avg_regret_A=0.000827\n",
      "  t=299997, avg_regret_A=0.000823\n",
      "  t=299998, avg_regret_A=0.000827\n",
      "  t=299999, avg_regret_A=0.000827\n",
      "  t=300000, avg_regret_A=0.000823\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check if empirical jointDist is CE:\n",
      "[A violates CE]: i=2, i'=7, diff=0.031\n",
      "Conclusion: Not CE.\n",
      "\n",
      "Some (i,j) with prob>0.01 in jointDist:\n",
      "   (i=2, j=2), prob=0.012, (A strat=(0, 2, 3), B strat=(0, 2, 3))\n",
      "   (i=2, j=3), prob=0.013, (A strat=(0, 2, 3), B strat=(0, 3, 2))\n",
      "   (i=2, j=7), prob=0.017, (A strat=(0, 2, 3), B strat=(1, 1, 3))\n",
      "   (i=2, j=9), prob=0.012, (A strat=(0, 2, 3), B strat=(1, 3, 1))\n",
      "   (i=2, j=11), prob=0.015, (A strat=(0, 2, 3), B strat=(2, 0, 3))\n",
      "   (i=2, j=14), prob=0.012, (A strat=(0, 2, 3), B strat=(2, 3, 0))\n",
      "   (i=2, j=15), prob=0.014, (A strat=(0, 2, 3), B strat=(3, 0, 2))\n",
      "   (i=2, j=16), prob=0.011, (A strat=(0, 2, 3), B strat=(3, 1, 1))\n",
      "   (i=3, j=2), prob=0.013, (A strat=(0, 3, 2), B strat=(0, 2, 3))\n",
      "   (i=3, j=3), prob=0.013, (A strat=(0, 3, 2), B strat=(0, 3, 2))\n",
      "   (i=3, j=7), prob=0.015, (A strat=(0, 3, 2), B strat=(1, 1, 3))\n",
      "   (i=3, j=11), prob=0.012, (A strat=(0, 3, 2), B strat=(2, 0, 3))\n",
      "   (i=3, j=15), prob=0.015, (A strat=(0, 3, 2), B strat=(3, 0, 2))\n",
      "   (i=3, j=16), prob=0.016, (A strat=(0, 3, 2), B strat=(3, 1, 1))\n",
      "   (i=3, j=17), prob=0.011, (A strat=(0, 3, 2), B strat=(3, 2, 0))\n",
      "   (i=7, j=2), prob=0.015, (A strat=(1, 1, 3), B strat=(0, 2, 3))\n",
      "   (i=7, j=3), prob=0.013, (A strat=(1, 1, 3), B strat=(0, 3, 2))\n",
      "   (i=7, j=7), prob=0.017, (A strat=(1, 1, 3), B strat=(1, 1, 3))\n",
      "   (i=7, j=9), prob=0.013, (A strat=(1, 1, 3), B strat=(1, 3, 1))\n",
      "   (i=7, j=11), prob=0.011, (A strat=(1, 1, 3), B strat=(2, 0, 3))\n",
      "   (i=7, j=14), prob=0.011, (A strat=(1, 1, 3), B strat=(2, 3, 0))\n",
      "   (i=7, j=17), prob=0.010, (A strat=(1, 1, 3), B strat=(3, 2, 0))\n",
      "   (i=9, j=2), prob=0.011, (A strat=(1, 3, 1), B strat=(0, 2, 3))\n",
      "   (i=9, j=7), prob=0.013, (A strat=(1, 3, 1), B strat=(1, 1, 3))\n",
      "   (i=9, j=11), prob=0.012, (A strat=(1, 3, 1), B strat=(2, 0, 3))\n",
      "   (i=9, j=14), prob=0.011, (A strat=(1, 3, 1), B strat=(2, 3, 0))\n",
      "   (i=9, j=15), prob=0.013, (A strat=(1, 3, 1), B strat=(3, 0, 2))\n",
      "   (i=9, j=16), prob=0.015, (A strat=(1, 3, 1), B strat=(3, 1, 1))\n",
      "   (i=9, j=17), prob=0.013, (A strat=(1, 3, 1), B strat=(3, 2, 0))\n",
      "   (i=11, j=2), prob=0.015, (A strat=(2, 0, 3), B strat=(0, 2, 3))\n",
      "   (i=11, j=3), prob=0.013, (A strat=(2, 0, 3), B strat=(0, 3, 2))\n",
      "   (i=11, j=7), prob=0.013, (A strat=(2, 0, 3), B strat=(1, 1, 3))\n",
      "   (i=11, j=9), prob=0.013, (A strat=(2, 0, 3), B strat=(1, 3, 1))\n",
      "   (i=11, j=14), prob=0.012, (A strat=(2, 0, 3), B strat=(2, 3, 0))\n",
      "   (i=11, j=16), prob=0.014, (A strat=(2, 0, 3), B strat=(3, 1, 1))\n",
      "   (i=11, j=17), prob=0.016, (A strat=(2, 0, 3), B strat=(3, 2, 0))\n",
      "   (i=14, j=2), prob=0.011, (A strat=(2, 3, 0), B strat=(0, 2, 3))\n",
      "   (i=14, j=7), prob=0.013, (A strat=(2, 3, 0), B strat=(1, 1, 3))\n",
      "   (i=14, j=9), prob=0.015, (A strat=(2, 3, 0), B strat=(1, 3, 1))\n",
      "   (i=14, j=11), prob=0.012, (A strat=(2, 3, 0), B strat=(2, 0, 3))\n",
      "   (i=14, j=14), prob=0.019, (A strat=(2, 3, 0), B strat=(2, 3, 0))\n",
      "   (i=14, j=16), prob=0.013, (A strat=(2, 3, 0), B strat=(3, 1, 1))\n",
      "   (i=14, j=17), prob=0.016, (A strat=(2, 3, 0), B strat=(3, 2, 0))\n",
      "   (i=15, j=2), prob=0.013, (A strat=(3, 0, 2), B strat=(0, 2, 3))\n",
      "   (i=15, j=3), prob=0.016, (A strat=(3, 0, 2), B strat=(0, 3, 2))\n",
      "   (i=15, j=9), prob=0.012, (A strat=(3, 0, 2), B strat=(1, 3, 1))\n",
      "   (i=15, j=15), prob=0.014, (A strat=(3, 0, 2), B strat=(3, 0, 2))\n",
      "   (i=15, j=16), prob=0.015, (A strat=(3, 0, 2), B strat=(3, 1, 1))\n",
      "   (i=15, j=17), prob=0.012, (A strat=(3, 0, 2), B strat=(3, 2, 0))\n",
      "   (i=16, j=3), prob=0.015, (A strat=(3, 1, 1), B strat=(0, 3, 2))\n",
      "   (i=16, j=7), prob=0.010, (A strat=(3, 1, 1), B strat=(1, 1, 3))\n",
      "   (i=16, j=9), prob=0.017, (A strat=(3, 1, 1), B strat=(1, 3, 1))\n",
      "   (i=16, j=11), prob=0.011, (A strat=(3, 1, 1), B strat=(2, 0, 3))\n",
      "   (i=16, j=14), prob=0.014, (A strat=(3, 1, 1), B strat=(2, 3, 0))\n",
      "   (i=16, j=15), prob=0.015, (A strat=(3, 1, 1), B strat=(3, 0, 2))\n",
      "   (i=16, j=16), prob=0.014, (A strat=(3, 1, 1), B strat=(3, 1, 1))\n",
      "   (i=16, j=17), prob=0.011, (A strat=(3, 1, 1), B strat=(3, 2, 0))\n",
      "   (i=17, j=3), prob=0.010, (A strat=(3, 2, 0), B strat=(0, 3, 2))\n",
      "   (i=17, j=7), prob=0.012, (A strat=(3, 2, 0), B strat=(1, 1, 3))\n",
      "   (i=17, j=9), prob=0.014, (A strat=(3, 2, 0), B strat=(1, 3, 1))\n",
      "   (i=17, j=11), prob=0.014, (A strat=(3, 2, 0), B strat=(2, 0, 3))\n",
      "   (i=17, j=14), prob=0.015, (A strat=(3, 2, 0), B strat=(2, 3, 0))\n",
      "   (i=17, j=15), prob=0.012, (A strat=(3, 2, 0), B strat=(3, 0, 2))\n"
     ]
    }
   ],
   "source": [
    "#检查CE和后悔值\n",
    "import random\n",
    "import math\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    返回所有将 S 个士兵分配到 N 个战场的纯策略列表。\n",
    "    (x1, x2, ..., xN) with sum(xi)=S, xi>=0.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def back(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            back(now+[i], left-i, slots-1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_A_vs_B_zero_sum(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    零和规则: \n",
    "      if a>b => (A=1,B=-1)\n",
    "      if a<b => (A=-1,B=1)\n",
    "      if a=b => (A=0,B=0)\n",
    "    累加 3 个战场 => (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA = 0\n",
    "    scoreB = 0\n",
    "    for (a, b) in zip(strategyA, strategyB):\n",
    "        if a > b:\n",
    "            scoreA += 1\n",
    "            scoreB -= 1\n",
    "        elif a < b:\n",
    "            scoreA -= 1\n",
    "            scoreB += 1\n",
    "        else:\n",
    "            pass\n",
    "    return scoreA, scoreB\n",
    "\n",
    "def regret_matching_update(regretSum, strategies, chosenIndex, oppStrategy):\n",
    "    \"\"\"\n",
    "    对行玩家(A)或列玩家(B)来说: \n",
    "    chosenIndex => 当前用的纯策略\n",
    "    oppStrategy => 对手用的具体纯策略(元组)\n",
    "    累加 external regret: for each i,\n",
    "      regretSum[i] += (score_i - score_chosen).\n",
    "    \"\"\"\n",
    "    chosenStrat = strategies[chosenIndex]\n",
    "    scoreChosen, _ = payoff_A_vs_B_zero_sum(chosenStrat, oppStrategy)\n",
    "    for i, s_i in enumerate(strategies):\n",
    "        score_i, _ = payoff_A_vs_B_zero_sum(s_i, oppStrategy)\n",
    "        diff = score_i - scoreChosen\n",
    "        regretSum[i] += diff\n",
    "    return regretSum\n",
    "\n",
    "def get_mixed_strategy_from_regret(regretSum):\n",
    "    \"\"\"\n",
    "    p[i] = max(0, regretSum[i])/ sum_of_positives\n",
    "    若 sum_of_positives=0 => 均匀分布\n",
    "    \"\"\"\n",
    "    positives = [max(r,0) for r in regretSum]\n",
    "    totalPos = sum(positives)\n",
    "    n = len(regretSum)\n",
    "    if totalPos>1e-15:\n",
    "        return [p/totalPos for p in positives]\n",
    "    else:\n",
    "        return [1.0/n]*n\n",
    "\n",
    "def sample_from_distribution(dist):\n",
    "    r = random.random()\n",
    "    cum = 0.0\n",
    "    for i, p in enumerate(dist):\n",
    "        cum += p\n",
    "        if r<cum:\n",
    "            return i\n",
    "    return len(dist)-1\n",
    "\n",
    "# ===========  构造 payoff 矩阵 for CE check  ===========\n",
    "\n",
    "def build_payoff_matrices(strategies):\n",
    "    \"\"\"\n",
    "    A[i][j] = 当 A 用 i, B 用 j 时, A的收益\n",
    "    B[i][j] = 当 A 用 i, B 用 j 时, B的收益 = -A[i][j], 因为零和\n",
    "    \"\"\"\n",
    "    n = len(strategies)\n",
    "    A = [[0]*n for _ in range(n)]\n",
    "    B = [[0]*n for _ in range(n)]\n",
    "    for i, sa in enumerate(strategies):\n",
    "        for j, sb in enumerate(strategies):\n",
    "            scoreA, scoreB = payoff_A_vs_B_zero_sum(sa, sb)\n",
    "            A[i][j] = scoreA\n",
    "            B[i][j] = scoreB\n",
    "    return A,B\n",
    "\n",
    "# ===========  CE 检查 ===========\n",
    "\n",
    "def check_CE(A, B, jointDist, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    检查给定(A,B)收益矩阵, jointDist(联合分布) 是否近似CE.\n",
    "    A,B形状均 n x n, jointDist同理, sum=1.\n",
    "    \"\"\"\n",
    "    n = len(A)\n",
    "    # 1) 行玩家(A)条件收益\n",
    "    for i in range(n):\n",
    "        mu_i = sum(jointDist[i][j] for j in range(n))\n",
    "        if mu_i<epsilon: \n",
    "            continue\n",
    "        old_payoff = 0.0\n",
    "        for j in range(n):\n",
    "            old_payoff += jointDist[i][j]*A[i][j]\n",
    "        old_payoff /= mu_i\n",
    "        for i_prime in range(n):\n",
    "            if i_prime==i: \n",
    "                continue\n",
    "            new_payoff=0.0\n",
    "            for j in range(n):\n",
    "                new_payoff += jointDist[i][j]*A[i_prime][j]\n",
    "            new_payoff/=mu_i\n",
    "            if new_payoff>old_payoff+epsilon:\n",
    "                print(f\"[A violates CE]: i={i}, i'={i_prime}, diff={new_payoff-old_payoff:.3f}\")\n",
    "                return False\n",
    "    # 2) 列玩家(B)条件收益\n",
    "    for j in range(n):\n",
    "        mu_j = sum(jointDist[i][j] for i in range(n))\n",
    "        if mu_j<epsilon:\n",
    "            continue\n",
    "        old_payoff = 0.0\n",
    "        for i in range(n):\n",
    "            old_payoff += jointDist[i][j]*B[i][j]\n",
    "        old_payoff/= mu_j\n",
    "        for j_prime in range(n):\n",
    "            if j_prime==j: \n",
    "                continue\n",
    "            new_payoff=0.0\n",
    "            for i in range(n):\n",
    "                new_payoff += jointDist[i][j]*B[i][j_prime]\n",
    "            new_payoff/=mu_j\n",
    "            if new_payoff>old_payoff+epsilon:\n",
    "                print(f\"[B violates CE]: j={j}, j'={j_prime}, diff={new_payoff-old_payoff:.3f}\")\n",
    "                return False\n",
    "    print(\"=> jointDist satisfies (approx) CE condition.\")\n",
    "    return True\n",
    "\n",
    "# ===========  主函数: 无悔学习+收敛速度表  ===========\n",
    "\n",
    "def main():\n",
    "    random.seed()\n",
    "    S=5\n",
    "    N=3\n",
    "    allStrats = get_all_strategies(S,N)    # 生成纯策略\n",
    "    m = len(allStrats)                    # ~21\n",
    "    print(f\"We have {m} pure strategies for S={S},N={N}, in zero-sum version(tie=0).\")\n",
    "\n",
    "    # payoff 矩阵\n",
    "    A_mat, B_mat = build_payoff_matrices(allStrats)\n",
    "    # B_mat 应该= -A_mat, 在 build时自动算了 (scoreB= -scoreA or 0)\n",
    "    # 这里是tie=0, a>b =>(1,-1), a<b =>(-1,1).\n",
    "\n",
    "    # 后悔值\n",
    "    regretSumA = [0.0]*m\n",
    "    regretSumB = [0.0]*m\n",
    "    strategyA = [1.0/m]*m\n",
    "    strategyB = [1.0/m]*m\n",
    "\n",
    "    # 记录 (iA,iB) 出现频率 => 检查CE\n",
    "    jointCount = [[0]*m for _ in range(m)]\n",
    "\n",
    "    num_iters = 300000\n",
    "    # 记录收敛速度: average regret(对A)\n",
    "    regret_list = []\n",
    "\n",
    "    for t in range(1, num_iters+1):\n",
    "        # 1) 从当前混合策略采样\n",
    "        iA = sample_from_distribution(strategyA)\n",
    "        iB = sample_from_distribution(strategyB)\n",
    "        # 2) 更新后悔\n",
    "        stratA = allStrats[iA]\n",
    "        stratB = allStrats[iB]\n",
    "        regretSumA = regret_matching_update(regretSumA, allStrats, iA, stratB)\n",
    "        regretSumB = regret_matching_update(regretSumB, allStrats, iB, stratA)\n",
    "        # 3) 得到新的混合\n",
    "        strategyA = get_mixed_strategy_from_regret(regretSumA)\n",
    "        strategyB = get_mixed_strategy_from_regret(regretSumB)\n",
    "        # 4) jointCount\n",
    "        jointCount[iA][iB]+=1\n",
    "\n",
    "        # === 计算 \"平均外部后悔\" for A ===\n",
    "        # external regret for A:  max_{i'} regretSumA[i'] 的正部分\n",
    "        max_regret_A = max(regretSumA)  # 可能有负值\n",
    "        if max_regret_A < 0:\n",
    "            max_regret_A = 0\n",
    "        avg_regret_A = max_regret_A / t\n",
    "        regret_list.append(avg_regret_A)\n",
    "\n",
    "        # 可以每隔一定步数打印一次\n",
    "        if t%50000==0:\n",
    "            print(f\"Iter={t}, avg_regret_A={avg_regret_A:.5f}\")\n",
    "\n",
    "    # 最终 jointDist\n",
    "    jointDist = [[ jointCount[i][j]/num_iters for j in range(m)] for i in range(m)]\n",
    "\n",
    "    # 打印收敛速度表(或画图)\n",
    "    print(\"\\n--- Final average regrets (last few) ---\")\n",
    "    for step in [num_iters-5, num_iters-4, num_iters-3, num_iters-2, num_iters-1, num_iters]:\n",
    "        print(f\"  t={step}, avg_regret_A={regret_list[step-1]:.6f}\")\n",
    "\n",
    "    # 如果想画图，可加 matplotlib:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(1,num_iters+1), regret_list, label='Average regret(A)')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Average regret(A)')\n",
    "    plt.title('Regret Matching Convergence')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # ========== 最后做 CE 检验 ==========\n",
    "    print(\"\\nCheck if empirical jointDist is CE:\")\n",
    "    is_ce = check_CE(A_mat,B_mat, jointDist, epsilon=1e-2)\n",
    "    if is_ce:\n",
    "        print(\"Conclusion: It's an approximate CE.\\n\")\n",
    "    else:\n",
    "        print(\"Conclusion: Not CE.\\n\")\n",
    "\n",
    "    # 展示几项 jointDist>0.01\n",
    "    print(\"Some (i,j) with prob>0.01 in jointDist:\")\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if jointDist[i][j]>0.01:\n",
    "                print(f\"   (i={i}, j={j}), prob={jointDist[i][j]:.3f}, (A strat={allStrats[i]}, B strat={allStrats[j]})\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c55a7665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Q-learning for Colonel Blotto with (S=5, N=3).\n",
      "episode=500000, iA=2, iB=6, rA=1.0, rB=2.0, Q1[2]=1.439, Q2[6]=1.518\n",
      "episode=1000000, iA=13, iB=14, rA=1.5, rB=1.5, Q1[13]=1.512, Q2[14]=1.501\n",
      "episode=1500000, iA=8, iB=3, rA=1.5, rB=1.5, Q1[8]=1.501, Q2[3]=1.498\n",
      "episode=2000000, iA=14, iB=3, rA=1.5, rB=1.5, Q1[14]=1.500, Q2[3]=1.500\n",
      "Number of actions = 21\n",
      "\n",
      "=== Learned Q ===\n",
      "Q1= [1.21, 1.41, 1.45, 1.45, 1.44, 1.23, 1.4, 1.46, 1.45, 1.48, 1.41, 1.46, 1.48, 1.45, 1.5, 1.49, 1.48, 1.46, 1.39, 1.44, 1.19]\n",
      "Q2= [1.15, 1.35, 1.47, 1.5, 1.39, 1.2, 1.39, 1.44, 1.46, 1.47, 1.4, 1.44, 1.46, 1.47, 1.48, 1.48, 1.49, 1.49, 1.43, 1.46, 1.23]\n",
      "bestA=14, Q1=1.50, strategy=(2, 3, 0)\n",
      "bestB=3, Q2=1.50, strategy=(0, 3, 2)\n",
      "\n",
      "=== Approx mixed strategies via softmax(Q/0.01) ===\n",
      " A: action=7, prob=0.010, (1, 1, 3)\n",
      " A: action=9, prob=0.110, (1, 3, 1)\n",
      " A: action=11, prob=0.012, (2, 0, 3)\n",
      " A: action=12, prob=0.044, (2, 1, 2)\n",
      " A: action=14, prob=0.516, (2, 3, 0)\n",
      " A: action=15, prob=0.226, (3, 0, 2)\n",
      " A: action=16, prob=0.050, (3, 1, 1)\n",
      " A: action=17, prob=0.015, (3, 2, 0)\n",
      " B: action=2, prob=0.038, (0, 2, 3)\n",
      " B: action=3, prob=0.453, (0, 3, 2)\n",
      " B: action=8, prob=0.014, (1, 2, 2)\n",
      " B: action=9, prob=0.016, (1, 3, 1)\n",
      " B: action=13, prob=0.027, (2, 2, 1)\n",
      " B: action=14, prob=0.089, (2, 3, 0)\n",
      " B: action=15, prob=0.082, (3, 0, 2)\n",
      " B: action=16, prob=0.143, (3, 1, 1)\n",
      " B: action=17, prob=0.118, (3, 2, 0)\n",
      "\n",
      "Check if the softmax distribution is near NE:\n",
      "[A can deviate] i=2, payoff_iq=1.570, UA_pq=1.454\n",
      "Conclusion: Not NE.\n"
     ]
    }
   ],
   "source": [
    "#强化学习算法\n",
    "import random\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    列举所有把 S 个兵力分配到 N 个战场的纯策略:\n",
    "    返回一个列表, 列表里的每个元素是 (x1,x2,...,xN), sum= S, xi>=0.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def back(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            back(now+[i], left-i, slots-1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_blotto(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    (N=3) Colonel Blotto计分:\n",
    "      战场i: if A_i> B_i => A得1分; A_i< B_i => B得1分; 相等 => 各0.5\n",
    "    返回 (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB = 0.0, 0.0\n",
    "    for a, b in zip(strategyA, strategyB):\n",
    "        if a > b:\n",
    "            scoreA += 1\n",
    "        elif a < b:\n",
    "            scoreB += 1\n",
    "        else:\n",
    "            scoreA += 0.5\n",
    "            scoreB += 0.5\n",
    "    return (scoreA, scoreB)\n",
    "\n",
    "def epsilon_greedy(Q, epsilon):\n",
    "    \"\"\"\n",
    "    在 Q (list/array) 上做 epsilon-贪心:\n",
    "      - 以 epsilon 的概率随机动作\n",
    "      - 否则选 Q值最大的动作(如有并列, 随机选并列)\n",
    "    返回选到的动作索引(0..len(Q)-1).\n",
    "    \"\"\"\n",
    "    n = len(Q)\n",
    "    if random.random()< epsilon:\n",
    "        return random.randint(0, n-1)\n",
    "    else:\n",
    "        maxQ = max(Q)\n",
    "        candidates = [i for i,qv in enumerate(Q) if qv == maxQ]\n",
    "        return random.choice(candidates)\n",
    "\n",
    "def q_learning_blotto(S=5, N=3, alpha=0.1, epsilon=0.1, episodes=200000):\n",
    "    \"\"\"\n",
    "    在 (S=5, N=3) Colonel Blotto 中, 用Q-Learning让A,B都学习.\n",
    "    返回 (Q1, Q2, actionsA, actionsB).\n",
    "\n",
    "    - Q1[i]: 玩家A对动作 i 的Q值\n",
    "    - Q2[j]: 玩家B对动作 j 的Q值\n",
    "    - actionsA, actionsB: 动作列表(本例中相同,各= get_all_strategies(S,N))\n",
    "    \"\"\"\n",
    "    # 1) 枚举动作(兵力分配)\n",
    "    actionsA = get_all_strategies(S,N)  # len= C(S+N-1, N-1)\n",
    "    actionsB = actionsA  # 假设对称(都 S=5,N=3)\n",
    "    nA = len(actionsA)\n",
    "    nB = len(actionsB)\n",
    "\n",
    "    # 2) 初始化 Q表\n",
    "    Q1 = [0.0]*nA   # A 的 Q 值\n",
    "    Q2 = [0.0]*nB   # B 的 Q 值\n",
    "\n",
    "    for episode in range(1, episodes+1):\n",
    "        # a) A,B 各用 epsilon_greedy 从各自Q里选动作\n",
    "        iA = epsilon_greedy(Q1, epsilon)  # A的动作索引\n",
    "        iB = epsilon_greedy(Q2, epsilon)  # B的动作索引\n",
    "        stratA = actionsA[iA]\n",
    "        stratB = actionsB[iB]\n",
    "\n",
    "        # b) 得到奖励\n",
    "        rA, rB = payoff_blotto(stratA, stratB)\n",
    "\n",
    "        # c) 更新Q: Q(a) = Q(a) + alpha*(r- Q(a))   (无状态,下个状态不考虑)\n",
    "        oldA = Q1[iA]\n",
    "        oldB = Q2[iB]\n",
    "        Q1[iA] = oldA + alpha*(rA - oldA)\n",
    "        Q2[iB] = oldB + alpha*(rB - oldB)\n",
    "\n",
    "        # d) 适当打印信息(可选)\n",
    "        if episode%500000==0 and episode>0:\n",
    "            epsilon/=2\n",
    "            alpha/=2\n",
    "            print(f\"episode={episode}, iA={iA}, iB={iB}, rA={rA}, rB={rB}, \"\n",
    "                  f\"Q1[{iA}]={Q1[iA]:.3f}, Q2[{iB}]={Q2[iB]:.3f}\")\n",
    "\n",
    "    return Q1, Q2, actionsA, actionsB\n",
    "\n",
    "# ============ (可选) 构造 payoff 矩阵 & 检查 NE =============\n",
    "\n",
    "def build_payoff_matrices(actions):\n",
    "    \"\"\"\n",
    "    给定 actions(纯策略列表), 构造 A[i][j], B[i][j]:\n",
    "      A[i][j]: A用动作i, B用动作j 时, A的收益\n",
    "      B[i][j]: B的收益\n",
    "    \"\"\"\n",
    "    n = len(actions)\n",
    "    A = [[0]*n for _ in range(n)]\n",
    "    B = [[0]*n for _ in range(n)]\n",
    "    for i, a in enumerate(actions):\n",
    "        for j, b in enumerate(actions):\n",
    "            rA, rB = payoff_blotto(a, b)\n",
    "            A[i][j] = rA\n",
    "            B[i][j] = rB\n",
    "    return A,B\n",
    "\n",
    "def check_NE(A, B, p, q, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    检查( p, q )是否是(近似)纳什均衡:\n",
    "      - p,q: size=n,\n",
    "      - A,B: n×n payoff matrix\n",
    "    \"\"\"\n",
    "    n = len(A)\n",
    "    # 1) 计算 U_A(p,q), U_B(p,q)\n",
    "    UA_pq=0; UB_pq=0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            UA_pq += p[i]*q[j]*A[i][j]\n",
    "            UB_pq += p[i]*q[j]*B[i][j]\n",
    "\n",
    "    # 2) 对A\n",
    "    for i in range(n):\n",
    "        # U_A(i,q)\n",
    "        U_iq = sum(q[j]*A[i][j] for j in range(n))\n",
    "        if U_iq>UA_pq+epsilon:\n",
    "            print(f\"[A can deviate] i={i}, payoff_iq={U_iq:.3f}, UA_pq={UA_pq:.3f}\")\n",
    "            return False\n",
    "        if p[i]>epsilon:\n",
    "            if abs(U_iq-UA_pq)>1e-1:\n",
    "                print(f\"[A support mismatch] i={i}, payoff_iq={U_iq:.3f}, UA_pq={UA_pq:.3f}\")\n",
    "                return False\n",
    "    # 3) 对B\n",
    "    for j in range(n):\n",
    "        U_pj = sum(p[i]*B[i][j] for i in range(n))\n",
    "        if U_pj>UB_pq+epsilon:\n",
    "            print(f\"[B can deviate] j={j}, payoff_pj={U_pj:.3f}, UB_pq={UB_pq:.3f}\")\n",
    "            return False\n",
    "        if q[j]>epsilon:\n",
    "            if abs(U_pj-UB_pq)>1e-1:\n",
    "                print(f\"[B support mismatch] j={j}, payoff_pj={U_pj:.3f}, UB_pq={UB_pq:.3f}\")\n",
    "                return False\n",
    "\n",
    "    print(\"=> (p,q) is a near NE.\")\n",
    "    return True\n",
    "\n",
    "# =============== 主程序 ================\n",
    "def main():\n",
    "    random.seed()\n",
    "    # 超参数\n",
    "    S, N = 5, 3\n",
    "    alpha = 0.1\n",
    "    epsilon=0.1\n",
    "    episodes=2000000\n",
    "\n",
    "    print(f\"Start Q-learning for Colonel Blotto with (S={S}, N={N}).\")\n",
    "    # 1) 运行 Q-Learning\n",
    "    Q1, Q2, actionsA, actionsB = q_learning_blotto(S, N, alpha, epsilon, episodes)\n",
    "    nA = len(actionsA)\n",
    "    print(f\"Number of actions = {nA}\")\n",
    "\n",
    "    # 2) 最后: 取 Q中最大的动作(纯策略) 并显示\n",
    "    bestA = max(range(nA), key=lambda i: Q1[i])\n",
    "    bestB = max(range(nA), key=lambda i: Q2[i])\n",
    "    print(\"\\n=== Learned Q ===\")\n",
    "    print(\"Q1=\", [round(q,2) for q in Q1])\n",
    "    print(\"Q2=\", [round(q,2) for q in Q2])\n",
    "    print(f\"bestA={bestA}, Q1={Q1[bestA]:.2f}, strategy={actionsA[bestA]}\")\n",
    "    print(f\"bestB={bestB}, Q2={Q2[bestB]:.2f}, strategy={actionsB[bestB]}\")\n",
    "\n",
    "    # 3) 若想用 Softmax/其他方式获得混合策略, 也可做:\n",
    "    def softmax(Q, tau=0.01):\n",
    "        import math\n",
    "        exps = [math.exp(qv/tau) for qv in Q]\n",
    "        s = sum(exps)\n",
    "        return [e/s for e in exps]\n",
    "\n",
    "    pA = softmax(Q1, tau=0.01)\n",
    "    pB = softmax(Q2, tau=0.01)\n",
    "    # 只显示概率>0.01\n",
    "    print(\"\\n=== Approx mixed strategies via softmax(Q/0.01) ===\")\n",
    "    for i, prob in enumerate(pA):\n",
    "        if prob>0.01:\n",
    "            print(f\" A: action={i}, prob={prob:.3f}, {actionsA[i]}\")\n",
    "    for j, prob in enumerate(pB):\n",
    "        if prob>0.01:\n",
    "            print(f\" B: action={j}, prob={prob:.3f}, {actionsB[j]}\")\n",
    "\n",
    "    # 4) 构造 payoff 矩阵, 做 NE 检查\n",
    "    A_mat, B_mat = build_payoff_matrices(actionsA)  \n",
    "    print(\"\\nCheck if the softmax distribution is near NE:\")\n",
    "    is_ne = check_NE(A_mat, B_mat, pA, pB, epsilon=1e-1)\n",
    "    if not is_ne:\n",
    "        print(\"Conclusion: Not NE.\")\n",
    "    else:\n",
    "        print(\"Conclusion: It's near NE.\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c7e03c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions=21 for S=5,N=3. (Should be 21)\n",
      "\n",
      "--- If Opp's last action= 0, A's distribution (softmax of Q1_sf[0]):\n",
      "   Action i=5, prob=1.000, (0, 5, 0)\n",
      "\n",
      "Done. We used multi-state Q-learning (based on last action) with softmax or eps-greedy.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    返回所有将 S 个兵力分配到 N 个战场的纯策略列表 (tuple)。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def backtrack(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            backtrack(now+[i], left-i, slots-1)\n",
    "    backtrack([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_blotto(stratA, stratB):\n",
    "    \"\"\"\n",
    "    (N=3) 兵力比大小: if a>b => A得1分; a<b => B得1分; a=b =>各0.5\n",
    "    返回 (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB=0.0,0.0\n",
    "    for a,b in zip(stratA, stratB):\n",
    "        if a>b:\n",
    "            scoreA+=1\n",
    "        elif a<b:\n",
    "            scoreB+=1\n",
    "        else:\n",
    "            scoreA+=0.5\n",
    "            scoreB+=0.5\n",
    "    return (scoreA, scoreB)\n",
    "\n",
    "def softmax_selection(Q_row, tau=0.01):\n",
    "    \"\"\"\n",
    "    Softmax 选动作: Q_row是 [Q(state,0), Q(state,1),..., Q(state,nActions-1)],\n",
    "    返回动作索引.\n",
    "    \"\"\"\n",
    "    exps = [math.exp(qv/tau) for qv in Q_row]\n",
    "    s    = sum(exps)\n",
    "    r    = random.random()*s\n",
    "    cum  = 0.0\n",
    "    for i, e in enumerate(exps):\n",
    "        cum+=e\n",
    "        if r<=cum:\n",
    "            return i\n",
    "    return len(Q_row)-1\n",
    "\n",
    "def epsilon_greedy_selection(Q_row, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    epsilon 贪心: 以 epsilon 的概率随机，否则选Q最大的动作(并列随机)。\n",
    "    \"\"\"\n",
    "    n = len(Q_row)\n",
    "    if random.random()<epsilon:\n",
    "        return random.randint(0,n-1)\n",
    "    else:\n",
    "        maxQ= max(Q_row)\n",
    "        candidates = [i for i,qv in enumerate(Q_row) if qv==maxQ]\n",
    "        return random.choice(candidates)\n",
    "\n",
    "def q_learning_blotto_stateful(S=5, N=3, alpha=0.1, episodes=200_000, \n",
    "                               choose_method='softmax', tau=0.01, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    在 (S=5, N=3) Colonel Blotto 中做 \"多状态\" Q-Learning:\n",
    "      状态= \"对手上一回合动作索引\"(取值0..nActions-1, + 特殊值-1 表示无历史).\n",
    "    返回 (Q1, Q2, actions, nStates).\n",
    "    \"\"\"\n",
    "\n",
    "    actions = get_all_strategies(S,N)\n",
    "    nA = len(actions)\n",
    "    # 状态空间=  nA +1(无历史=-1?)\n",
    "    # 这里简化: 我们用 range(nA) + special -1. => total nA+1\n",
    "    # 但为了方便数组索引, 我们做 \"state=0..nA-1\" + \"state=nA 表示无历史\"\n",
    "    # => 所以 Q1, Q2 形状= (nA+1) x nA\n",
    "    nStates = nA+1\n",
    "    Q1 = [[0.0]*nA for _ in range(nStates)]  # Q1[state][actionA]\n",
    "    Q2 = [[0.0]*nA for _ in range(nStates)]  # Q2[state][actionB]\n",
    "\n",
    "    def select_action(Q_table, state):\n",
    "        \"\"\" 根据 choose_method (softmax or eps-greedy) 选动作 \"\"\"\n",
    "        Q_row = Q_table[state]\n",
    "        if choose_method=='softmax':\n",
    "            return softmax_selection(Q_row, tau)\n",
    "        else:\n",
    "            return epsilon_greedy_selection(Q_row, epsilon)\n",
    "\n",
    "    # 初始状态: stateA = nA, stateB=nA => 表示\"无历史\"\n",
    "    stateA = nA\n",
    "    stateB = nA\n",
    "\n",
    "    # 训练 episodes 回\n",
    "    for episode in range(1, episodes+1):\n",
    "        # 1) A,B 各在自己的状态 (stateA, stateB) 下选动作\n",
    "        iA = select_action(Q1, stateA)\n",
    "        iB = select_action(Q2, stateB)\n",
    "\n",
    "        # 2) 获得收益\n",
    "        stratA = actions[iA]\n",
    "        stratB = actions[iB]\n",
    "        rA, rB = payoff_blotto(stratA, stratB)\n",
    "\n",
    "        # 3) Q更新: Q(state,action) += alpha*(r - Q(state,action))\n",
    "        oldQ1 = Q1[stateA][iA]\n",
    "        oldQ2 = Q2[stateB][iB]\n",
    "        Q1[stateA][iA] = oldQ1 + alpha*(rA - oldQ1)\n",
    "        Q2[stateB][iB] = oldQ2 + alpha*(rB - oldQ2)\n",
    "\n",
    "        # 4) 下一个状态: A的 stateA' = iB, B的 stateB'= iA\n",
    "        nextStateA = iB\n",
    "        nextStateB = iA\n",
    "\n",
    "        # 5) 转移\n",
    "        stateA = nextStateA\n",
    "        stateB = nextStateB\n",
    "\n",
    "        # 可选:debug\n",
    "        # if episode%50000==0:\n",
    "        #    print(f\"episode={episode}, stateA={stateA}, iA={iA}, rA={rA}, Q1={Q1[stateA][iA]:.2f}\")\n",
    "\n",
    "    return Q1, Q2, actions\n",
    "\n",
    "# =========== 对最终策略做说明并可检验 ===========\n",
    "\n",
    "def main():\n",
    "    random.seed(42)\n",
    "\n",
    "    S,N=5,3\n",
    "    alpha=0.1\n",
    "    episodes=400000\n",
    "    # 1) 先用softmax训练\n",
    "    Q1_sf, Q2_sf, actions = q_learning_blotto_stateful(\n",
    "        S=S, N=N, alpha=alpha, episodes=episodes, \n",
    "        choose_method='softmax', tau=0.01)\n",
    "\n",
    "    # 2) 再用 eps-greedy 训练(对比)\n",
    "    Q1_eps, Q2_eps, actions2= q_learning_blotto_stateful(\n",
    "        S=S, N=N, alpha=alpha, episodes=episodes, \n",
    "        choose_method='eps-greedy', epsilon=0.1)\n",
    "\n",
    "    nA = len(actions)\n",
    "    print(f\"Number of actions={nA} for S={S},N={N}. (Should be 21)\")\n",
    "\n",
    "    # 如果想查看 Q1_sf: shape=(nA+1) x nA\n",
    "    # 最终你若对\"某个 state(对手上次动作= iOpp)\" => Q1_sf[iOpp], 再 softmax => 得到一个混合策略\n",
    "\n",
    "    # 这里举例: 如果对手上次动作= iOpp=0, 我方的策略行= Q1_sf[0],\n",
    "    # 用 softmax 变成概率分布:\n",
    "    def softmax_probs(Q_row, tau=0.01):\n",
    "        import math\n",
    "        exps = [math.exp(qv/tau) for qv in Q_row]\n",
    "        s= sum(exps)\n",
    "        return [e/s for e in exps]\n",
    "\n",
    "    # A: 查看对手上次动作 iOpp=0 -> A 的策略分布\n",
    "    iOpp=0\n",
    "    pA = softmax_probs(Q1_sf[iOpp], 0.01)\n",
    "    # 只显示大于0.01概率的\n",
    "    print(f\"\\n--- If Opp's last action= {iOpp}, A's distribution (softmax of Q1_sf[0]):\")\n",
    "    for iAction, pr in enumerate(pA):\n",
    "        if pr>0.01:\n",
    "            print(f\"   Action i={iAction}, prob={pr:.3f}, {actions[iAction]}\")\n",
    "\n",
    "    # 如果想得到\"总体策略\", 在重复对局中可能要再加一个对手动作分布假设。\n",
    "\n",
    "    # 这里就演示到此。你也可做 check_NE, 需先构造 payoff 矩阵 & 假设对手动作分布等。\n",
    "    print(\"\\nDone. We used multi-state Q-learning (based on last action) with softmax or eps-greedy.\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28803eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-state Q-Learning: S=5, N=3, episodes=20000000\n",
      "episode=50000, stateA=2, iA=2, rA=1.5, Q1=1.19\n",
      "episode=100000, stateA=7, iA=6, rA=1.5, Q1=0.53\n",
      "episode=150000, stateA=10, iA=8, rA=1.5, Q1=1.16\n",
      "episode=200000, stateA=3, iA=3, rA=1.5, Q1=1.06\n",
      "episode=250000, stateA=12, iA=12, rA=1.5, Q1=1.38\n",
      "episode=300000, stateA=12, iA=17, rA=2.0, Q1=1.50\n",
      "episode=350000, stateA=11, iA=3, rA=1.0, Q1=1.43\n",
      "episode=400000, stateA=8, iA=3, rA=1.5, Q1=1.35\n",
      "episode=450000, stateA=15, iA=3, rA=1.5, Q1=1.50\n",
      "episode=500000, stateA=3, iA=3, rA=1.5, Q1=1.16\n",
      "episode=550000, stateA=14, iA=6, rA=1.0, Q1=1.30\n",
      "episode=600000, stateA=14, iA=15, rA=2.0, Q1=1.50\n",
      "episode=650000, stateA=3, iA=15, rA=1.5, Q1=1.27\n",
      "episode=700000, stateA=15, iA=3, rA=1.5, Q1=1.48\n",
      "episode=750000, stateA=15, iA=16, rA=1.5, Q1=1.28\n",
      "episode=800000, stateA=1, iA=15, rA=1.0, Q1=1.27\n",
      "episode=850000, stateA=17, iA=14, rA=1.5, Q1=1.42\n",
      "episode=900000, stateA=1, iA=9, rA=2.0, Q1=1.52\n",
      "episode=950000, stateA=14, iA=8, rA=1.0, Q1=1.37\n",
      "episode=1000000, stateA=16, iA=12, rA=1.5, Q1=1.17\n",
      "episode=1050000, stateA=14, iA=8, rA=1.0, Q1=1.37\n",
      "episode=1100000, stateA=1, iA=15, rA=1.0, Q1=1.30\n",
      "episode=1150000, stateA=3, iA=9, rA=1.5, Q1=1.34\n",
      "episode=1200000, stateA=9, iA=2, rA=1.0, Q1=0.62\n",
      "episode=1250000, stateA=17, iA=4, rA=2.0, Q1=1.51\n",
      "episode=1300000, stateA=17, iA=12, rA=1.0, Q1=1.24\n",
      "episode=1350000, stateA=7, iA=14, rA=2.0, Q1=0.85\n",
      "episode=1400000, stateA=2, iA=2, rA=1.5, Q1=1.37\n",
      "episode=1450000, stateA=3, iA=17, rA=1.0, Q1=1.17\n",
      "episode=1500000, stateA=3, iA=4, rA=1.5, Q1=1.27\n",
      "episode=1550000, stateA=2, iA=2, rA=1.5, Q1=1.37\n",
      "episode=1600000, stateA=17, iA=4, rA=2.0, Q1=1.51\n",
      "episode=1650000, stateA=3, iA=4, rA=1.5, Q1=1.31\n",
      "episode=1700000, stateA=17, iA=4, rA=2.0, Q1=1.50\n",
      "episode=1750000, stateA=3, iA=4, rA=1.5, Q1=1.33\n",
      "episode=1800000, stateA=12, iA=4, rA=1.0, Q1=1.14\n",
      "episode=1850000, stateA=2, iA=12, rA=1.0, Q1=1.36\n",
      "episode=1900000, stateA=3, iA=14, rA=1.5, Q1=1.38\n",
      "episode=1950000, stateA=2, iA=14, rA=2.0, Q1=1.51\n",
      "episode=2000000, stateA=11, iA=3, rA=1.0, Q1=1.46\n",
      "episode=2050000, stateA=14, iA=12, rA=1.5, Q1=1.44\n",
      "episode=2100000, stateA=1, iA=14, rA=2.0, Q1=1.32\n",
      "episode=2150000, stateA=17, iA=17, rA=1.5, Q1=1.38\n",
      "episode=2200000, stateA=13, iA=2, rA=1.5, Q1=1.21\n",
      "episode=2250000, stateA=16, iA=17, rA=1.5, Q1=1.46\n",
      "episode=2300000, stateA=3, iA=9, rA=1.5, Q1=1.41\n",
      "episode=2350000, stateA=8, iA=7, rA=1.5, Q1=1.50\n",
      "episode=2400000, stateA=17, iA=11, rA=1.0, Q1=1.34\n",
      "episode=2450000, stateA=3, iA=16, rA=1.0, Q1=1.38\n",
      "episode=2500000, stateA=11, iA=17, rA=2.0, Q1=1.50\n",
      "episode=2550000, stateA=15, iA=14, rA=1.0, Q1=1.27\n",
      "episode=2600000, stateA=7, iA=8, rA=1.5, Q1=1.12\n",
      "episode=2650000, stateA=7, iA=15, rA=1.0, Q1=1.14\n",
      "episode=2700000, stateA=11, iA=17, rA=2.0, Q1=1.58\n",
      "episode=2750000, stateA=11, iA=3, rA=1.0, Q1=1.48\n",
      "episode=2800000, stateA=11, iA=3, rA=1.0, Q1=1.48\n",
      "episode=2850000, stateA=11, iA=11, rA=1.5, Q1=1.48\n",
      "episode=2900000, stateA=7, iA=3, rA=1.0, Q1=0.93\n",
      "episode=2950000, stateA=17, iA=9, rA=2.0, Q1=1.51\n",
      "episode=3000000, stateA=11, iA=17, rA=2.0, Q1=1.50\n",
      "episode=3050000, stateA=14, iA=2, rA=1.0, Q1=1.45\n",
      "episode=3100000, stateA=15, iA=2, rA=2.0, Q1=1.50\n",
      "episode=3150000, stateA=7, iA=15, rA=1.0, Q1=1.24\n",
      "episode=3200000, stateA=17, iA=14, rA=1.5, Q1=1.50\n",
      "episode=3250000, stateA=11, iA=9, rA=1.0, Q1=1.35\n",
      "episode=3300000, stateA=15, iA=13, rA=1.0, Q1=1.25\n",
      "episode=3350000, stateA=17, iA=11, rA=1.0, Q1=1.37\n",
      "episode=3400000, stateA=3, iA=17, rA=1.0, Q1=1.46\n",
      "episode=3450000, stateA=3, iA=9, rA=1.5, Q1=1.48\n",
      "episode=3500000, stateA=17, iA=11, rA=1.0, Q1=1.41\n",
      "episode=3550000, stateA=17, iA=3, rA=2.0, Q1=1.48\n",
      "episode=3600000, stateA=11, iA=2, rA=1.5, Q1=1.48\n",
      "episode=3650000, stateA=3, iA=3, rA=1.5, Q1=1.48\n",
      "episode=3700000, stateA=17, iA=17, rA=1.5, Q1=1.48\n",
      "episode=3750000, stateA=9, iA=12, rA=2.0, Q1=1.50\n",
      "episode=3800000, stateA=9, iA=12, rA=2.0, Q1=1.50\n",
      "episode=3850000, stateA=11, iA=14, rA=1.5, Q1=1.46\n",
      "episode=3900000, stateA=11, iA=14, rA=1.5, Q1=1.46\n",
      "episode=3950000, stateA=17, iA=15, rA=1.5, Q1=1.46\n",
      "episode=4000000, stateA=13, iA=10, rA=1.0, Q1=1.25\n",
      "episode=4050000, stateA=11, iA=16, rA=2.0, Q1=1.49\n",
      "episode=4100000, stateA=7, iA=15, rA=1.0, Q1=1.30\n",
      "episode=4150000, stateA=3, iA=17, rA=1.0, Q1=1.46\n",
      "episode=4200000, stateA=17, iA=17, rA=1.5, Q1=1.45\n",
      "episode=4250000, stateA=14, iA=16, rA=2.0, Q1=1.49\n",
      "episode=4300000, stateA=3, iA=15, rA=1.5, Q1=1.45\n",
      "episode=4350000, stateA=7, iA=17, rA=2.0, Q1=1.51\n",
      "episode=4400000, stateA=15, iA=7, rA=2.0, Q1=1.50\n",
      "episode=4450000, stateA=16, iA=17, rA=1.5, Q1=1.45\n",
      "episode=4500000, stateA=3, iA=11, rA=2.0, Q1=1.50\n",
      "episode=4550000, stateA=16, iA=3, rA=2.0, Q1=1.51\n",
      "episode=4600000, stateA=2, iA=7, rA=1.5, Q1=1.44\n",
      "episode=4650000, stateA=15, iA=7, rA=2.0, Q1=1.51\n",
      "episode=4700000, stateA=11, iA=12, rA=1.5, Q1=1.41\n",
      "episode=4750000, stateA=9, iA=3, rA=1.5, Q1=1.34\n",
      "episode=4800000, stateA=13, iA=14, rA=1.5, Q1=1.12\n",
      "episode=4850000, stateA=9, iA=17, rA=1.0, Q1=1.35\n",
      "episode=4900000, stateA=11, iA=11, rA=1.5, Q1=1.47\n",
      "episode=4950000, stateA=15, iA=7, rA=2.0, Q1=1.50\n",
      "episode=5000000, stateA=15, iA=17, rA=1.5, Q1=1.43\n",
      "episode=5050000, stateA=2, iA=3, rA=1.5, Q1=1.46\n",
      "episode=5100000, stateA=2, iA=14, rA=2.0, Q1=1.51\n",
      "episode=5150000, stateA=14, iA=14, rA=1.5, Q1=1.46\n",
      "episode=5200000, stateA=14, iA=3, rA=1.5, Q1=1.45\n",
      "episode=5250000, stateA=11, iA=17, rA=2.0, Q1=1.51\n",
      "episode=5300000, stateA=14, iA=15, rA=2.0, Q1=1.50\n",
      "episode=5350000, stateA=17, iA=14, rA=1.5, Q1=1.46\n",
      "episode=5400000, stateA=14, iA=3, rA=1.5, Q1=1.46\n",
      "episode=5450000, stateA=15, iA=15, rA=1.5, Q1=1.43\n",
      "episode=5500000, stateA=3, iA=15, rA=1.5, Q1=1.46\n",
      "episode=5550000, stateA=15, iA=2, rA=2.0, Q1=1.51\n",
      "episode=5600000, stateA=15, iA=15, rA=1.5, Q1=1.43\n",
      "episode=5650000, stateA=14, iA=3, rA=1.5, Q1=1.47\n",
      "episode=5700000, stateA=3, iA=14, rA=1.5, Q1=1.47\n",
      "episode=5750000, stateA=17, iA=14, rA=1.5, Q1=1.46\n",
      "episode=5800000, stateA=11, iA=2, rA=1.5, Q1=1.47\n",
      "episode=5850000, stateA=2, iA=15, rA=1.0, Q1=1.48\n",
      "episode=5900000, stateA=17, iA=14, rA=1.5, Q1=1.46\n",
      "episode=5950000, stateA=16, iA=11, rA=1.0, Q1=1.40\n",
      "episode=6000000, stateA=2, iA=2, rA=1.5, Q1=1.48\n",
      "episode=6050000, stateA=14, iA=2, rA=1.0, Q1=1.46\n",
      "episode=6100000, stateA=14, iA=3, rA=1.5, Q1=1.47\n",
      "episode=6150000, stateA=2, iA=12, rA=1.0, Q1=1.38\n",
      "episode=6200000, stateA=11, iA=6, rA=1.5, Q1=1.47\n",
      "episode=6250000, stateA=3, iA=17, rA=1.0, Q1=1.49\n",
      "episode=6300000, stateA=1, iA=17, rA=2.0, Q1=1.49\n",
      "episode=6350000, stateA=14, iA=2, rA=1.0, Q1=1.44\n",
      "episode=6400000, stateA=15, iA=3, rA=1.5, Q1=1.46\n",
      "episode=6450000, stateA=15, iA=3, rA=1.5, Q1=1.46\n",
      "episode=6500000, stateA=15, iA=15, rA=1.5, Q1=1.47\n",
      "episode=6550000, stateA=2, iA=15, rA=1.0, Q1=1.45\n",
      "episode=6600000, stateA=15, iA=15, rA=1.5, Q1=1.47\n",
      "episode=6650000, stateA=3, iA=3, rA=1.5, Q1=1.46\n",
      "episode=6700000, stateA=15, iA=11, rA=1.5, Q1=1.46\n",
      "episode=6750000, stateA=2, iA=14, rA=2.0, Q1=1.50\n",
      "episode=6800000, stateA=15, iA=4, rA=1.0, Q1=1.40\n",
      "episode=6850000, stateA=3, iA=7, rA=2.0, Q1=1.51\n",
      "episode=6900000, stateA=14, iA=11, rA=1.5, Q1=1.50\n",
      "episode=6950000, stateA=15, iA=2, rA=2.0, Q1=1.50\n",
      "episode=7000000, stateA=13, iA=9, rA=1.5, Q1=1.34\n",
      "episode=7050000, stateA=1, iA=11, rA=1.0, Q1=1.29\n",
      "episode=7100000, stateA=2, iA=2, rA=1.5, Q1=1.43\n",
      "episode=7150000, stateA=2, iA=16, rA=1.0, Q1=1.42\n",
      "episode=7200000, stateA=15, iA=3, rA=1.5, Q1=1.49\n",
      "episode=7250000, stateA=2, iA=17, rA=1.5, Q1=1.49\n",
      "episode=7300000, stateA=2, iA=11, rA=1.5, Q1=1.47\n",
      "episode=7350000, stateA=16, iA=11, rA=1.0, Q1=1.42\n",
      "episode=7400000, stateA=9, iA=3, rA=1.5, Q1=1.37\n",
      "episode=7450000, stateA=3, iA=17, rA=1.0, Q1=1.47\n",
      "episode=7500000, stateA=3, iA=7, rA=2.0, Q1=1.49\n",
      "episode=7550000, stateA=7, iA=15, rA=1.0, Q1=1.40\n",
      "episode=7600000, stateA=7, iA=9, rA=1.5, Q1=1.43\n",
      "episode=7650000, stateA=7, iA=17, rA=2.0, Q1=1.50\n",
      "episode=7700000, stateA=14, iA=15, rA=2.0, Q1=1.51\n",
      "episode=7750000, stateA=17, iA=9, rA=2.0, Q1=1.51\n",
      "episode=7800000, stateA=15, iA=17, rA=1.5, Q1=1.47\n",
      "episode=7850000, stateA=17, iA=2, rA=1.5, Q1=1.45\n",
      "episode=7900000, stateA=7, iA=20, rA=1.0, Q1=1.17\n",
      "episode=7950000, stateA=15, iA=11, rA=1.5, Q1=1.46\n",
      "episode=8000000, stateA=15, iA=11, rA=1.5, Q1=1.47\n",
      "episode=8050000, stateA=7, iA=11, rA=1.5, Q1=1.47\n",
      "episode=8100000, stateA=17, iA=16, rA=1.5, Q1=1.46\n",
      "episode=8150000, stateA=15, iA=9, rA=1.0, Q1=1.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=8200000, stateA=7, iA=11, rA=1.5, Q1=1.48\n",
      "episode=8250000, stateA=3, iA=2, rA=1.5, Q1=1.47\n",
      "episode=8300000, stateA=15, iA=7, rA=2.0, Q1=1.51\n",
      "episode=8350000, stateA=7, iA=9, rA=1.5, Q1=1.48\n",
      "episode=8400000, stateA=9, iA=2, rA=1.0, Q1=1.33\n",
      "episode=8450000, stateA=14, iA=2, rA=1.0, Q1=1.43\n",
      "episode=8500000, stateA=8, iA=16, rA=1.0, Q1=1.33\n",
      "episode=8550000, stateA=7, iA=16, rA=1.5, Q1=1.47\n",
      "episode=8600000, stateA=17, iA=2, rA=1.5, Q1=1.47\n",
      "episode=8650000, stateA=14, iA=15, rA=2.0, Q1=1.51\n",
      "episode=8700000, stateA=16, iA=15, rA=1.5, Q1=1.43\n",
      "episode=8750000, stateA=16, iA=2, rA=2.0, Q1=1.50\n",
      "episode=8800000, stateA=17, iA=2, rA=1.5, Q1=1.49\n",
      "episode=8850000, stateA=17, iA=17, rA=1.5, Q1=1.50\n",
      "episode=8900000, stateA=17, iA=17, rA=1.5, Q1=1.48\n",
      "episode=8950000, stateA=7, iA=17, rA=2.0, Q1=1.50\n",
      "episode=9000000, stateA=16, iA=17, rA=1.5, Q1=1.50\n",
      "episode=9050000, stateA=13, iA=2, rA=1.5, Q1=1.40\n",
      "episode=9100000, stateA=16, iA=15, rA=1.5, Q1=1.41\n",
      "episode=9150000, stateA=14, iA=11, rA=1.5, Q1=1.47\n",
      "episode=9200000, stateA=11, iA=17, rA=2.0, Q1=1.50\n",
      "episode=9250000, stateA=2, iA=9, rA=2.0, Q1=1.50\n",
      "episode=9300000, stateA=16, iA=14, rA=1.0, Q1=1.45\n",
      "episode=9350000, stateA=15, iA=16, rA=1.5, Q1=1.46\n",
      "episode=9400000, stateA=9, iA=3, rA=1.5, Q1=1.45\n",
      "episode=9450000, stateA=2, iA=15, rA=1.0, Q1=1.47\n",
      "episode=9500000, stateA=16, iA=9, rA=1.5, Q1=1.45\n",
      "episode=9550000, stateA=3, iA=11, rA=2.0, Q1=1.58\n",
      "episode=9600000, stateA=2, iA=2, rA=1.5, Q1=1.46\n",
      "episode=9650000, stateA=16, iA=9, rA=1.5, Q1=1.46\n",
      "episode=9700000, stateA=7, iA=17, rA=2.0, Q1=1.50\n",
      "episode=9750000, stateA=16, iA=9, rA=1.5, Q1=1.47\n",
      "episode=9800000, stateA=7, iA=11, rA=1.5, Q1=1.46\n",
      "episode=9850000, stateA=15, iA=3, rA=1.5, Q1=1.46\n",
      "episode=9900000, stateA=15, iA=11, rA=1.5, Q1=1.45\n",
      "episode=9950000, stateA=2, iA=9, rA=2.0, Q1=1.50\n",
      "episode=10000000, stateA=14, iA=18, rA=2.0, Q1=1.47\n",
      "episode=10050000, stateA=15, iA=16, rA=1.5, Q1=1.44\n",
      "episode=10100000, stateA=2, iA=9, rA=2.0, Q1=1.51\n",
      "episode=10150000, stateA=2, iA=11, rA=1.5, Q1=1.46\n",
      "episode=10200000, stateA=7, iA=17, rA=2.0, Q1=1.50\n",
      "episode=10250000, stateA=2, iA=9, rA=2.0, Q1=1.51\n",
      "episode=10300000, stateA=9, iA=17, rA=1.0, Q1=1.47\n",
      "episode=10350000, stateA=16, iA=11, rA=1.0, Q1=1.47\n",
      "episode=10400000, stateA=16, iA=14, rA=1.0, Q1=1.40\n",
      "episode=10450000, stateA=3, iA=9, rA=1.5, Q1=1.44\n",
      "episode=10500000, stateA=7, iA=17, rA=2.0, Q1=1.51\n",
      "episode=10550000, stateA=15, iA=3, rA=1.5, Q1=1.45\n",
      "episode=10600000, stateA=17, iA=7, rA=1.0, Q1=1.42\n",
      "episode=10650000, stateA=2, iA=14, rA=2.0, Q1=1.51\n",
      "episode=10700000, stateA=2, iA=3, rA=1.5, Q1=1.45\n",
      "episode=10750000, stateA=2, iA=2, rA=1.5, Q1=1.47\n",
      "episode=10800000, stateA=15, iA=7, rA=2.0, Q1=1.49\n",
      "episode=10850000, stateA=2, iA=19, rA=1.0, Q1=1.40\n",
      "episode=10900000, stateA=7, iA=3, rA=1.0, Q1=1.45\n",
      "episode=10950000, stateA=1, iA=17, rA=2.0, Q1=1.50\n",
      "episode=11000000, stateA=17, iA=3, rA=2.0, Q1=1.50\n",
      "episode=11050000, stateA=9, iA=3, rA=1.5, Q1=1.47\n",
      "episode=11100000, stateA=9, iA=16, rA=1.5, Q1=1.46\n",
      "episode=11150000, stateA=14, iA=16, rA=2.0, Q1=1.50\n",
      "episode=11200000, stateA=9, iA=11, rA=2.0, Q1=1.51\n",
      "episode=11250000, stateA=14, iA=16, rA=2.0, Q1=1.50\n",
      "episode=11300000, stateA=15, iA=9, rA=1.0, Q1=1.45\n",
      "episode=11350000, stateA=7, iA=8, rA=1.5, Q1=1.48\n",
      "episode=11400000, stateA=7, iA=7, rA=1.5, Q1=1.46\n",
      "episode=11450000, stateA=16, iA=17, rA=1.5, Q1=1.48\n",
      "episode=11500000, stateA=17, iA=7, rA=1.0, Q1=1.43\n",
      "episode=11550000, stateA=14, iA=9, rA=1.5, Q1=1.45\n",
      "episode=11600000, stateA=11, iA=9, rA=1.0, Q1=1.38\n",
      "episode=11650000, stateA=2, iA=11, rA=1.5, Q1=1.48\n",
      "episode=11700000, stateA=15, iA=6, rA=1.5, Q1=1.45\n",
      "episode=11750000, stateA=14, iA=16, rA=2.0, Q1=1.51\n",
      "episode=11800000, stateA=15, iA=3, rA=1.5, Q1=1.51\n",
      "episode=11850000, stateA=9, iA=3, rA=1.5, Q1=1.47\n",
      "episode=11900000, stateA=9, iA=16, rA=1.5, Q1=1.47\n",
      "episode=11950000, stateA=16, iA=11, rA=1.0, Q1=1.46\n",
      "episode=12000000, stateA=14, iA=17, rA=1.5, Q1=1.47\n",
      "episode=12050000, stateA=9, iA=16, rA=1.5, Q1=1.49\n",
      "episode=12100000, stateA=2, iA=17, rA=1.5, Q1=1.46\n",
      "episode=12150000, stateA=15, iA=2, rA=2.0, Q1=1.50\n",
      "episode=12200000, stateA=15, iA=16, rA=1.5, Q1=1.50\n",
      "episode=12250000, stateA=10, iA=14, rA=1.5, Q1=1.05\n",
      "episode=12300000, stateA=10, iA=14, rA=1.5, Q1=1.08\n",
      "episode=12350000, stateA=15, iA=3, rA=1.5, Q1=1.50\n",
      "episode=12400000, stateA=15, iA=2, rA=2.0, Q1=1.50\n",
      "episode=12450000, stateA=9, iA=17, rA=1.0, Q1=1.47\n",
      "episode=12500000, stateA=13, iA=7, rA=1.0, Q1=1.35\n",
      "episode=12550000, stateA=14, iA=11, rA=1.5, Q1=1.46\n",
      "episode=12600000, stateA=3, iA=11, rA=2.0, Q1=1.51\n",
      "episode=12650000, stateA=17, iA=2, rA=1.5, Q1=1.47\n",
      "episode=12700000, stateA=7, iA=13, rA=2.0, Q1=1.50\n",
      "episode=12750000, stateA=14, iA=7, rA=1.0, Q1=1.47\n",
      "episode=12800000, stateA=11, iA=17, rA=2.0, Q1=1.50\n",
      "episode=12850000, stateA=14, iA=11, rA=1.5, Q1=1.47\n",
      "episode=12900000, stateA=7, iA=3, rA=1.0, Q1=1.42\n",
      "episode=12950000, stateA=2, iA=16, rA=1.0, Q1=1.45\n",
      "episode=13000000, stateA=3, iA=11, rA=2.0, Q1=1.49\n",
      "episode=13050000, stateA=3, iA=3, rA=1.5, Q1=1.47\n",
      "episode=13100000, stateA=11, iA=17, rA=2.0, Q1=1.50\n",
      "episode=13150000, stateA=16, iA=17, rA=1.5, Q1=1.42\n",
      "episode=13200000, stateA=14, iA=17, rA=1.5, Q1=1.51\n",
      "episode=13250000, stateA=2, iA=15, rA=1.0, Q1=1.46\n",
      "episode=13300000, stateA=15, iA=7, rA=2.0, Q1=1.50\n",
      "episode=13350000, stateA=14, iA=9, rA=1.5, Q1=1.46\n",
      "episode=13400000, stateA=2, iA=9, rA=2.0, Q1=1.50\n",
      "episode=13450000, stateA=7, iA=3, rA=1.0, Q1=1.45\n",
      "episode=13500000, stateA=15, iA=7, rA=2.0, Q1=1.51\n",
      "episode=13550000, stateA=16, iA=7, rA=1.5, Q1=1.47\n",
      "episode=13600000, stateA=9, iA=3, rA=1.5, Q1=1.46\n",
      "episode=13650000, stateA=2, iA=7, rA=1.5, Q1=1.47\n",
      "episode=13700000, stateA=9, iA=16, rA=1.5, Q1=1.47\n",
      "episode=13750000, stateA=7, iA=11, rA=1.5, Q1=1.48\n",
      "episode=13800000, stateA=16, iA=8, rA=2.0, Q1=1.49\n",
      "episode=13850000, stateA=15, iA=17, rA=1.5, Q1=1.44\n",
      "episode=13900000, stateA=15, iA=2, rA=2.0, Q1=1.51\n",
      "episode=13950000, stateA=17, iA=11, rA=1.0, Q1=1.44\n",
      "episode=14000000, stateA=2, iA=2, rA=1.5, Q1=1.47\n",
      "episode=14050000, stateA=15, iA=2, rA=2.0, Q1=1.51\n",
      "episode=14100000, stateA=14, iA=14, rA=1.5, Q1=1.46\n",
      "episode=14150000, stateA=14, iA=14, rA=1.5, Q1=1.46\n",
      "episode=14200000, stateA=9, iA=17, rA=1.0, Q1=1.44\n",
      "episode=14250000, stateA=14, iA=14, rA=1.5, Q1=1.47\n",
      "episode=14300000, stateA=9, iA=16, rA=1.5, Q1=1.47\n",
      "episode=14350000, stateA=2, iA=3, rA=1.5, Q1=1.48\n",
      "episode=14400000, stateA=14, iA=3, rA=1.5, Q1=1.47\n",
      "episode=14450000, stateA=2, iA=16, rA=1.0, Q1=1.47\n",
      "episode=14500000, stateA=14, iA=3, rA=1.5, Q1=1.47\n",
      "episode=14550000, stateA=11, iA=11, rA=1.5, Q1=1.46\n",
      "episode=14600000, stateA=16, iA=11, rA=1.0, Q1=1.45\n",
      "episode=14650000, stateA=9, iA=11, rA=2.0, Q1=1.50\n",
      "episode=14700000, stateA=11, iA=12, rA=1.5, Q1=1.47\n",
      "episode=14750000, stateA=11, iA=9, rA=1.0, Q1=1.40\n",
      "episode=14800000, stateA=11, iA=2, rA=1.5, Q1=1.45\n",
      "episode=14850000, stateA=14, iA=16, rA=2.0, Q1=1.50\n",
      "episode=14900000, stateA=3, iA=14, rA=1.5, Q1=1.45\n",
      "episode=14950000, stateA=3, iA=14, rA=1.5, Q1=1.45\n",
      "episode=15000000, stateA=15, iA=2, rA=2.0, Q1=1.50\n",
      "episode=15050000, stateA=3, iA=16, rA=1.0, Q1=1.43\n",
      "episode=15100000, stateA=17, iA=3, rA=2.0, Q1=1.51\n",
      "episode=15150000, stateA=9, iA=11, rA=2.0, Q1=1.51\n",
      "episode=15200000, stateA=2, iA=16, rA=1.0, Q1=1.46\n",
      "episode=15250000, stateA=8, iA=16, rA=1.0, Q1=1.36\n",
      "episode=15300000, stateA=2, iA=7, rA=1.5, Q1=1.45\n",
      "episode=15350000, stateA=15, iA=9, rA=1.0, Q1=1.47\n",
      "episode=15400000, stateA=11, iA=11, rA=1.5, Q1=1.46\n",
      "episode=15450000, stateA=17, iA=11, rA=1.0, Q1=1.44\n",
      "episode=15500000, stateA=3, iA=4, rA=1.5, Q1=1.45\n",
      "episode=15550000, stateA=8, iA=3, rA=1.5, Q1=1.47\n",
      "episode=15600000, stateA=9, iA=3, rA=1.5, Q1=1.47\n",
      "episode=15650000, stateA=15, iA=9, rA=1.0, Q1=1.45\n",
      "episode=15700000, stateA=15, iA=16, rA=1.5, Q1=1.45\n",
      "episode=15750000, stateA=14, iA=11, rA=1.5, Q1=1.47\n",
      "episode=15800000, stateA=15, iA=3, rA=1.5, Q1=1.45\n",
      "episode=15850000, stateA=15, iA=9, rA=1.0, Q1=1.44\n",
      "episode=15900000, stateA=9, iA=17, rA=1.0, Q1=1.44\n",
      "episode=15950000, stateA=16, iA=3, rA=2.0, Q1=1.51\n",
      "episode=16000000, stateA=9, iA=11, rA=2.0, Q1=1.50\n",
      "episode=16050000, stateA=2, iA=7, rA=1.5, Q1=1.46\n",
      "episode=16100000, stateA=9, iA=17, rA=1.0, Q1=1.45\n",
      "episode=16150000, stateA=15, iA=13, rA=1.0, Q1=1.41\n",
      "episode=16200000, stateA=2, iA=15, rA=1.0, Q1=1.46\n",
      "episode=16250000, stateA=2, iA=9, rA=2.0, Q1=1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=16300000, stateA=9, iA=11, rA=2.0, Q1=1.50\n",
      "episode=16350000, stateA=11, iA=9, rA=1.0, Q1=1.41\n",
      "episode=16400000, stateA=9, iA=2, rA=1.0, Q1=1.48\n",
      "episode=16450000, stateA=9, iA=3, rA=1.5, Q1=1.51\n",
      "episode=16500000, stateA=11, iA=14, rA=1.5, Q1=1.47\n",
      "episode=16550000, stateA=2, iA=9, rA=2.0, Q1=1.49\n",
      "episode=16600000, stateA=2, iA=3, rA=1.5, Q1=1.45\n",
      "episode=16650000, stateA=15, iA=14, rA=1.0, Q1=1.45\n",
      "episode=16700000, stateA=9, iA=17, rA=1.0, Q1=1.42\n",
      "episode=16750000, stateA=13, iA=12, rA=1.5, Q1=1.38\n",
      "episode=16800000, stateA=3, iA=17, rA=1.0, Q1=1.41\n",
      "episode=16850000, stateA=17, iA=17, rA=1.5, Q1=1.44\n",
      "episode=16900000, stateA=17, iA=15, rA=1.5, Q1=1.45\n",
      "episode=16950000, stateA=11, iA=3, rA=1.0, Q1=1.45\n",
      "episode=17000000, stateA=15, iA=15, rA=1.5, Q1=1.46\n",
      "episode=17050000, stateA=15, iA=2, rA=2.0, Q1=1.51\n",
      "episode=17100000, stateA=14, iA=14, rA=1.5, Q1=1.45\n",
      "episode=17150000, stateA=2, iA=14, rA=2.0, Q1=1.50\n",
      "episode=17200000, stateA=3, iA=3, rA=1.5, Q1=1.45\n",
      "episode=17250000, stateA=3, iA=3, rA=1.5, Q1=1.45\n",
      "episode=17300000, stateA=2, iA=15, rA=1.0, Q1=1.45\n",
      "episode=17350000, stateA=15, iA=16, rA=1.5, Q1=1.50\n",
      "episode=17400000, stateA=11, iA=11, rA=1.5, Q1=1.47\n",
      "episode=17450000, stateA=14, iA=3, rA=1.5, Q1=1.46\n",
      "episode=17500000, stateA=11, iA=3, rA=1.0, Q1=1.45\n",
      "episode=17550000, stateA=11, iA=11, rA=1.5, Q1=1.48\n",
      "episode=17600000, stateA=11, iA=3, rA=1.0, Q1=1.44\n",
      "episode=17650000, stateA=8, iA=7, rA=1.5, Q1=1.45\n",
      "episode=17700000, stateA=11, iA=17, rA=2.0, Q1=1.52\n",
      "episode=17750000, stateA=3, iA=15, rA=1.5, Q1=1.48\n",
      "episode=17800000, stateA=17, iA=17, rA=1.5, Q1=1.46\n",
      "episode=17850000, stateA=14, iA=3, rA=1.5, Q1=1.45\n",
      "episode=17900000, stateA=11, iA=3, rA=1.0, Q1=1.47\n",
      "episode=17950000, stateA=15, iA=2, rA=2.0, Q1=1.51\n",
      "episode=18000000, stateA=2, iA=15, rA=1.0, Q1=1.47\n",
      "episode=18050000, stateA=15, iA=3, rA=1.5, Q1=1.47\n",
      "episode=18100000, stateA=17, iA=11, rA=1.0, Q1=1.47\n",
      "episode=18150000, stateA=11, iA=2, rA=1.5, Q1=1.46\n",
      "episode=18200000, stateA=2, iA=9, rA=2.0, Q1=1.50\n",
      "episode=18250000, stateA=2, iA=11, rA=1.5, Q1=1.46\n",
      "episode=18300000, stateA=2, iA=9, rA=2.0, Q1=1.51\n",
      "episode=18350000, stateA=14, iA=15, rA=2.0, Q1=1.49\n",
      "episode=18400000, stateA=3, iA=7, rA=2.0, Q1=1.49\n",
      "episode=18450000, stateA=2, iA=9, rA=2.0, Q1=1.50\n",
      "episode=18500000, stateA=8, iA=7, rA=1.5, Q1=1.45\n",
      "episode=18550000, stateA=12, iA=17, rA=2.0, Q1=1.50\n",
      "episode=18600000, stateA=3, iA=7, rA=2.0, Q1=1.51\n",
      "episode=18650000, stateA=11, iA=3, rA=1.0, Q1=1.46\n",
      "episode=18700000, stateA=14, iA=15, rA=2.0, Q1=1.50\n",
      "episode=18750000, stateA=3, iA=11, rA=2.0, Q1=1.50\n",
      "episode=18800000, stateA=17, iA=19, rA=1.5, Q1=1.46\n",
      "episode=18850000, stateA=11, iA=9, rA=1.0, Q1=1.45\n",
      "episode=18900000, stateA=3, iA=10, rA=2.0, Q1=1.51\n",
      "episode=18950000, stateA=2, iA=15, rA=1.0, Q1=1.44\n",
      "episode=19000000, stateA=3, iA=17, rA=1.0, Q1=1.43\n",
      "episode=19050000, stateA=7, iA=9, rA=1.5, Q1=1.46\n",
      "episode=19100000, stateA=17, iA=2, rA=1.5, Q1=1.47\n",
      "episode=19150000, stateA=15, iA=16, rA=1.5, Q1=1.46\n",
      "episode=19200000, stateA=15, iA=11, rA=1.5, Q1=1.46\n",
      "episode=19250000, stateA=2, iA=2, rA=1.5, Q1=1.45\n",
      "episode=19300000, stateA=15, iA=11, rA=1.5, Q1=1.45\n",
      "episode=19350000, stateA=2, iA=15, rA=1.0, Q1=1.46\n",
      "episode=19400000, stateA=3, iA=16, rA=1.0, Q1=1.46\n",
      "episode=19450000, stateA=7, iA=11, rA=1.5, Q1=1.45\n",
      "episode=19500000, stateA=17, iA=11, rA=1.0, Q1=1.45\n",
      "episode=19550000, stateA=15, iA=3, rA=1.5, Q1=1.48\n",
      "episode=19600000, stateA=2, iA=16, rA=1.0, Q1=1.43\n",
      "episode=19650000, stateA=9, iA=3, rA=1.5, Q1=1.47\n",
      "episode=19700000, stateA=7, iA=7, rA=1.5, Q1=1.49\n",
      "episode=19750000, stateA=9, iA=14, rA=1.5, Q1=1.45\n",
      "episode=19800000, stateA=9, iA=2, rA=1.0, Q1=1.46\n",
      "episode=19850000, stateA=16, iA=16, rA=1.5, Q1=1.47\n",
      "episode=19900000, stateA=9, iA=15, rA=2.0, Q1=1.50\n",
      "episode=19950000, stateA=9, iA=9, rA=1.5, Q1=1.47\n",
      "episode=20000000, stateA=16, iA=16, rA=1.5, Q1=1.48\n",
      "Number of actions= 21 (should be 21)\n",
      "\n",
      "From 'no-history' state perspective, bestA= 17, bestB= 18\n",
      "A strategy= (3, 2, 0), B strategy= (4, 0, 1)\n",
      "\n",
      "Softmax distribution from 'no-history' state (prob>0.01):\n",
      " A: i=17, prob=0.999, (3, 2, 0)\n",
      " B: j=18, prob=1.000, (4, 0, 1)\n",
      "\n",
      "Check NE for the softmax distribution at 'no-history' state:\n",
      "[A can deviate] i=0, payoff_iq=1.500 > UA_pq=1.001\n",
      "Conclusion: Not NE.\n"
     ]
    }
   ],
   "source": [
    "#考虑对手行为的强化学习\n",
    "import random\n",
    "import math\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    列举所有把 S 个兵力分配到 N 个战场的纯策略:\n",
    "    返回一个列表, 列表里的每个元素是 (x1,x2,...,xN), sum= S, xi>=0.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def back(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            back(now+[i], left-i, slots-1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_blotto(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    (N=3) Colonel Blotto计分:\n",
    "      战场i: if A_i> B_i => A得1分; A_i< B_i => B得1分; 相等 => 各0.5\n",
    "    返回 (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB = 0.0, 0.0\n",
    "    for a, b in zip(strategyA, strategyB):\n",
    "        if a > b:\n",
    "            scoreA += 1\n",
    "        elif a < b:\n",
    "            scoreB += 1\n",
    "        else:\n",
    "            scoreA += 0.5\n",
    "            scoreB += 0.5\n",
    "    return (scoreA, scoreB)\n",
    "\n",
    "def epsilon_greedy(Q_row, epsilon):\n",
    "    \"\"\"\n",
    "    在 Q_row(list/array) 上做 epsilon-贪心:\n",
    "      - 以 epsilon 的概率随机动作(0..len(Q_row)-1)\n",
    "      - 否则选 Q值最大的动作(如有并列, 随机选并列)\n",
    "    返回选到的动作索引(0..len(Q_row)-1).\n",
    "    \"\"\"\n",
    "    n = len(Q_row)\n",
    "    if random.random()< epsilon:\n",
    "        return random.randint(0, n-1)\n",
    "    else:\n",
    "        maxQ = max(Q_row)\n",
    "        candidates = [i for i,qv in enumerate(Q_row) if qv == maxQ]\n",
    "        return random.choice(candidates)\n",
    "\n",
    "\n",
    "def q_learning_blotto_multi_state(S=5, N=3, alpha=0.1, epsilon=0.1, episodes=200000):\n",
    "    \"\"\"\n",
    "    A,B 各自的状态 = \"对手上一回合选的动作\" (0..nActs-1),\n",
    "    动作空间= nActs(=21).\n",
    "    若是第一回合, 用一个特殊 state (nActs) 表示 \"无历史\".\n",
    "    \n",
    "    Q1[stateA][actionA], Q2[stateB][actionB].\n",
    "    每回合:\n",
    "      stateA = B上回合动作, stateB= A上回合动作\n",
    "      A 用 epsilon_greedy(Q1[stateA]) 选 iA\n",
    "      B 用 epsilon_greedy(Q2[stateB]) 选 iB\n",
    "      payoff => rA,rB\n",
    "      Q1[stateA][iA]+= alpha*(rA - oldQ)\n",
    "      Q2[stateB][iB]+= alpha*(rB - oldQ)\n",
    "      nextStateA= iB, nextStateB=iA\n",
    "    \"\"\"\n",
    "    actions = get_all_strategies(S,N)  # 21 个动作\n",
    "    nActs   = len(actions)\n",
    "    # 状态数= nActs +1 (其中最后一个表示 \"无历史\" )\n",
    "    nStates = nActs+1\n",
    "\n",
    "    # 初始化 Q表: Q1, Q2 均 shape= (nStates x nActs)\n",
    "    Q1 = [[0.0]*nActs for _ in range(nStates)]\n",
    "    Q2 = [[0.0]*nActs for _ in range(nStates)]\n",
    "\n",
    "    # 初始状态(无历史)\n",
    "    stateA= nActs\n",
    "    stateB= nActs\n",
    "\n",
    "    for ep in range(1, episodes+1):\n",
    "        # 1) 根据 stateA, stateB 选动作\n",
    "        iA = epsilon_greedy(Q1[stateA], epsilon)\n",
    "        iB = epsilon_greedy(Q2[stateB], epsilon)\n",
    "\n",
    "        # 2) payoff\n",
    "        stratA= actions[iA]\n",
    "        stratB= actions[iB]\n",
    "        rA, rB= payoff_blotto(stratA, stratB)\n",
    "\n",
    "        # 3) 更新 Q\n",
    "        oldQ1= Q1[stateA][iA]\n",
    "        oldQ2= Q2[stateB][iB]\n",
    "        Q1[stateA][iA] = oldQ1 + alpha*(rA - oldQ1)\n",
    "        Q2[stateB][iB] = oldQ2 + alpha*(rB - oldQ2)\n",
    "\n",
    "        # 4) 转移\n",
    "        nextStateA= iB\n",
    "        nextStateB= iA\n",
    "        stateA= nextStateA\n",
    "        stateB= nextStateB\n",
    "\n",
    "        # (可选)每隔一段减少epsilon等\n",
    "        if ep%50000==0:\n",
    "            epsilon= max(0.01, epsilon*0.5)\n",
    "            alpha= max(0.01, alpha*0.5)\n",
    "            print(f\"episode={ep}, stateA={stateA}, iA={iA}, rA={rA}, Q1={Q1[stateA][iA]:.2f}\")\n",
    "            # print( ... )\n",
    "\n",
    "    return Q1, Q2, actions\n",
    "\n",
    "def build_payoff_matrices(actions):\n",
    "    \"\"\"\n",
    "    构造 payoff矩阵 A[i][j], B[i][j]:\n",
    "      A[i][j]: A用 i, B用 j, A的得分\n",
    "      B[i][j]: B的得分\n",
    "    \"\"\"\n",
    "    n= len(actions)\n",
    "    A= [[0]*n for _ in range(n)]\n",
    "    B= [[0]*n for _ in range(n)]\n",
    "    for i, a in enumerate(actions):\n",
    "        for j, b in enumerate(actions):\n",
    "            rA, rB= payoff_blotto(a,b)\n",
    "            A[i][j]= rA\n",
    "            B[i][j]= rB\n",
    "    return A,B\n",
    "\n",
    "def check_NE(A,B, p, q, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    检查( p, q )是否(近似)纳什均衡:\n",
    "       U_A(p,q)>= U_A(i,q), ∀i; \n",
    "       U_B(p,q)>= U_B(p,j), ∀j; \n",
    "       对支撑集中动作收益= UA(p,q) or UB(p,q).\n",
    "    \"\"\"\n",
    "    n= len(A)\n",
    "    # 先算 UA_pq, UB_pq\n",
    "    UA_pq=0; UB_pq=0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            UA_pq+= p[i]*q[j]* A[i][j]\n",
    "            UB_pq+= p[i]*q[j]* B[i][j]\n",
    "    # A\n",
    "    for i in range(n):\n",
    "        U_iq= sum(q[j]*A[i][j] for j in range(n))\n",
    "        if U_iq> UA_pq+epsilon:\n",
    "            print(f\"[A can deviate] i={i}, payoff_iq={U_iq:.3f} > UA_pq={UA_pq:.3f}\")\n",
    "            return False\n",
    "        if p[i]>epsilon:\n",
    "            if abs(U_iq-UA_pq)>1e-3:\n",
    "                print(f\"[A support mismatch] i={i}, payoff_iq={U_iq:.3f}, UA={UA_pq:.3f}\")\n",
    "                return False\n",
    "    # B\n",
    "    for j in range(n):\n",
    "        U_pj= sum(p[i]*B[i][j] for i in range(n))\n",
    "        if U_pj>UB_pq+epsilon:\n",
    "            print(f\"[B can deviate] j={j}, payoff_pj={U_pj:.3f} > UB_pq={UB_pq:.3f}\")\n",
    "            return False\n",
    "        if q[j]>epsilon:\n",
    "            if abs(U_pj-UB_pq)>1e-3:\n",
    "                print(f\"[B support mismatch] j={j}, payoff_pj={U_pj:.3f}, UB={UB_pq:.3f}\")\n",
    "                return False\n",
    "    print(\"=> (p,q) is near NE.\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    random.seed()\n",
    "    S,N=5,3\n",
    "    alpha=0.1\n",
    "    epsilon=0.1\n",
    "    episodes=20000000\n",
    "\n",
    "    print(f\"Multi-state Q-Learning: S={S}, N={N}, episodes={episodes}\")\n",
    "    # 1) 运行多状态 Q-Learning\n",
    "    Q1, Q2, actions= q_learning_blotto_multi_state(S,N, alpha,epsilon, episodes)\n",
    "    nActs= len(actions)\n",
    "    print(f\"Number of actions= {nActs} (should be 21)\")\n",
    "\n",
    "    # 2) 在对战(执行)时, 仍需要 \"stateA= B 上次动作, stateB= A 上次动作\".\n",
    "    # 但是若只想看看Q表: Q1[state][action], Q2[state][action].\n",
    "    # 例如, 如果对手上一回合动作= iOpp=0, 我可以取 argmax(Q1[iOpp]) 之类。\n",
    "\n",
    "    # 这里简单地: 取 \"无历史\" state= nActs 行 => 选 Q值最高的动作\n",
    "    stateNoHist= nActs\n",
    "    bestA= max(range(nActs), key=lambda a: Q1[stateNoHist][a])\n",
    "    bestB= max(range(nActs), key=lambda b: Q2[stateNoHist][b])\n",
    "    print(f\"\\nFrom 'no-history' state perspective, bestA= {bestA}, bestB= {bestB}\")\n",
    "    print(f\"A strategy= {actions[bestA]}, B strategy= {actions[bestB]}\")\n",
    "\n",
    "    # 如果要构造 \"混合策略\" for \"no-history\" state,可做softmax:\n",
    "    def softmax(Q_row, tau=0.01):\n",
    "        exps= [math.exp(qv/tau) for qv in Q_row]\n",
    "        s= sum(exps)\n",
    "        return [e/s for e in exps]\n",
    "\n",
    "    pA= softmax(Q1[stateNoHist], tau=0.01)\n",
    "    pB= softmax(Q2[stateNoHist], tau=0.01)\n",
    "    print(\"\\nSoftmax distribution from 'no-history' state (prob>0.01):\")\n",
    "    for i,pr in enumerate(pA):\n",
    "        if pr>0.01:\n",
    "            print(f\" A: i={i}, prob={pr:.3f}, {actions[i]}\")\n",
    "    for j,pr in enumerate(pB):\n",
    "        if pr>0.01:\n",
    "            print(f\" B: j={j}, prob={pr:.3f}, {actions[j]}\")\n",
    "\n",
    "    # 3) 检测 (pA,pB) 是否近似 NE:\n",
    "    # 构造 payoff matrix\n",
    "    A_mat, B_mat= build_payoff_matrices(actions)\n",
    "    print(\"\\nCheck NE for the softmax distribution at 'no-history' state:\")\n",
    "    is_ne= check_NE(A_mat,B_mat, pA, pB, epsilon=1e-3)\n",
    "    print(\"Conclusion:\", \"Is NE.\" if is_ne else \"Not NE.\")\n",
    "\n",
    "if __name__==\"__main__\":。\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08220a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax value= 13.468\n",
      "A's minmax distribution (prob>0.01):\n",
      "  a=2, prob=0.106, act=(0, 2, 3)\n",
      "  a=3, prob=0.073, act=(0, 3, 2)\n",
      "  a=6, prob=0.052, act=(1, 0, 4)\n",
      "  a=7, prob=0.123, act=(1, 1, 3)\n",
      "  a=9, prob=0.105, act=(1, 3, 1)\n",
      "  a=13, prob=0.045, act=(2, 2, 1)\n",
      "  a=14, prob=0.100, act=(2, 3, 0)\n",
      "  a=15, prob=0.249, act=(3, 0, 2)\n",
      "  a=17, prob=0.148, act=(3, 2, 0)\n",
      "B's minmax value= -13.468, distribution(prob>0.01):\n",
      "  b=2, prob=0.194, act=(0, 2, 3)\n",
      "  b=3, prob=0.066, act=(0, 3, 2)\n",
      "  b=7, prob=0.055, act=(1, 1, 3)\n",
      "  b=9, prob=0.182, act=(1, 3, 1)\n",
      "  b=11, prob=0.169, act=(2, 0, 3)\n",
      "  b=14, prob=0.066, act=(2, 3, 0)\n",
      "  b=15, prob=0.033, act=(3, 0, 2)\n",
      "  b=16, prob=0.223, act=(3, 1, 1)\n",
      "  b=19, prob=0.012, act=(4, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "#强化学习纳什均衡，使用已有的qminmax算法完成\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def solve_minimax_Q(Q):\n",
    "    \"\"\"\n",
    "    给定 Q: shape=(nA, nB),\n",
    "    计算 v = max_{x} min_{b} sum_{a} x[a]*Q[a,b],\n",
    "    并返回 (v, x) 其中 x是A的最优混合策略, size=nA.\n",
    "\n",
    "    用线性规划:\n",
    "      maximize v\n",
    "      s.t. sum_a x[a] =1, x[a]>=0\n",
    "           sum_a x[a]*Q[a,b] >= v, 对所有 b\n",
    "    \"\"\"\n",
    "    nA, nB = Q.shape\n",
    "\n",
    "    # linprog: maximize c^T z =>  变成 minimize -c^T z\n",
    "    # 这里 变量= x[0..nA-1], v(最后1个)\n",
    "    # => x共 nA, plus 1 var => v\n",
    "    # => total nA+1 dimension\n",
    "    # decision var order: z= (x[0],...,x[nA-1], v)\n",
    "\n",
    "    # object: max v => min -v => c= [0,0,..., -1]\n",
    "    c= [0.0]*nA + [-1.0]\n",
    "\n",
    "    # 约束:\n",
    "    # 1) sum_a x[a] =1 => A eq\n",
    "    # 2) x[a]>=0 => A ineq\n",
    "    # 3) sum_a x[a]*Q[a,b] - v >=0 => => sum_a x[a]*Q[a,b] >= v\n",
    "    #    => - sum_a x[a]*Q[a,b] + v <=0\n",
    "\n",
    "    # build A_ub, b_ub, A_eq, b_eq\n",
    "    # eq: sum_a x[a]=1 => \n",
    "    # => sum_a x[a]=1 => A_eq= [ [1,1,...,1, 0], ...], b_eq=[1]\n",
    "    A_eq= np.zeros((1, nA+1))\n",
    "    A_eq[0,:nA]=1.0\n",
    "    b_eq= np.array([1.0])\n",
    "\n",
    "    # ineq x[a]>=0 => z[a]>=0 => not used in A_ub,b_ub but in bounds\n",
    "    # main ineq: for each b, sum_a x[a]*Q[a,b] - v >=0\n",
    "    # => - sum_a x[a]*Q[a,b] + v <=0\n",
    "    # => v - sum_a x[a]*Q[a,b] <=0\n",
    "    # => v + sum_a(-Q[a,b])* x[a] <=0 ??? we want <= 0\n",
    "    # We'll do: v - sum_a( x[a]*Q[a,b]) <= 0\n",
    "    # => sum_a (-Q[a,b]) x[a] + 1*v <=0\n",
    "    # but we want 0 on the right => so b_ub= 0\n",
    "    # => row b => [ -Q[0,b], -Q[1,b],..., -Q[nA-1,b], 1]\n",
    "    # bounds= <=0\n",
    "    A_ub= []\n",
    "    b_ub= []\n",
    "    for b in range(nB):\n",
    "        row= [-Q[a,b] for a in range(nA)] + [1.0]\n",
    "        A_ub.append(row)\n",
    "        b_ub.append(0.0)\n",
    "\n",
    "    A_ub= np.array(A_ub)\n",
    "    b_ub= np.array(b_ub)\n",
    "\n",
    "    # Bounds:\n",
    "    # x[a]>=0, v free?\n",
    "    #  -> x[a]>=0 => (0, None)\n",
    "    # v can be any real => (None, None), but for zero-sum payoff v>0 maybe?\n",
    "    # let's do v in (None, None) to be safe\n",
    "    bounds= [(0,None)]*nA + [(None,None)]\n",
    "\n",
    "    res= linprog(c= c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds,\n",
    "                 method=\"highs\") \n",
    "    if res.success:\n",
    "        z= res.x\n",
    "        x= z[:nA]\n",
    "        v= -res.fun  # cause we did minimize -v\n",
    "        return v, x\n",
    "    else:\n",
    "        # fallback or error\n",
    "        return None, None\n",
    "\n",
    "def minmaxQ_learn(nA, nB, alpha=0.1, gamma=0.9, episodes=50000, payoff=None):\n",
    "    \"\"\"\n",
    "    Minimax-Q for a 2-player zero-sum repeated game(only 1 state).\n",
    "    Q shape= (nA x nB).\n",
    "    payoff[a,b] => immediate reward for A, B=-payoff[a,b].\n",
    "    Return Q.\n",
    "    \"\"\"\n",
    "    Q= np.zeros((nA,nB))  # Q(a,b)\n",
    "    # random or 0 init\n",
    "\n",
    "    for t in range(episodes):\n",
    "        # 1) A,B 选动作 => 这里可简单 random exploration\n",
    "        a= random.randint(0,nA-1)\n",
    "        b= random.randint(0,nB-1)\n",
    "        r= payoff[a,b]  # A's reward\n",
    "        # 2) 先计算 \"val\" =  max_x min_b sum_a x[a]*Q[a,b]\n",
    "        val, _= solve_minimax_Q(Q)\n",
    "        if val is None:\n",
    "            continue  # skip or break\n",
    "\n",
    "        # 3) update Q[a,b]\n",
    "        oldQ= Q[a,b]\n",
    "        newQ= oldQ + alpha*(r + gamma* val - oldQ)\n",
    "        Q[a,b]= newQ\n",
    "\n",
    "    return Q\n",
    "\n",
    "# ===========  小例子: N=3, S=5 ColonelBlotto(21 x 21) ============\n",
    "\n",
    "def get_blotto_payoff_mat(S=5,N=3):\n",
    "    \"\"\"\n",
    "    返回 payoff 矩阵 M shape=(nA x nA),\n",
    "    M[a,b] = A 的收益( zero-sum => B = -M[a,b] ).\n",
    "    \"\"\"\n",
    "    actions= get_all_strategies(S,N)\n",
    "    n= len(actions)\n",
    "    M= np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            scoreA, scoreB= payoff_blotto(actions[i], actions[j])\n",
    "            M[i,j]= scoreA  # B=scoreB= sum -scoreA in zero-sum?\n",
    "    return M, actions\n",
    "\n",
    "def example_blotto_qminmax():\n",
    "    S,N=5,3\n",
    "    M, acts= get_blotto_payoff_mat(S,N)\n",
    "    nA= len(acts)\n",
    "    # M[i,j] = A's reward if (a=i,b=j).\n",
    "    # 1-state => Q shape= nA x nA\n",
    "    # => Q= minmaxQ_learn(nA,nA, payoff=M,...)\n",
    "    Q= minmaxQ_learn(nA, nA, alpha=0.1, gamma=0.9, episodes=100000, payoff=M)\n",
    "\n",
    "    # 最后由 Q 求 minmax strategy => solve_minimax_Q\n",
    "    val, x= solve_minimax_Q(Q)\n",
    "    print(f\"Minimax value= {val:.3f}\")\n",
    "    print(\"A's minmax distribution (prob>0.01):\")\n",
    "    for i,pr in enumerate(x):\n",
    "        if pr>0.01:\n",
    "            print(f\"  a={i}, prob={pr:.3f}, act={acts[i]}\")\n",
    "    # B 类 best response => B 选 argmin? or do the same solve on -Q for B side\n",
    "\n",
    "    # B's distribution => if truly zero-sum, we can do solve_minimax_Q(-Q^T)? \n",
    "    # or we skip?  In practice can also do solve_minimax_Q on Q^T w/ negative sign\n",
    "    # let's do quick hack:\n",
    "    Q_B= -Q.T  # B sees payoff= -A\n",
    "    valB, y= solve_minimax_Q(Q_B) \n",
    "    print(f\"B's minmax value= {valB:.3f}, distribution(prob>0.01):\")\n",
    "    for j,pb in enumerate(y):\n",
    "        if pb>0.01:\n",
    "            print(f\"  b={j}, prob={pb:.3f}, act={acts[j]}\")\n",
    "\n",
    "def main():\n",
    "    random.seed()\n",
    "    example_blotto_qminmax()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bec47eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Q-Minimax for Colonel Blotto (S=5,N=3), episodes=200000\n",
      "t=10000, v=-0.007, a=2, b=7, r=0.00, Q[2,7]=-0.01\n",
      "t=20000, v=-0.007, a=14, b=9, r=0.00, Q[14,9]=-0.01\n",
      "t=30000, v=-0.007, a=15, b=2, r=-1.00, Q[15,2]=-1.01\n",
      "t=40000, v=-0.007, a=15, b=2, r=-1.00, Q[15,2]=-1.01\n",
      "t=50000, v=-0.007, a=2, b=14, r=-1.00, Q[2,14]=-1.01\n",
      "t=60000, v=-0.007, a=14, b=14, r=0.00, Q[14,14]=-0.01\n",
      "t=70000, v=-0.007, a=15, b=7, r=-1.00, Q[15,7]=-1.01\n",
      "t=80000, v=-0.007, a=14, b=3, r=0.00, Q[14,3]=-0.01\n",
      "t=90000, v=-0.007, a=15, b=9, r=1.00, Q[15,9]=0.99\n",
      "t=100000, v=-0.007, a=2, b=16, r=1.00, Q[2,16]=0.99\n",
      "t=110000, v=-0.007, a=2, b=2, r=0.00, Q[2,2]=-0.01\n",
      "t=120000, v=-0.007, a=14, b=14, r=0.00, Q[14,14]=-0.01\n",
      "t=130000, v=-0.007, a=14, b=9, r=0.00, Q[14,9]=-0.01\n",
      "t=140000, v=-0.007, a=14, b=2, r=1.00, Q[14,2]=0.99\n",
      "t=150000, v=-0.007, a=15, b=3, r=0.00, Q[15,3]=-0.01\n",
      "t=160000, v=-0.007, a=15, b=3, r=0.00, Q[15,3]=-0.01\n",
      "t=170000, v=-0.007, a=2, b=14, r=-1.00, Q[2,14]=-1.01\n",
      "t=180000, v=-0.007, a=14, b=14, r=0.00, Q[14,14]=-0.01\n",
      "t=190000, v=-0.007, a=15, b=3, r=0.00, Q[15,3]=-0.01\n",
      "t=200000, v=-0.007, a=15, b=14, r=1.00, Q[15,14]=0.99\n",
      "Final Q sample row0: [0.0, 0.0, 0.0, -0.000483741234173469, 0.0]  ...\n",
      "\n",
      "Minimax value= -0.007, row p*(>0.01), col q*(>0.01)\n",
      " A: i=2, prob=0.333, (0, 2, 3)\n",
      " A: i=14, prob=0.333, (2, 3, 0)\n",
      " A: i=15, prob=0.333, (3, 0, 2)\n",
      " B: j=2, prob=0.143, (0, 2, 3)\n",
      " B: j=3, prob=0.143, (0, 3, 2)\n",
      " B: j=7, prob=0.143, (1, 1, 3)\n",
      " B: j=9, prob=0.143, (1, 3, 1)\n",
      " B: j=11, prob=0.143, (2, 0, 3)\n",
      " B: j=14, prob=0.143, (2, 3, 0)\n",
      " B: j=16, prob=0.143, (3, 1, 1)\n",
      "[ 0.          0.          0.33333333 -0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.\n",
      " -0.          0.          0.33333333  0.33333333  0.         -0.\n",
      "  0.          0.          0.        ]\n",
      "[0.0, 0.0, 0.14285714285714285, 0.14285714285714285, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.14285714285714285, 0.0, 0.14285714285714285, 0.0, 0.0, 0.14285714285714285, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Check NE for (p*,q*) from Q-minmax result:\n",
      "[A support mismatch] i=2, payoff_iq=-0.143, UA=0.000\n",
      "Conclusion: not NE\n"
     ]
    }
   ],
   "source": [
    "#强化学习纳什均衡，使用已有的qminmax算法完成\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    枚举所有将 S 个兵力分配到 N 个战场的纯策略 (x1,x2,...,xN).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def backtrack(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            backtrack(now+[i], left-i, slots-1)\n",
    "    backtrack([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_blotto(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    Blotto计分: N=3\n",
    "      如果 a_i > b_i => A得1分, a_i < b_i => B得1分, a_i=b_i => 各0.5\n",
    "    返回(scoreA,scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB=0.0,0.0\n",
    "    for a,b in zip(strategyA, strategyB):\n",
    "        if a>b:\n",
    "            scoreA+=1\n",
    "        elif a<b:\n",
    "            scoreB+=1\n",
    "        else:\n",
    "            scoreA+=0.5\n",
    "            scoreB+=0.5\n",
    "    return(scoreA,scoreB)\n",
    "\n",
    "def payoff_zero_sum(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    将Blotto得分转成A的收益(零和):\n",
    "      r = scoreA - scoreB\n",
    "    B的收益= -r\n",
    "    \"\"\"\n",
    "    scA, scB= payoff_blotto(strategyA, strategyB)\n",
    "    return (scA - scB)\n",
    "\n",
    "def solve_minmax(Q):\n",
    "    \"\"\"\n",
    "    在零和情形下, 给定行玩家的 payoff 矩阵 Q (shape=(nA,nB)),\n",
    "    通过线性规划求解 row player 的极大极小问题:\n",
    "      max_{p>=0, sum p=1}  v\n",
    "      s.t. sum_a p_a * Q[a,b] >= v, for all b.\n",
    "    返回 (v, p*, q*) 或 (0,None,None) 如果失败.\n",
    "    \"\"\"\n",
    "    nA, nB = Q.shape\n",
    "    # linprog: minimize c^T x\n",
    "    # 我们要 maximize v => minimize -v\n",
    "    # x= [p_0..p_{nA-1}, v]\n",
    "    c= [0.0]*nA + [-1.0]  # -v\n",
    "\n",
    "    # 约束1) sum p_a=1 => A_eq, b_eq\n",
    "    A_eq= [[1.0]*nA + [0.0]]\n",
    "    b_eq= [1.0]\n",
    "\n",
    "    # 约束2) p_a>=0 => bounds\n",
    "    # 约束3) sum_a p_a*Q[a,b] >= v => sum_a p_a*Q[a,b] - v >=0 => \n",
    "    # => -sum_a p_a*Q[a,b] + v <=0\n",
    "    A_ub= []\n",
    "    b_ub= []\n",
    "    for b in range(nB):\n",
    "        row_ub= []\n",
    "        for a in range(nA):\n",
    "            row_ub.append(-Q[a,b]) \n",
    "        row_ub.append(1.0)  # + v\n",
    "        A_ub.append(row_ub)\n",
    "        b_ub.append(0.0)\n",
    "\n",
    "    bounds= []\n",
    "    for a in range(nA):\n",
    "        bounds.append((0,None))  # p[a]>=0\n",
    "    bounds.append((None,None))   # v free\n",
    "\n",
    "    res= linprog(c, A_ub, b_ub, A_eq, b_eq, bounds, method='highs')\n",
    "    if res.success:\n",
    "        x= res.x\n",
    "        p= x[:nA]\n",
    "        v= x[nA]\n",
    "        # 计算 q* (列玩家策略) => \n",
    "        #   row payoff= p^T Q => want col player min => do a small LP or dual\n",
    "        # 这里做个最简单argmin => 可能只给出\"纯\" best response, 并不一定是混合. \n",
    "        # 为了完整, 我们再做一个 col-lp\n",
    "        # For demonstration we do a quick approach:\n",
    "        R= [0.0]*nB\n",
    "        for b in range(nB):\n",
    "            val=0.0\n",
    "            for a in range(nA):\n",
    "                val+= p[a]*Q[a,b]\n",
    "            R[b]= val\n",
    "        minR= min(R)\n",
    "        bestBs= [b for b in range(nB) if abs(R[b]-minR)<1e-9]\n",
    "        q= [0.0]*nB\n",
    "        for b in bestBs:\n",
    "            q[b]= 1.0/len(bestBs)\n",
    "        return (v, p, q)\n",
    "    else:\n",
    "        # fallback\n",
    "        return (0, None, None)\n",
    "\n",
    "def qminmax_blotto(S=5, N=3, alpha=0.1, episodes=50000):\n",
    "    \"\"\"\n",
    "    Minimax-Q for single-state repeated zero-sum game, \n",
    "    with fallback if solve_minmax fails.\n",
    "    \"\"\"\n",
    "    actions= get_all_strategies(S,N)\n",
    "    nA= len(actions)\n",
    "    Q= [[0.0]*nA for _ in range(nA)]  # row player payoff Q[a,b]\n",
    "\n",
    "    for t in range(1, episodes+1):\n",
    "        # 1) solve minmax\n",
    "        Q_mat= np.array(Q)\n",
    "        v, pstar, qstar= solve_minmax(Q_mat)\n",
    "        if pstar is None or qstar is None:\n",
    "            # fallback => use uniform\n",
    "            pstar= [1.0/nA]*nA\n",
    "            qstar= [1.0/nA]*nA\n",
    "            v= 0.0\n",
    "\n",
    "        # 2) sample from (pstar, qstar)\n",
    "        def sample_dist(prob):\n",
    "            r= random.random()\n",
    "            cum=0.0\n",
    "            for i,pp in enumerate(prob):\n",
    "                cum+=pp\n",
    "                if r<=cum:\n",
    "                    return i\n",
    "            return len(prob)-1\n",
    "        a= sample_dist(pstar)\n",
    "        b= sample_dist(qstar)\n",
    "\n",
    "        # 3) payoff\n",
    "        r= payoff_zero_sum(actions[a], actions[b])\n",
    "\n",
    "        # 4) update\n",
    "        oldQ= Q[a][b]\n",
    "        Q[a][b]= oldQ + alpha*( (r + v) - oldQ )\n",
    "\n",
    "        # 5) optional debug\n",
    "        if t%10000==0:\n",
    "            print(f\"t={t}, v={v:.3f}, a={a}, b={b}, r={r:.2f}, Q[{a},{b}]={Q[a][b]:.2f}\")\n",
    "    return Q, actions\n",
    "\n",
    "# ====== 构造 payoff 矩阵 & 检查 NE ======\n",
    "def build_payoff_matrices(actions):\n",
    "    n= len(actions)\n",
    "    A= [[0.0]*n for _ in range(n)]\n",
    "    B= [[0.0]*n for _ in range(n)]\n",
    "    for i,a in enumerate(actions):\n",
    "        for j,b in enumerate(actions):\n",
    "            # zero-sum payoff\n",
    "            r= payoff_zero_sum(a,b)\n",
    "            A[i][j]= r\n",
    "            B[i][j]= -r\n",
    "    return A,B\n",
    "\n",
    "def check_NE(A,B, p,q, epsilon=1e-5):\n",
    "    n= len(A)\n",
    "    UA_pq=0\n",
    "    UB_pq=0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            UA_pq+= p[i]*q[j]* A[i][j]\n",
    "            UB_pq+= p[i]*q[j]* B[i][j]\n",
    "    # A\n",
    "    for i in range(n):\n",
    "        Ui_q= sum(q[j]*A[i][j] for j in range(n))\n",
    "        if Ui_q>UA_pq+epsilon:\n",
    "            print(f\"[A deviate] i={i}, payoff_iq={Ui_q:.3f} > UA_pq={UA_pq:.3f}\")\n",
    "            return False\n",
    "        if p[i]>epsilon:\n",
    "            if abs(Ui_q -UA_pq)>1e-3:\n",
    "                print(f\"[A support mismatch] i={i}, payoff_iq={Ui_q:.3f}, UA={UA_pq:.3f}\")\n",
    "                return False\n",
    "    # B\n",
    "    for j in range(n):\n",
    "        Up_j= sum(p[i]*B[i][j] for i in range(n))\n",
    "        if Up_j>UB_pq+epsilon:\n",
    "            print(f\"[B deviate] j={j}, payoff_pj={Up_j:.3f} > UB_pq={UB_pq:.3f}\")\n",
    "            return False\n",
    "        if q[j]>epsilon:\n",
    "            if abs(Up_j -UB_pq)>1e-3:\n",
    "                print(f\"[B support mismatch] j={j}, payoff_pj={Up_j:.3f}, UB={UB_pq:.3f}\")\n",
    "                return False\n",
    "    print(\"=> (p,q) is near NE.\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    random.seed(8)\n",
    "    S,N=5,3\n",
    "    alpha=0.1\n",
    "    episodes=200000\n",
    "    print(f\"Running Q-Minimax for Colonel Blotto (S={S},N={N}), episodes={episodes}\")\n",
    "    Q, actions= qminmax_blotto(S,N, alpha=alpha, episodes=episodes)\n",
    "    nA= len(actions)\n",
    "\n",
    "    # final Q\n",
    "    print(\"Final Q sample row0:\", Q[0][:5], \" ...\")\n",
    "\n",
    "    # do a minmax solve\n",
    "    import numpy as np\n",
    "    Q_mat= np.array(Q)\n",
    "    v, pstar, qstar= solve_minmax(Q_mat)\n",
    "    if pstar is None or qstar is None:\n",
    "        print(\"Minmax solver failed at the end => fallback uniform p,q.\")\n",
    "        pstar=[1.0/nA]*nA\n",
    "        qstar=[1.0/nA]*nA\n",
    "        v=0.0\n",
    "    else:\n",
    "        print(f\"\\nMinimax value= {v:.3f}, row p*(>0.01), col q*(>0.01)\")\n",
    "        for i,pp in enumerate(pstar):\n",
    "            if pp>0.01:\n",
    "                print(f\" A: i={i}, prob={pp:.3f}, {actions[i]}\")\n",
    "        for j,qq in enumerate(qstar):\n",
    "            if qq>0.01:\n",
    "                print(f\" B: j={j}, prob={qq:.3f}, {actions[j]}\")\n",
    "\n",
    "    # NE check\n",
    "    print(pstar)\n",
    "    print(qstar)\n",
    "    A_mat,B_mat= build_payoff_matrices(actions)\n",
    "    print(\"\\nCheck NE for (p*,q*) from Q-minmax result:\")\n",
    "    is_ne= check_NE(A_mat,B_mat,pstar,qstar, epsilon=1e-1)\n",
    "    print(\"Conclusion:\", \"near NE\" if is_ne else \"not NE\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a2cc3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Q-Minimax for Colonel Blotto (S=5,N=3), episodes=20000\n",
      "t=10000, v=0.010, a=17, b=14, r=0.00, Q[17,14]=0.01\n",
      "t=20000, v=0.010, a=11, b=14, r=0.00, Q[11,14]=0.01\n",
      "Final Q sample row0: [0.0, 0.0, 0.0, 0.0, 0.0]  ...\n",
      "\n",
      "Minimax value= 0.010, row p*(>0.01), col q*(>0.01)\n",
      " A: i=3, prob=0.167, (0, 3, 2)\n",
      " A: i=7, prob=0.167, (1, 1, 3)\n",
      " A: i=9, prob=0.167, (1, 3, 1)\n",
      " A: i=11, prob=0.167, (2, 0, 3)\n",
      " A: i=16, prob=0.167, (3, 1, 1)\n",
      " A: i=17, prob=0.167, (3, 2, 0)\n",
      " B: j=2, prob=0.143, (0, 2, 3)\n",
      " B: j=3, prob=0.143, (0, 3, 2)\n",
      " B: j=11, prob=0.143, (2, 0, 3)\n",
      " B: j=13, prob=0.143, (2, 2, 1)\n",
      " B: j=14, prob=0.143, (2, 3, 0)\n",
      " B: j=15, prob=0.143, (3, 0, 2)\n",
      " B: j=16, prob=0.143, (3, 1, 1)\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 1.66667197e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.66666146e-01\n",
      " 0.00000000e+00 1.66665918e-01 3.81129397e-07 1.66666978e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.66666306e-01 1.66667074e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00]\n",
      "[0.0, 0.0, 0.14285714285714285, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Check NE for (p*,q*) from Q-minmax result:\n",
      "[A deviate] i=2, payoff_iq=0.143 > UA_pq=0.000\n",
      "Conclusion: not NE\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    枚举所有将 S 个兵力分配到 N 个战场的纯策略 (x1,x2,...,xN).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def backtrack(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            backtrack(now+[i], left-i, slots-1)\n",
    "    backtrack([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_blotto(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    Blotto计分: N=3\n",
    "      如果 a_i > b_i => A得1分, a_i < b_i => B得1分, a_i=b_i => 各0.5\n",
    "    返回(scoreA,scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB=0.0,0.0\n",
    "    for a,b in zip(strategyA, strategyB):\n",
    "        if a>b:\n",
    "            scoreA+=1\n",
    "        elif a<b:\n",
    "            scoreB+=1\n",
    "        else:\n",
    "            scoreA+=0.5\n",
    "            scoreB+=0.5\n",
    "    return(scoreA,scoreB)\n",
    "\n",
    "def payoff_zero_sum(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    将Blotto得分转成A的收益(零和):\n",
    "      r = scoreA - scoreB\n",
    "    B的收益= -r\n",
    "    \"\"\"\n",
    "    scA, scB= payoff_blotto(strategyA, strategyB)\n",
    "    return (scA - scB)\n",
    "\n",
    "def solve_minmax(Q):\n",
    "    \"\"\"\n",
    "    在零和情形下, 给定行玩家的 payoff 矩阵 Q (shape=(nA,nB)),\n",
    "    通过线性规划求解 row player 的极大极小问题:\n",
    "      max_{p>=0, sum p=1}  v\n",
    "      s.t. sum_a p_a * Q[a,b] >= v, for all b.\n",
    "    返回 (v, p*, q*) 或 (0,None,None) 如果失败.\n",
    "    \"\"\"\n",
    "    nA, nB = Q.shape\n",
    "    # linprog: minimize c^T x\n",
    "    # 我们要 maximize v => minimize -v\n",
    "    # x= [p_0..p_{nA-1}, v]\n",
    "    c= [0.0]*nA + [-1.0]  # -v\n",
    "\n",
    "    # 约束1) sum p_a=1 => A_eq, b_eq\n",
    "    A_eq= [[1.0]*nA + [0.0]]\n",
    "    b_eq= [1.0]\n",
    "\n",
    "    # 约束2) p_a>=0 => bounds\n",
    "    # 约束3) sum_a p_a*Q[a,b] >= v => sum_a p_a*Q[a,b] - v >=0 => \n",
    "    # => -sum_a p_a*Q[a,b] + v <=0\n",
    "    A_ub= []\n",
    "    b_ub= []\n",
    "    for b in range(nB):\n",
    "        row_ub= []\n",
    "        for a in range(nA):\n",
    "            row_ub.append(-Q[a,b]) \n",
    "        row_ub.append(1.0)  # + v\n",
    "        A_ub.append(row_ub)\n",
    "        b_ub.append(0.0)\n",
    "\n",
    "    bounds= []\n",
    "    for a in range(nA):\n",
    "        bounds.append((0,None))  # p[a]>=0\n",
    "    bounds.append((None,None))   # v free\n",
    "\n",
    "    res= linprog(c, A_ub, b_ub, A_eq, b_eq, bounds, method='highs')\n",
    "    if res.success:\n",
    "        x= res.x\n",
    "        p= x[:nA]\n",
    "        v= x[nA]\n",
    "        # 计算 q* (列玩家策略) => \n",
    "        #   row payoff= p^T Q => want col player min => do a small LP or dual\n",
    "        # 这里做个最简单argmin => 可能只给出\"纯\" best response, 并不一定是混合. \n",
    "        # 为了完整, 我们再做一个 col-lp\n",
    "        # For demonstration we do a quick approach:\n",
    "        R= [0.0]*nB\n",
    "        for b in range(nB):\n",
    "            val=0.0\n",
    "            for a in range(nA):\n",
    "                val+= p[a]*Q[a,b]\n",
    "            R[b]= val\n",
    "        minR= min(R)\n",
    "        bestBs= [b for b in range(nB) if abs(R[b]-minR)<1e-9]\n",
    "        q= [0.0]*nB\n",
    "        for b in bestBs:\n",
    "            q[b]= 1.0/len(bestBs)\n",
    "        return (v, p, q)\n",
    "    else:\n",
    "        # fallback\n",
    "        return (0, None, None)\n",
    "\n",
    "def qminmax_blotto(S=5, N=3, alpha=0.1, episodes=50000):\n",
    "    \"\"\"\n",
    "    Minimax-Q for single-state repeated zero-sum game, \n",
    "    with fallback if solve_minmax fails.\n",
    "    \"\"\"\n",
    "    actions= get_all_strategies(S,N)\n",
    "    nA= len(actions)\n",
    "    Q= [[0.0]*nA for _ in range(nA)]  # row player payoff Q[a,b]\n",
    "\n",
    "    for t in range(1, episodes+1):\n",
    "        # 1) solve minmax\n",
    "        Q_mat= np.array(Q)\n",
    "        v, pstar, qstar= solve_minmax(Q_mat)\n",
    "        if pstar is None or qstar is None:\n",
    "            # fallback => use uniform\n",
    "            pstar= [1.0/nA]*nA\n",
    "            qstar= [1.0/nA]*nA\n",
    "            v= 0.0\n",
    "\n",
    "        # 2) sample from (pstar, qstar)\n",
    "        def sample_dist(prob):\n",
    "            r= random.random()\n",
    "            cum=0.0\n",
    "            for i,pp in enumerate(prob):\n",
    "                cum+=pp\n",
    "                if r<=cum:\n",
    "                    return i\n",
    "            return len(prob)-1\n",
    "        a= sample_dist(pstar)\n",
    "        b= sample_dist(qstar)\n",
    "\n",
    "        # 3) payoff\n",
    "        r= payoff_zero_sum(actions[a], actions[b])\n",
    "\n",
    "        # 4) update\n",
    "        oldQ= Q[a][b]\n",
    "        Q[a][b]= oldQ + alpha*( (r + v) - oldQ )\n",
    "\n",
    "        # 5) optional debug\n",
    "        if t%10000==0:\n",
    "            print(f\"t={t}, v={v:.3f}, a={a}, b={b}, r={r:.2f}, Q[{a},{b}]={Q[a][b]:.2f}\")\n",
    "    return Q, actions\n",
    "\n",
    "# ====== 构造 payoff 矩阵 & 检查 NE ======\n",
    "def build_payoff_matrices(actions):\n",
    "    n= len(actions)\n",
    "    A= [[0.0]*n for _ in range(n)]\n",
    "    B= [[0.0]*n for _ in range(n)]\n",
    "    for i,a in enumerate(actions):\n",
    "        for j,b in enumerate(actions):\n",
    "            # zero-sum payoff\n",
    "            r= payoff_zero_sum(a,b)\n",
    "            A[i][j]= r\n",
    "            B[i][j]= -r\n",
    "    return A,B\n",
    "\n",
    "def check_NE(A,B, p,q, epsilon=1e-5):\n",
    "    n= len(A)\n",
    "    UA_pq=0\n",
    "    UB_pq=0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            UA_pq+= p[i]*q[j]* A[i][j]\n",
    "            UB_pq+= p[i]*q[j]* B[i][j]\n",
    "    # A\n",
    "    for i in range(n):\n",
    "        Ui_q= sum(q[j]*A[i][j] for j in range(n))\n",
    "        if Ui_q>UA_pq+epsilon:\n",
    "            print(f\"[A deviate] i={i}, payoff_iq={Ui_q:.3f} > UA_pq={UA_pq:.3f}\")\n",
    "            return False\n",
    "        if p[i]>epsilon:\n",
    "            if abs(Ui_q -UA_pq)>1e-3:\n",
    "                print(f\"[A support mismatch] i={i}, payoff_iq={Ui_q:.3f}, UA={UA_pq:.3f}\")\n",
    "                return False\n",
    "    # B\n",
    "    for j in range(n):\n",
    "        Up_j= sum(p[i]*B[i][j] for i in range(n))\n",
    "        if Up_j>UB_pq+epsilon:\n",
    "            print(f\"[B deviate] j={j}, payoff_pj={Up_j:.3f} > UB_pq={UB_pq:.3f}\")\n",
    "            return False\n",
    "        if q[j]>epsilon:\n",
    "            if abs(Up_j -UB_pq)>1e-3:\n",
    "                print(f\"[B support mismatch] j={j}, payoff_pj={Up_j:.3f}, UB={UB_pq:.3f}\")\n",
    "                return False\n",
    "    print(\"=> (p,q) is near NE.\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    random.seed(4)\n",
    "    S,N=5,3\n",
    "    alpha=0.1\n",
    "    episodes=20000\n",
    "    print(f\"Running Q-Minimax for Colonel Blotto (S={S},N={N}), episodes={episodes}\")\n",
    "    Q, actions= qminmax_blotto(S,N, alpha=alpha, episodes=episodes)\n",
    "    nA= len(actions)\n",
    "\n",
    "    # final Q\n",
    "    print(\"Final Q sample row0:\", Q[0][:5], \" ...\")\n",
    "\n",
    "    # do a minmax solve\n",
    "    import numpy as np\n",
    "    Q_mat= np.array(Q)\n",
    "    v, pstar, qstar= solve_minmax(Q_mat)\n",
    "    if pstar is None or qstar is None:\n",
    "        print(\"Minmax solver failed at the end => fallback uniform p,q.\")\n",
    "        pstar=[1.0/nA]*nA\n",
    "        qstar=[1.0/nA]*nA\n",
    "        v=0.0\n",
    "    else:\n",
    "        print(f\"\\nMinimax value= {v:.3f}, row p*(>0.01), col q*(>0.01)\")\n",
    "        for i,pp in enumerate(pstar):\n",
    "            if pp>0.01:\n",
    "                print(f\" A: i={i}, prob={pp:.3f}, {actions[i]}\")\n",
    "        for j,qq in enumerate(qstar):\n",
    "            if qq>0.01:\n",
    "                print(f\" B: j={j}, prob={qq:.3f}, {actions[j]}\")\n",
    "\n",
    "    # NE check\n",
    "    print(pstar)\n",
    "    print(qstar)\n",
    "    A_mat,B_mat= build_payoff_matrices(actions)\n",
    "    print(\"\\nCheck NE for (p*,q*) from Q-minmax result:\")\n",
    "    is_ne= check_NE(A_mat,B_mat,pstar,qstar, epsilon=1e-1)\n",
    "    print(\"Conclusion:\", \"near NE\" if is_ne else \"not NE\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "363af7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonel Blotto RL with L1-dist <=2 constraint, S=5,N=3, episodes=100000\n",
      "Finished RL. Checking final distributions..\n",
      "\n",
      "(A) time-average distribution ~ prob>0.01:\n",
      "  i=2, prob=0.213, dist=(0, 2, 3)\n",
      "  i=3, prob=0.043, dist=(0, 3, 2)\n",
      "  i=7, prob=0.077, dist=(1, 1, 3)\n",
      "  i=8, prob=0.342, dist=(1, 2, 2)\n",
      "  i=9, prob=0.088, dist=(1, 3, 1)\n",
      "  i=12, prob=0.056, dist=(2, 1, 2)\n",
      "  i=13, prob=0.041, dist=(2, 2, 1)\n",
      "  i=15, prob=0.058, dist=(3, 0, 2)\n",
      "  i=16, prob=0.034, dist=(3, 1, 1)\n",
      "  i=17, prob=0.011, dist=(3, 2, 0)\n",
      "\n",
      "(B) time-average distribution ~ prob>0.01:\n",
      "  j=2, prob=0.174, dist=(0, 2, 3)\n",
      "  j=3, prob=0.154, dist=(0, 3, 2)\n",
      "  j=4, prob=0.018, dist=(0, 4, 1)\n",
      "  j=6, prob=0.012, dist=(1, 0, 4)\n",
      "  j=7, prob=0.086, dist=(1, 1, 3)\n",
      "  j=8, prob=0.311, dist=(1, 2, 2)\n",
      "  j=9, prob=0.030, dist=(1, 3, 1)\n",
      "  j=13, prob=0.029, dist=(2, 2, 1)\n",
      "  j=14, prob=0.034, dist=(2, 3, 0)\n",
      "  j=15, prob=0.037, dist=(3, 0, 2)\n",
      "  j=16, prob=0.020, dist=(3, 1, 1)\n",
      "  j=17, prob=0.039, dist=(3, 2, 0)\n",
      "[A deviate] i=6, payoff=1.610, UA=1.499\n",
      "\n",
      "Check NE => No dev found => not near NE\n"
     ]
    }
   ],
   "source": [
    "#强化学习做推广的资源移动限制的游戏\n",
    "import random, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    列举所有把 S 个兵力分配到 N=3 个战场的纯策略 (x1, x2, ..., xN)，\n",
    "    其中 x1 + x2 + ... + xN = S, xi >= 0.\n",
    "    通常对于 (S=5, N=3) 会生成 21 个分配。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    def back(now, left, slots):\n",
    "        if slots == 1:\n",
    "            results.append(tuple(now + [left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            back(now + [i], left - i, slots - 1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def l1_distance(distA, distB):\n",
    "    \"\"\"\n",
    "    计算 L1 距离(曼哈顿距离): sum(|a - b|)\n",
    "    如果 distA=(x1,x2,x3), distB=(y1,y2,y3),\n",
    "    返回 |x1-y1| + |x2-y2| + |x3-y3|.\n",
    "    \"\"\"\n",
    "    return sum(abs(a-b) for a,b in zip(distA, distB))\n",
    "\n",
    "def payoff_blotto(stratA, stratB):\n",
    "    \"\"\"\n",
    "    Colonel Blotto 的计分规则:\n",
    "      - 若 A_i > B_i => A 得 1 分, B 得 0 分\n",
    "      - 若 A_i < B_i => B 得 1 分, A 得 0 分\n",
    "      - 若 A_i = B_i => A, B 各得 0.5 分\n",
    "    返回 (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB = 0.0, 0.0\n",
    "    for a, b in zip(stratA, stratB):\n",
    "        if a > b:\n",
    "            scoreA += 1\n",
    "        elif a < b:\n",
    "            scoreB += 1\n",
    "        else:\n",
    "            scoreA += 0.5\n",
    "            scoreB += 0.5\n",
    "    return scoreA, scoreB\n",
    "\n",
    "def init_adjacency(actions):\n",
    "    \"\"\"\n",
    "    预先计算每个分配下允许转移到的下一个分配(基于 L1距离 <= 2 的约束)。\n",
    "    actions 是一个长度 nA 的列表(每个元素是 (x1,x2,x3)), nA=21 对应 (S=5, N=3)。\n",
    "    返回一个字典 adj[d]，其中 d 是分配索引 => \n",
    "    adj[d] 是所有与 actions[d] 的L1距离 <=2 的分配索引列表。\n",
    "    \"\"\"\n",
    "    adj = {}\n",
    "    for i, distA in enumerate(actions):\n",
    "        neighbors = []\n",
    "        for j, distB in enumerate(actions):\n",
    "            if l1_distance(distA, distB) <= 2:\n",
    "                neighbors.append(j)\n",
    "        adj[i] = neighbors\n",
    "    return adj\n",
    "\n",
    "def eps_greedy(Qrow, feasible_actions, eps=0.1):\n",
    "    \"\"\"\n",
    "    在给定的 Qrow (比如 Q[state]) 里做 epsilon-greedy。 \n",
    "    但只能在 feasible_actions 中选(因为有“最多能移动 2 个兵力”的限制)。\n",
    "    English:\n",
    "      - Epsilon greedy on Qrow, restricted to feasible_actions.\n",
    "    \"\"\"\n",
    "    if random.random() < eps:\n",
    "        return random.choice(feasible_actions)\n",
    "    else:\n",
    "        # 从可行动作里找最大 Q 值\n",
    "        maxQ = max(Qrow[a] for a in feasible_actions)\n",
    "        cands = [a for a in feasible_actions if Qrow[a] == maxQ]\n",
    "        return random.choice(cands)\n",
    "\n",
    "def main():\n",
    "    random.seed(0)\n",
    "    S, N = 5, 3\n",
    "    alpha = 0.1\n",
    "    eps   = 0.1\n",
    "    episodes = 100000\n",
    "\n",
    "    print(f\"Colonel Blotto RL with L1-dist <=2 constraint, S={S},N={N}, episodes={episodes}\")\n",
    "\n",
    "    # 1) 生成全部分配(21个)\n",
    "    actions = get_all_strategies(S, N)  # ~21\n",
    "    nA = len(actions)\n",
    "    # adjacency: 对于每个分配, 哪些分配的 L1距离 <=2\n",
    "    adjacency = init_adjacency(actions)\n",
    "\n",
    "    # 2) 定义 Q 表: Q1[stateA][action], Q2[stateB][action]\n",
    "    #    stateA=上一次的分配编号(0..20), 若无历史可额外+1 行\n",
    "    #    这里我们只做 nA+1 => 其中 nA 用于表示无历史\n",
    "    Q1 = [ [0.0]*nA for _ in range(nA+1) ]\n",
    "    Q2 = [ [0.0]*nA for _ in range(nA+1) ]\n",
    "\n",
    "    # 当前状态(=上一次的分配索引)\n",
    "    stateA = nA  # 表示A无历史\n",
    "    stateB = nA  # B无历史\n",
    "\n",
    "    # 时间平均分布计数\n",
    "    distCountA = np.zeros(nA)\n",
    "    distCountB = np.zeros(nA)\n",
    "\n",
    "    for t in range(1, episodes+1):\n",
    "        # 对 A 来说, 可行动作是 adjacency[stateA], 如果 stateA<nA\n",
    "        # 如果 stateA==nA(无历史), 不限制 => range(nA)\n",
    "        feasA = adjacency[stateA] if stateA < nA else range(nA)\n",
    "        feasB = adjacency[stateB] if stateB < nA else range(nA)\n",
    "\n",
    "        # A, B 分别做 eps-greedy\n",
    "        iA = eps_greedy(Q1[stateA], feasA, eps)\n",
    "        iB = eps_greedy(Q2[stateB], feasB, eps)\n",
    "\n",
    "        # 得到即时收益\n",
    "        scA, scB = payoff_blotto(actions[iA], actions[iB])\n",
    "\n",
    "        # 更新 Q\n",
    "        oldQ1 = Q1[stateA][iA]\n",
    "        oldQ2 = Q2[stateB][iB]\n",
    "        Q1[stateA][iA] = oldQ1 + alpha*(scA - oldQ1)\n",
    "        Q2[stateB][iB] = oldQ2 + alpha*(scB - oldQ2)\n",
    "\n",
    "        # 状态转移: 下一个 stateA= iA\n",
    "        stateA = iA\n",
    "        stateB = iB\n",
    "\n",
    "        # 记录本回合A,B用的是谁(用于时间平均)\n",
    "        distCountA[iA] += 1\n",
    "        distCountB[iB] += 1\n",
    "\n",
    "        # 可选: 学习率, eps 动态衰减\n",
    "        if t % 10000 == 0:\n",
    "            eps *= 0.5\n",
    "\n",
    "    print(\"Finished RL. Checking final distributions..\")\n",
    "    finalA = distCountA / float(episodes)\n",
    "    finalB = distCountB / float(episodes)\n",
    "\n",
    "    # 打印 A,B 概率>0.01的分配\n",
    "    print(\"\\n(A) time-average distribution ~ prob>0.01:\")\n",
    "    for i, pr in enumerate(finalA):\n",
    "        if pr > 0.01:\n",
    "            print(f\"  i={i}, prob={pr:.3f}, dist={actions[i]}\")\n",
    "\n",
    "    print(\"\\n(B) time-average distribution ~ prob>0.01:\")\n",
    "    for j, pr in enumerate(finalB):\n",
    "        if pr > 0.01:\n",
    "            print(f\"  j={j}, prob={pr:.3f}, dist={actions[j]}\")\n",
    "\n",
    "    # ========== 检查 NE ==========\n",
    "    # 构造 A 矩阵, B 矩阵\n",
    "    n = nA\n",
    "    A_mat = np.zeros((n,n))\n",
    "    B_mat = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            scA, scB = payoff_blotto(actions[i], actions[j])\n",
    "            A_mat[i,j] = scA\n",
    "            B_mat[i,j] = scB\n",
    "\n",
    "    # 定义一个 NE 检测函数\n",
    "    def check_NE(A, B, p, q, eps=1e-1):\n",
    "        # 计算 U_A(p,q), U_B(p,q)\n",
    "        n = len(A)\n",
    "        UA=0.0\n",
    "        UB=0.0\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                UA += p[i]*q[j]* A[i,j]\n",
    "                UB += p[i]*q[j]* B[i,j]\n",
    "        # row dev\n",
    "        for i in range(n):\n",
    "            U_iq = sum(q[j]*A[i,j] for j in range(n))\n",
    "            if U_iq> UA+ eps:\n",
    "                print(f\"[A deviate] i={i}, payoff={U_iq:.3f}, UA={UA:.3f}\")\n",
    "                return False\n",
    "        # col dev\n",
    "        for j in range(n):\n",
    "            U_pj = sum(p[i]*B[i,j] for i in range(n))\n",
    "            if U_pj> UB+ eps:\n",
    "                print(f\"[B deviate] j={j}, payoff={U_pj:.3f}, UB={UB:.3f}\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    is_ne = check_NE(A_mat, B_mat, finalA, finalB)\n",
    "    print(\"\\nCheck NE =>\", \"Yes near NE\" if is_ne else \"No dev found => not near NE\")\n",
    "\n",
    "    # =========== 若还要检查 CE(相关均衡) ===========\n",
    "    # 我们需要记录每次(iA, iB) 的出现频率 => jointDist[i][j]，\n",
    "    # 再做相关均衡检验，这里可自行扩展。\n",
    "    # ...\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "692e7391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha-Beta result value= -1.000, bestAct for A= 0, strategy= (0, 0, 5)\n",
      "Then B picks= 0, strategy= (0, 0, 5), value= -1.000\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    列举所有把 S 个兵力分配到 N=3 个战场的纯策略 (x1,x2,x3) (sum= S).\n",
    "    对 (S=5, N=3) => 21 种.\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    def backtrack(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            backtrack(now+[i], left-i, slots-1)\n",
    "    backtrack([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_blotto(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    计算 Colonel Blotto 中 (A的分配, B的分配) 的得分 (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB=0.0, 0.0\n",
    "    for a, b in zip(strategyA, strategyB):\n",
    "        if a>b:\n",
    "            scoreA+=1\n",
    "        elif a<b:\n",
    "            scoreB+=1\n",
    "        else:\n",
    "            scoreA+=0.5\n",
    "            scoreB+=0.5\n",
    "    return (scoreA, scoreB)\n",
    "\n",
    "def evaluate_final(state, actions):\n",
    "    \"\"\"\n",
    "    当深度=4 时, state=[a0, b0, a1, b1] (示例), \n",
    "    这里仅取最后一对 (a1,b1) 的 payoff 作为评估.\n",
    "    或者也可以把 2 次对决累计, 这里示例只看最后一次.\n",
    "    \"\"\"\n",
    "    # state: e.g. [ idxA0, idxB0, idxA1, idxB1 ] (length=4)\n",
    "    # 取最后 A,B:\n",
    "    if len(state)<2:\n",
    "        # no meaningful\n",
    "        return 0.0\n",
    "    # 这里取 state[-2], state[-1]\n",
    "    iA= state[-2]\n",
    "    iB= state[-1]\n",
    "    stratA= actions[iA]\n",
    "    stratB= actions[iB]\n",
    "    scA, scB= payoff_blotto(stratA, stratB)\n",
    "    # 只返回 A 的得分, 视为AlphaBeta\n",
    "    return scA- scB   # 如果想 zero-sum, row player 的价值= scA- scB\n",
    "\n",
    "def alphaBeta(state, depth, alpha, beta, maximizingPlayer, actions, maxDepth=4):\n",
    "    \"\"\"\n",
    "    Alpha-Beta 剪枝:\n",
    "    state: list of move indices, e.g. [ iA, iB, iA, iB,... ]\n",
    "    depth: 当前深度\n",
    "    maximizingPlayer: True => 轮到 A, False => 轮到 B\n",
    "    actions: 21个动作, e.g. 0..20\n",
    "    maxDepth=4: 超过就评估\n",
    "    返回 (bestValue, bestAction) \n",
    "    \"\"\"\n",
    "    if depth>=maxDepth:\n",
    "        val= evaluate_final(state, actions)\n",
    "        return (val, None)\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        # A's turn\n",
    "        value= -math.inf\n",
    "        bestAct= None\n",
    "        for a in range(len(actions)):\n",
    "            # A选动作 a\n",
    "            # newState\n",
    "            newState= state + [a]\n",
    "            val,_= alphaBeta(newState, depth+1, alpha, beta, False, actions, maxDepth)\n",
    "            if val> value:\n",
    "                value= val\n",
    "                bestAct= a\n",
    "            alpha= max(alpha, value)\n",
    "            if beta<= alpha:\n",
    "                break\n",
    "        return (value, bestAct)\n",
    "    else:\n",
    "        # B's turn => min\n",
    "        value= math.inf\n",
    "        bestAct= None\n",
    "        for b in range(len(actions)):\n",
    "            newState= state + [b]\n",
    "            val,_= alphaBeta(newState, depth+1, alpha, beta, True, actions, maxDepth)\n",
    "            if val< value:\n",
    "                value= val\n",
    "                bestAct= b\n",
    "            beta= min(beta, value)\n",
    "            if beta<= alpha:\n",
    "                break\n",
    "        return (value, bestAct)\n",
    "\n",
    "def main():\n",
    "    S,N=5,3\n",
    "    actions= get_all_strategies(S,N)  # 21 actions\n",
    "    maxDepth=4\n",
    "    # rootState= []\n",
    "    # call alphaBeta => A moves first\n",
    "    val, actA= alphaBeta(state=[], depth=0, alpha=-math.inf, beta=math.inf,\n",
    "                         maximizingPlayer=True, actions=actions, maxDepth=maxDepth)\n",
    "    print(f\"Alpha-Beta result value= {val:.3f}, bestAct for A= {actA}, strategy= {actions[actA]}\")\n",
    "    # if we proceed => B responds:\n",
    "    # but we only do one step in example. If want the full line of play => we do a sim\n",
    "    lineState= [actA]\n",
    "    val2, actB= alphaBeta(lineState,1, -math.inf, math.inf, False, actions, maxDepth)\n",
    "    print(f\"Then B picks= {actB}, strategy= {actions[actB]}, value= {val2:.3f}\")\n",
    "    lineState2= lineState+[actB]\n",
    "    # keep going up to depth=4\n",
    "\n",
    "    # ...\n",
    "    # after we get a final sequence, we know final payoff => check if near NE?\n",
    "    # This is purely a demonstration\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2c825fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Alpha-Beta each turn, with depth=2, repeated for some rounds. S=5,N=3, #actions=21.\n",
      "(i=0, j=7), prob=1.000, A= (0, 0, 5), B=(1, 1, 3)\n",
      "[A violates CE]: i=0, i'=1, diff=0.500\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "# -------------- Enumerate distributions ---------------\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"All ways to allocate S troops into N=3 fields => ~21 combos.\"\"\"\n",
    "    results=[]\n",
    "    def backtrack(cur, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(cur+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            backtrack(cur+[i], left-i, slots-1)\n",
    "    backtrack([], S, N)\n",
    "    return results\n",
    "\n",
    "# -------------- payoff for Blotto --------------\n",
    "def payoff_blotto(stratA, stratB):\n",
    "    \"\"\"\n",
    "    Compare each battlefield:\n",
    "      if a>b => A++,\n",
    "      if a<b => B++,\n",
    "      else => A+=0.5, B+=0.5\n",
    "    Return (A_score, B_score).\n",
    "    \"\"\"\n",
    "    scA, scB= 0.0, 0.0\n",
    "    for a,b in zip(stratA, stratB):\n",
    "        if a>b:\n",
    "            scA+=1\n",
    "        elif a<b:\n",
    "            scB+=1\n",
    "        else:\n",
    "            scA+=0.5\n",
    "            scB+=0.5\n",
    "    return (scA, scB)\n",
    "\n",
    "# -------------- Evaluate state (leaf) --------------\n",
    "def evaluate_state(moves, actions):\n",
    "    \"\"\"\n",
    "    Suppose we have a small game tree: \n",
    "      - A moves, then B moves, up to depth=2 or 4, etc.\n",
    "    The final payoff is from the last (iA, iB).\n",
    "    For a depth=2 scenario: moves= [iA, iB].\n",
    "    Return value from A's perspective => (scoreA - scoreB).\n",
    "    \"\"\"\n",
    "    if len(moves)<2:\n",
    "        return 0.0\n",
    "    iA= moves[-2]\n",
    "    iB= moves[-1]\n",
    "    sA, sB= payoff_blotto(actions[iA], actions[iB])\n",
    "    return sA - sB  # zero-sum perspective => A's net\n",
    "\n",
    "# -------------- AlphaBeta for A --------------\n",
    "def alphaBetaA(movesSoFar, depth, maxDepth, alpha, beta, actions):\n",
    "    \"\"\"\n",
    "    A tries to maximize (scoreA - scoreB).\n",
    "    depth= current depth, if depth>= maxDepth => evaluate.\n",
    "    movesSoFar => [iA, iB, iA, iB,...].\n",
    "    Return (bestValue, bestAction).\n",
    "    \"\"\"\n",
    "    if depth>= maxDepth:\n",
    "        val= evaluate_state(movesSoFar, actions)\n",
    "        return (val, None)\n",
    "    # A's turn => we try all possible actions\n",
    "    n= len(actions)\n",
    "    bestVal= -math.inf\n",
    "    bestAct= None\n",
    "    for iA in range(n):\n",
    "        newState= movesSoFar+[ iA ]\n",
    "        # then B's turn => call alphaBetaB\n",
    "        valB, _= alphaBetaB(newState, depth+1, maxDepth, alpha, beta, actions)\n",
    "        if valB> bestVal:\n",
    "            bestVal= valB\n",
    "            bestAct= iA\n",
    "        alpha= max(alpha, bestVal)\n",
    "        if beta<= alpha:\n",
    "            break\n",
    "    return (bestVal, bestAct)\n",
    "\n",
    "def alphaBetaB(movesSoFar, depth, maxDepth, alpha, beta, actions):\n",
    "    \"\"\"\n",
    "    B tries to minimize (scoreA - scoreB), or equivalently maximize (scoreB -scoreA).\n",
    "    But we can keep the perspective: B wants to minimize 'value= (A-B)' => so do min search.\n",
    "    \"\"\"\n",
    "    if depth>= maxDepth:\n",
    "        val= evaluate_state(movesSoFar, actions)\n",
    "        return (val, None)\n",
    "    n= len(actions)\n",
    "    bestVal= math.inf\n",
    "    bestAct= None\n",
    "    for iB in range(n):\n",
    "        newState= movesSoFar+[ iB ]\n",
    "        # next => alphaBetaA\n",
    "        valA, _= alphaBetaA(newState, depth+1, maxDepth, alpha, beta, actions)\n",
    "        if valA< bestVal:\n",
    "            bestVal= valA\n",
    "            bestAct= iB\n",
    "        beta= min(beta, bestVal)\n",
    "        if beta<= alpha:\n",
    "            break\n",
    "    return (bestVal, bestAct)\n",
    "\n",
    "# -------------- Main repeated loop --------------\n",
    "\n",
    "def build_payoff_matrices(actions):\n",
    "    \"\"\"\n",
    "    A[i][j]= payoff(A= i, B=j) for row player => sA\n",
    "    B[i][j]= payoff for B => sB\n",
    "    \"\"\"\n",
    "    n= len(actions)\n",
    "    A= np.zeros((n,n))\n",
    "    B= np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            scA, scB= payoff_blotto(actions[i], actions[j])\n",
    "            A[i,j]= scA\n",
    "            B[i,j]= scB\n",
    "    return A,B\n",
    "\n",
    "def check_CE(A,B, jointDist, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Check correlated eq => for A: condition on i => no beneficial dev i-> i'\n",
    "    for B: condition on j => no beneficial dev j-> j'.\n",
    "    jointDist shape= n x n, sum=1\n",
    "    \"\"\"\n",
    "    n= len(A)\n",
    "    total= np.sum(jointDist)\n",
    "    if abs(total-1.0)> 1e-5:\n",
    "        print(f\"[Warning] jointDist sum={total:.3f} not=1?\")\n",
    "    for i in range(n):\n",
    "        # Probability that A is recommended i => mu_i= sum_{j} jointDist[i,j]\n",
    "        mu_i= np.sum(jointDist[i,:])\n",
    "        if mu_i< 1e-15:\n",
    "            continue\n",
    "        # E[uA| i], E[uA| i-> i']\n",
    "        oldPay=0.0\n",
    "        for j in range(n):\n",
    "            oldPay+= jointDist[i,j]* A[i,j]\n",
    "        oldPay/= mu_i\n",
    "        for i2 in range(n):\n",
    "            if i2== i:\n",
    "                continue\n",
    "            newPay= 0.0\n",
    "            for j in range(n):\n",
    "                newPay+= jointDist[i,j]* A[i2,j]\n",
    "            newPay/= mu_i\n",
    "            if newPay> oldPay+ epsilon:\n",
    "                print(f\"[A violates CE]: i={i}, i'={i2}, diff={newPay-oldPay:.3f}\")\n",
    "                return False\n",
    "    # B side\n",
    "    for j in range(n):\n",
    "        mu_j= np.sum(jointDist[:, j])\n",
    "        if mu_j<1e-15:\n",
    "            continue\n",
    "        oldPay=0.0\n",
    "        for i in range(n):\n",
    "            oldPay+= jointDist[i,j]* B[i,j]\n",
    "        oldPay/= mu_j\n",
    "        for j2 in range(n):\n",
    "            if j2== j:\n",
    "                continue\n",
    "            newPay=0.0\n",
    "            for i in range(n):\n",
    "                newPay+= jointDist[i,j]* B[i,j2]\n",
    "            newPay/= mu_j\n",
    "            if newPay> oldPay+ epsilon:\n",
    "                print(f\"[B violates CE]: j={j}, j'={j2}, diff={newPay-oldPay:.3f}\")\n",
    "                return False\n",
    "    print(\"=> jointDist satisfies approximate CE.\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    random.seed(0)\n",
    "    S,N=5,3\n",
    "    actions= get_all_strategies(S,N)  # 21\n",
    "    n= len(actions)\n",
    "    print(f\"Use Alpha-Beta each turn, with depth=2, repeated for some rounds. S={S},N={N}, #actions={n}.\")\n",
    "\n",
    "    # For demonstration: each round A does alphaBetaA(...), B does alphaBetaB(...)\n",
    "    # We'll store final (iA,iB) => building jointDist\n",
    "    num_rounds= 50\n",
    "    maxDepth= 2  # or 4\n",
    "\n",
    "    # 统计 (iA, iB) 次数\n",
    "    jointCount= np.zeros((n,n))\n",
    "\n",
    "    for round_i in range(1,num_rounds+1):\n",
    "        # 1) A uses alphaBeta to pick iA\n",
    "        # rootState= [] => depth=0 => alpha= -inf, beta= +inf => we get (value, bestAct)\n",
    "        valA, bestA= alphaBetaA([], 0, maxDepth, -math.inf, math.inf, actions)\n",
    "        if bestA is None:\n",
    "            # fallback\n",
    "            bestA= random.randint(0,n-1)\n",
    "        # 2) then B => we call alphaBetaB\n",
    "        valB, bestB= alphaBetaB([bestA], 1, maxDepth, -math.inf, math.inf, actions)\n",
    "        if bestB is None:\n",
    "            bestB= random.randint(0,n-1)\n",
    "        # record\n",
    "        jointCount[bestA, bestB]+=1\n",
    "\n",
    "    # 经验分布\n",
    "    jointDist= jointCount/ float(num_rounds)\n",
    "    # 打印频率> 0.05\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            p= jointDist[i,j]\n",
    "            if p> 0.05:\n",
    "                print(f\"(i={i}, j={j}), prob={p:.3f}, A= {actions[i]}, B={actions[j]}\")\n",
    "\n",
    "    # 构造 payoff 矩阵\n",
    "    A_mat,B_mat= build_payoff_matrices(actions)\n",
    "    # 检查CE\n",
    "    _= check_CE(A_mat, B_mat, jointDist, epsilon=1e-3)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a784bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Alpha-Beta each turn. S=5,N=3, #actions=21.\n",
      "Round 2: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 4: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 6: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 8: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 10: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 12: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 14: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 16: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 18: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Round 20: A picked i=0, strategy=(0, 0, 5), B picked j=0, strategy=(0, 0, 5)\n",
      "   => alphaBeta final values= (A side)=-1.000, (B side)=-1.000\n",
      "   => Real payoff: A= 1.5, B= 1.5\n",
      "\n",
      "Final jointDist>0.05:\n",
      "  (i=0, j=0), prob=1.000, A= (0, 0, 5), B= (0, 0, 5)\n",
      "\n",
      "Check CE:\n",
      "[A violates CE] i=0 -> i'=7, diff=0.500\n"
     ]
    }
   ],
   "source": [
    "#剪枝算法轮流做决策\n",
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "# =========== Enumerate Colonel Blotto pure strategies =============\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    列举所有把 S 个兵力分配到 N=3 个战场的纯策略 (x1,x2,x3), x1+x2+x3=S.\n",
    "    对 (S=5,N=3) => 21 种.\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    def backtrack(cur, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(cur+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            backtrack(cur+[i], left-i, slots-1)\n",
    "    backtrack([], S, N)\n",
    "    return results\n",
    "\n",
    "# =========== payoff for Blotto =============\n",
    "def payoff_blotto(stratA, stratB):\n",
    "    \"\"\"\n",
    "    - 若 A_i > B_i => A +=1\n",
    "    - 若 A_i < B_i => B +=1\n",
    "    - 若 A_i == B_i => A +=0.5, B +=0.5\n",
    "    返回 (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB= 0.0, 0.0\n",
    "    for a,b in zip(stratA, stratB):\n",
    "        if a>b:\n",
    "            scoreA+=1\n",
    "        elif a<b:\n",
    "            scoreB+=1\n",
    "        else:\n",
    "            scoreA+=0.5\n",
    "            scoreB+=0.5\n",
    "    return (scoreA, scoreB)\n",
    "\n",
    "# =========== Evaluate final state =============\n",
    "def evaluate_state(moves, actions):\n",
    "    \"\"\"\n",
    "    对一个深度=2 的小博弈树： moves=[iA, iB] => 只看最后 (iA,iB).\n",
    "    对深度=4: moves=[iA, iB, iA2, iB2], 也可只看最后一对 iA2,iB2 或把2次对决累加\n",
    "    这里示例取最后 (iA, iB).\n",
    "    返回 row-player(A)的价值(=scoreA - scoreB) => zero-sum perspective.\n",
    "    \"\"\"\n",
    "    if len(moves)<2:\n",
    "        return 0.0\n",
    "    iA= moves[-2]\n",
    "    iB= moves[-1]\n",
    "    scA, scB= payoff_blotto(actions[iA], actions[iB])\n",
    "    return scA- scB  # zero-sum perspective\n",
    "\n",
    "# =========== AlphaBeta for A (max) =============\n",
    "def alphaBetaA(moves, depth, maxDepth, alpha, beta, actions):\n",
    "    \"\"\"\n",
    "    A要最大化( A的score - B的score ).\n",
    "    moves: 已经下过的动作列表, e.g. [] => depth=0 => A走\n",
    "    depth: 当前在树的深度\n",
    "    maxDepth: 最大搜索深度\n",
    "    alpha,beta: 剪枝参数\n",
    "    actions: 21个纯策略\n",
    "    返回 (bestVal, bestAction)\n",
    "    \"\"\"\n",
    "    if depth>= maxDepth:\n",
    "        val= evaluate_state(moves, actions)\n",
    "        return (val, None)\n",
    "    # 遍历 A 的全部动作 (0..20)\n",
    "    bestVal= -math.inf\n",
    "    bestAct= None\n",
    "    for iA in range(len(actions)):\n",
    "        newMoves= moves + [iA]\n",
    "        # 轮到 B => alphaBetaB\n",
    "        valB, _= alphaBetaB(newMoves, depth+1, maxDepth, alpha, beta, actions)\n",
    "        if valB> bestVal:\n",
    "            bestVal= valB\n",
    "            bestAct= iA\n",
    "        alpha= max(alpha, bestVal)\n",
    "        if beta<= alpha:\n",
    "            break\n",
    "    return (bestVal, bestAct)\n",
    "\n",
    "# =========== AlphaBeta for B (min) =============\n",
    "def alphaBetaB(moves, depth, maxDepth, alpha, beta, actions):\n",
    "    \"\"\"\n",
    "    B要最小化( A的score - B的score ) => (row player's payoff).\n",
    "    也可理解为 B在最大化(scoreB - scoreA).\n",
    "    返回 (bestVal, bestAction).\n",
    "    \"\"\"\n",
    "    if depth>= maxDepth:\n",
    "        val= evaluate_state(moves, actions)\n",
    "        return (val, None)\n",
    "    bestVal= math.inf\n",
    "    bestAct= None\n",
    "    for iB in range(len(actions)):\n",
    "        newMoves= moves + [iB]\n",
    "        # 下一层 => A 走\n",
    "        valA, _= alphaBetaA(newMoves, depth+1, maxDepth, alpha, beta, actions)\n",
    "        if valA< bestVal:\n",
    "            bestVal= valA\n",
    "            bestAct= iB\n",
    "        beta= min(beta, bestVal)\n",
    "        if beta<= alpha:\n",
    "            break\n",
    "    return (bestVal, bestAct)\n",
    "\n",
    "\n",
    "# =========== Build payoff matrices to check CE =============\n",
    "def build_payoff_matrices(actions):\n",
    "    \"\"\"\n",
    "    A[i][j]= (scoreA) if A用 i, B用 j\n",
    "    B[i][j]= (scoreB) \n",
    "    \"\"\"\n",
    "    n= len(actions)\n",
    "    import numpy as np\n",
    "    A= np.zeros((n,n))\n",
    "    B= np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            scA, scB= payoff_blotto(actions[i], actions[j])\n",
    "            A[i,j]= scA\n",
    "            B[i,j]= scB\n",
    "    return A,B\n",
    "\n",
    "def check_CE(A,B, jointDist, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    检测 correlated equilibrium (CE):\n",
    "    对 A: condition on i => no beneficial dev i-> i'\n",
    "    对 B: condition on j => no beneficial dev j-> j'\n",
    "    jointDist shape= n x n, sum=1.\n",
    "    \"\"\"\n",
    "    n= len(A)\n",
    "    total= jointDist.sum()\n",
    "    if abs(total-1.0)>1e-5:\n",
    "        print(f\"[Warning] jointDist sum={total:.3f} !=1.0?\")\n",
    "\n",
    "    # A check\n",
    "    for i in range(n):\n",
    "        mu_i= jointDist[i,:].sum()\n",
    "        if mu_i<1e-15:\n",
    "            continue\n",
    "        oldPay=0.0\n",
    "        for j in range(n):\n",
    "            oldPay+= jointDist[i,j]* A[i,j]\n",
    "        oldPay/= mu_i\n",
    "        for i2 in range(n):\n",
    "            if i2== i:\n",
    "                continue\n",
    "            newPay=0.0\n",
    "            for j in range(n):\n",
    "                newPay+= jointDist[i,j]* A[i2,j]\n",
    "            newPay/= mu_i\n",
    "            if newPay> oldPay+ epsilon:\n",
    "                print(f\"[A violates CE] i={i} -> i'={i2}, diff={newPay-oldPay:.3f}\")\n",
    "                return False\n",
    "    # B check\n",
    "    for j in range(n):\n",
    "        mu_j= jointDist[:,j].sum()\n",
    "        if mu_j<1e-15:\n",
    "            continue\n",
    "        oldPay=0.0\n",
    "        for i in range(n):\n",
    "            oldPay+= jointDist[i,j]* B[i,j]\n",
    "        oldPay/= mu_j\n",
    "        for j2 in range(n):\n",
    "            if j2== j:\n",
    "                continue\n",
    "            newPay=0.0\n",
    "            for i in range(n):\n",
    "                newPay+= jointDist[i,j]* B[i,j2]\n",
    "            newPay/= mu_j\n",
    "            if newPay> oldPay+ epsilon:\n",
    "                print(f\"[B violates CE] j={j} -> j'={j2}, diff={newPay-oldPay:.3f}\")\n",
    "                return False\n",
    "    print(\"=> jointDist is approx CE.\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    random.seed()\n",
    "    S,N= 5,3\n",
    "    actions= get_all_strategies(S,N)  # 21\n",
    "    n= len(actions)\n",
    "    print(f\"Use Alpha-Beta each turn. S={S},N={N}, #actions={n}.\")\n",
    "\n",
    "    # 我们设定 searchDepth=2 => (A->B)\n",
    "    searchDepth=4\n",
    "    # 进行多轮对局:\n",
    "    num_rounds= 20\n",
    "\n",
    "    jointCount= np.zeros((n,n))\n",
    "\n",
    "    for rd in range(1, num_rounds+1):\n",
    "        # 1) A 先搜索\n",
    "        valA, bestA= alphaBetaA([], 0, searchDepth, -math.inf, math.inf, actions)\n",
    "        if bestA is None:\n",
    "            bestA= random.randint(0,n-1)\n",
    "        # 2) B再搜索\n",
    "        valB, bestB= alphaBetaB([bestA], 1, searchDepth, -math.inf, math.inf, actions)\n",
    "        if bestB is None:\n",
    "            bestB= random.randint(0,n-1)\n",
    "\n",
    "        # 记录 (bestA, bestB)\n",
    "        jointCount[bestA, bestB]+=1\n",
    "\n",
    "        if(rd%2==0):\n",
    "            # 打印: A/B 做了什么\n",
    "            print(f\"Round {rd}: A picked i={bestA}, strategy={actions[bestA]}, \"\n",
    "                  f\"B picked j={bestB}, strategy={actions[bestB]}\")\n",
    "            print(f\"   => alphaBeta final values= (A side)={valA:.3f}, (B side)={valB:.3f}\")\n",
    "            scA, scB= payoff_blotto(actions[bestA], actions[bestB])\n",
    "            print(f\"   => Real payoff: A= {scA}, B= {scB}\\n\")\n",
    "\n",
    "    # 经验分布\n",
    "    jointDist= jointCount/ float(num_rounds)\n",
    "    print(\"Final jointDist>0.05:\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            p= jointDist[i,j]\n",
    "            if p>0.05:\n",
    "                print(f\"  (i={i}, j={j}), prob={p:.3f}, A= {actions[i]}, B= {actions[j]}\")\n",
    "\n",
    "    # 构造 payoff 矩阵, check CE\n",
    "    A_mat,B_mat= build_payoff_matrices(actions)\n",
    "    print(\"\\nCheck CE:\")\n",
    "    _= check_CE(A_mat, B_mat, jointDist, epsilon=1e-2)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dbc9583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use AB each turn with random factor. S=5,N=3, #actions=21\n",
      "Round 10000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 20000: A => 10=(1, 4, 0), B => 12=(2, 1, 2)\n",
      "  alphaBeta valA= 1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 30000: A => 0=(0, 0, 5), B => 5=(0, 5, 0)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.5, B=1.5)\n",
      "\n",
      "Round 40000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 50000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 60000: A => 11=(2, 0, 3), B => 1=(0, 1, 4)\n",
      "  alphaBeta valA= 0.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 70000: A => 2=(0, 2, 3), B => 8=(1, 2, 2)\n",
      "  alphaBeta valA= 0.000, valB= 0.000, real payoff= (A=1.5, B=1.5)\n",
      "\n",
      "Round 80000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 90000: A => 13=(2, 2, 1), B => 3=(0, 3, 2)\n",
      "  alphaBeta valA= 1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 100000: A => 4=(0, 4, 1), B => 6=(1, 0, 4)\n",
      "  alphaBeta valA= 0.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 110000: A => 8=(1, 2, 2), B => 11=(2, 0, 3)\n",
      "  alphaBeta valA= 1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 120000: A => 16=(3, 1, 1), B => 2=(0, 2, 3)\n",
      "  alphaBeta valA= 1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 130000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 140000: A => 18=(4, 0, 1), B => 1=(0, 1, 4)\n",
      "  alphaBeta valA= 0.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 150000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= 0.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 160000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 170000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 180000: A => 0=(0, 0, 5), B => 7=(1, 1, 3)\n",
      "  alphaBeta valA= -1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 190000: A => 6=(1, 0, 4), B => 12=(2, 1, 2)\n",
      "  alphaBeta valA= 1.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Round 200000: A => 4=(0, 4, 1), B => 6=(1, 0, 4)\n",
      "  alphaBeta valA= 0.000, valB= -1.000, real payoff= (A=1.0, B=2.0)\n",
      "\n",
      "Final jointDist>0.01:\n",
      "  (i=0, j=0), prob=0.011, A= (0, 0, 5), B= (0, 0, 5)\n",
      "  (i=0, j=5), prob=0.010, A= (0, 0, 5), B= (0, 5, 0)\n",
      "  (i=0, j=7), prob=0.360, A= (0, 0, 5), B= (1, 1, 3)\n",
      "  (i=0, j=8), prob=0.016, A= (0, 0, 5), B= (1, 2, 2)\n",
      "  (i=1, j=8), prob=0.025, A= (0, 1, 4), B= (1, 2, 2)\n",
      "  (i=2, j=6), prob=0.029, A= (0, 2, 3), B= (1, 0, 4)\n",
      "  (i=3, j=6), prob=0.029, A= (0, 3, 2), B= (1, 0, 4)\n",
      "  (i=4, j=6), prob=0.024, A= (0, 4, 1), B= (1, 0, 4)\n",
      "  (i=5, j=6), prob=0.016, A= (0, 5, 0), B= (1, 0, 4)\n",
      "  (i=6, j=12), prob=0.022, A= (1, 0, 4), B= (2, 1, 2)\n",
      "  (i=7, j=13), prob=0.026, A= (1, 1, 3), B= (2, 2, 1)\n",
      "  (i=8, j=11), prob=0.028, A= (1, 2, 2), B= (2, 0, 3)\n",
      "  (i=9, j=11), prob=0.025, A= (1, 3, 1), B= (2, 0, 3)\n",
      "  (i=10, j=11), prob=0.020, A= (1, 4, 0), B= (2, 0, 3)\n",
      "  (i=11, j=1), prob=0.026, A= (2, 0, 3), B= (0, 1, 4)\n",
      "  (i=12, j=2), prob=0.029, A= (2, 1, 2), B= (0, 2, 3)\n",
      "  (i=13, j=3), prob=0.028, A= (2, 2, 1), B= (0, 3, 2)\n",
      "  (i=14, j=4), prob=0.024, A= (2, 3, 0), B= (0, 4, 1)\n",
      "  (i=15, j=1), prob=0.024, A= (3, 0, 2), B= (0, 1, 4)\n",
      "  (i=16, j=2), prob=0.025, A= (3, 1, 1), B= (0, 2, 3)\n",
      "  (i=17, j=3), prob=0.022, A= (3, 2, 0), B= (0, 3, 2)\n",
      "  (i=18, j=1), prob=0.020, A= (4, 0, 1), B= (0, 1, 4)\n",
      "  (i=19, j=2), prob=0.019, A= (4, 1, 0), B= (0, 2, 3)\n",
      "  (i=20, j=1), prob=0.012, A= (5, 0, 0), B= (0, 1, 4)\n",
      "\n",
      "Check CE:\n",
      "[A violates CE] i=0 -> i'=1, diff=0.417\n"
     ]
    }
   ],
   "source": [
    "#使用随机决策的剪枝算法\n",
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    返回所有把 S 个兵力分配到 N=3 个战场的纯策略 (x1,x2,x3)。\n",
    "    对 (S=5,N=3) => 21 种。\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    def backtrack(cur, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(cur+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            backtrack(cur+[i], left-i, slots-1)\n",
    "    backtrack([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_blotto(stratA, stratB):\n",
    "    \"\"\"\n",
    "    Colonel Blotto计分:\n",
    "      if A_i> B_i => A++,\n",
    "      if A_i< B_i => B++,\n",
    "      else => each++0.5\n",
    "    返回 (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scA, scB= 0.0,0.0\n",
    "    for a,b in zip(stratA, stratB):\n",
    "        if a>b:\n",
    "            scA+=1\n",
    "        elif a<b:\n",
    "            scB+=1\n",
    "        else:\n",
    "            scA+=0.5\n",
    "            scB+=0.5\n",
    "    return scA, scB\n",
    "\n",
    "def evaluate_state(moves, actions):\n",
    "    \"\"\"\n",
    "    对 depth=2 (即A->B)的场景: moves=[iA, iB].\n",
    "    只看最后 (iA, iB) 做 payoff => A视角 => scA - scB.\n",
    "    \"\"\"\n",
    "    if len(moves)<2:\n",
    "        return 0.0\n",
    "    iA= moves[-2]\n",
    "    iB= moves[-1]\n",
    "    scA, scB= payoff_blotto(actions[iA], actions[iB])\n",
    "    return scA - scB  # zero-sum\n",
    "\n",
    "def alphaBetaA(moves, depth, maxDepth, alpha, beta, actions,\n",
    "               eps=0.1, feasibleA=None):\n",
    "    \"\"\"\n",
    "    A是max层: A要最大化(A-B).\n",
    "    如果随机概率 eps 发生，则在 feasibleA 里随机选，否则做 alpha-beta。\n",
    "    - 若 feasibleA=None => 默认 all actions= range(n).\n",
    "    返回 (bestVal, bestAction).\n",
    "    \"\"\"\n",
    "    n= len(actions)\n",
    "    if feasibleA is None:\n",
    "        feasibleA= range(n)\n",
    "\n",
    "    # 先判定：若随机探索\n",
    "    if random.random()< eps:\n",
    "        iA= random.choice(list(feasibleA))\n",
    "        newMoves= moves+[ iA ]\n",
    "        if depth+1>= maxDepth:\n",
    "            val= evaluate_state(newMoves, actions)\n",
    "            return (val, iA)\n",
    "        # 递归 => B\n",
    "        valB, _= alphaBetaB(newMoves, depth+1, maxDepth, alpha, beta, actions,\n",
    "                            eps=eps, feasibleB=None)\n",
    "        return (valB, iA)\n",
    "\n",
    "    # 否则 => alpha-beta\n",
    "    if depth>= maxDepth:\n",
    "        val= evaluate_state(moves, actions)\n",
    "        return (val, None)\n",
    "\n",
    "    bestVal= -math.inf\n",
    "    bestAct= None\n",
    "    for iA in feasibleA:\n",
    "        newMoves= moves+[ iA ]\n",
    "        valB, _= alphaBetaB(newMoves, depth+1, maxDepth, alpha, beta, actions,\n",
    "                            eps=eps, feasibleB=None)\n",
    "        if valB> bestVal:\n",
    "            bestVal= valB\n",
    "            bestAct= iA\n",
    "        alpha= max(alpha, bestVal)\n",
    "        if beta<= alpha:\n",
    "            break\n",
    "    return (bestVal, bestAct)\n",
    "\n",
    "def alphaBetaB(moves, depth, maxDepth, alpha, beta, actions,\n",
    "               eps=0.1, feasibleB=None):\n",
    "    \"\"\"\n",
    "    B是min层: B要最小化(A-B).\n",
    "    同样可能随机: 在 feasibleB 中随机选动作。\n",
    "    若 feasibleB=None => range(n).\n",
    "    \"\"\"\n",
    "    n= len(actions)\n",
    "    if feasibleB is None:\n",
    "        feasibleB= range(n)\n",
    "\n",
    "    # 随机\n",
    "    if random.random()< eps:\n",
    "        iB= random.choice(list(feasibleB))\n",
    "        newMoves= moves+[ iB ]\n",
    "        if depth+1>= maxDepth:\n",
    "            val= evaluate_state(newMoves, actions)\n",
    "            return (val, iB)\n",
    "        valA, _= alphaBetaA(newMoves, depth+1, maxDepth, alpha, beta, actions,\n",
    "                            eps=eps, feasibleA=None)\n",
    "        return (valA, iB)\n",
    "\n",
    "    # alpha-beta\n",
    "    if depth>= maxDepth:\n",
    "        val= evaluate_state(moves, actions)\n",
    "        return (val, None)\n",
    "\n",
    "    bestVal= math.inf\n",
    "    bestAct= None\n",
    "    for iB in feasibleB:\n",
    "        newMoves= moves+[ iB ]\n",
    "        valA, _= alphaBetaA(newMoves, depth+1, maxDepth, alpha, beta, actions,\n",
    "                            eps=eps, feasibleA=None)\n",
    "        if valA< bestVal:\n",
    "            bestVal= valA\n",
    "            bestAct= iB\n",
    "        beta= min(beta, bestVal)\n",
    "        if beta<= alpha:\n",
    "            break\n",
    "    return (bestVal, bestAct)\n",
    "\n",
    "def build_payoff_matrices(actions):\n",
    "    \"\"\"\n",
    "    A[i][j]= scoreA if A uses i, B uses j\n",
    "    B[i][j]= scoreB\n",
    "    \"\"\"\n",
    "    n= len(actions)\n",
    "    A= np.zeros((n,n))\n",
    "    B= np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            scA, scB= payoff_blotto(actions[i], actions[j])\n",
    "            A[i,j]= scA\n",
    "            B[i,j]= scB\n",
    "    return A,B\n",
    "\n",
    "def check_CE(A,B, jointDist, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    相关均衡检验 (CE).\n",
    "    \"\"\"\n",
    "    n= len(A)\n",
    "    total= jointDist.sum()\n",
    "    if abs(total-1.0)>1e-5:\n",
    "        print(f\"[Warn] jointDist sum={total:.3f} !=1\")\n",
    "    # A's side\n",
    "    for i in range(n):\n",
    "        mu_i= jointDist[i,:].sum()\n",
    "        if mu_i<1e-15:\n",
    "            continue\n",
    "        oldPay=0.0\n",
    "        for j in range(n):\n",
    "            oldPay+= jointDist[i,j]* A[i,j]\n",
    "        oldPay/= mu_i\n",
    "        for i2 in range(n):\n",
    "            if i2==i: continue\n",
    "            newPay=0.0\n",
    "            for j in range(n):\n",
    "                newPay+= jointDist[i,j]* A[i2,j]\n",
    "            newPay/= mu_i\n",
    "            if newPay> oldPay+ epsilon:\n",
    "                print(f\"[A violates CE] i={i} -> i'={i2}, diff={newPay-oldPay:.3f}\")\n",
    "                return False\n",
    "    # B's side\n",
    "    for j in range(n):\n",
    "        mu_j= jointDist[:,j].sum()\n",
    "        if mu_j<1e-15:\n",
    "            continue\n",
    "        oldPay=0.0\n",
    "        for i in range(n):\n",
    "            oldPay+= jointDist[i,j]* B[i,j]\n",
    "        oldPay/= mu_j\n",
    "        for j2 in range(n):\n",
    "            if j2==j: continue\n",
    "            newPay=0.0\n",
    "            for i in range(n):\n",
    "                newPay+= jointDist[i,j]* B[i,j2]\n",
    "            newPay/= mu_j\n",
    "            if newPay> oldPay+ epsilon:\n",
    "                print(f\"[B violates CE] j={j} -> j'={j2}, diff={newPay-oldPay:.3f}\")\n",
    "                return False\n",
    "    print(\"=> jointDist is approx CE.\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    random.seed()\n",
    "    S,N= 5,3\n",
    "    actions= get_all_strategies(S,N)  #21\n",
    "    n= len(actions)\n",
    "    print(f\"Use AB each turn with random factor. S={S},N={N}, #actions={n}\")\n",
    "    maxDepth=2\n",
    "    epsExplore=0.05\n",
    "    rounds= 200000\n",
    "\n",
    "    # 记录 (iA,iB)\n",
    "    jointCount= np.zeros((n,n))\n",
    "\n",
    "    for rd in range(1, rounds+1):\n",
    "        # Step1: A => alphaBetaA (no adjacency used here, but feasibleA=range(n) or something)\n",
    "        valA, bestA= alphaBetaA([], 0, maxDepth, -math.inf, math.inf, actions,\n",
    "                                eps= epsExplore, feasibleA= range(n))\n",
    "        if bestA is None:\n",
    "            bestA= random.randint(0,n-1)\n",
    "\n",
    "        # Step2: B => alphaBetaB\n",
    "        valB, bestB= alphaBetaB([bestA],1,maxDepth, -math.inf, math.inf, actions,\n",
    "                                eps=epsExplore, feasibleB= range(n))\n",
    "        if bestB is None:\n",
    "            bestB= random.randint(0,n-1)\n",
    "\n",
    "        # 记录\n",
    "        jointCount[bestA,bestB]+=1\n",
    "\n",
    "        if(rd%10000==0):\n",
    "            scA, scB= payoff_blotto(actions[bestA], actions[bestB])\n",
    "            print(f\"Round {rd}: A => {bestA}={actions[bestA]}, B => {bestB}={actions[bestB]}\")\n",
    "            print(f\"  alphaBeta valA= {valA:.3f}, valB= {valB:.3f}, real payoff= (A={scA}, B={scB})\\n\")\n",
    "\n",
    "    jointDist= jointCount / rounds\n",
    "    print(\"Final jointDist>0.01:\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            p= jointDist[i,j]\n",
    "            if p>0.01:\n",
    "                print(f\"  (i={i}, j={j}), prob={p:.3f}, A= {actions[i]}, B= {actions[j]}\")\n",
    "\n",
    "    A_mat,B_mat= build_payoff_matrices(actions)\n",
    "    print(\"\\nCheck CE:\")\n",
    "    _= check_CE(A_mat,B_mat, jointDist, epsilon=1e-1)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94a3f7fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pure strategies = 21 (S=5,N=3).\n",
      "=== T=200, after 20 runs, final avg regret A,B= 0.0985,0.0897\n",
      "=== T=1000, after 20 runs, final avg regret A,B= 0.0445,0.0438\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#后悔值的曲线\n",
    "import random\n",
    "import math\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    返回所有将 S 个士兵分配到 N 个战场的纯策略列表: (x1,...,xN), sum(xi)=S, xi>=0\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    def back(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            back(now+[i], left-i, slots-1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_zero_sum(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    零和规则: 若 a_i>b_i => (A+=1,B-=1); a_i<b_i => (A-=1,B+=1); a_i=b_i =>(0,0).\n",
    "    \"\"\"\n",
    "    scoreA, scoreB=0,0\n",
    "    for (a,b) in zip(strategyA, strategyB):\n",
    "        if a>b:\n",
    "            scoreA+=1\n",
    "            scoreB-=1\n",
    "        elif a<b:\n",
    "            scoreA-=1\n",
    "            scoreB+=1\n",
    "    return scoreA,scoreB\n",
    "\n",
    "def regret_matching_update(regretSum, allStrats, chosenIndex, oppStrat):\n",
    "    \"\"\"\n",
    "    外部后悔更新: regretSum[j]+= payoff(j, opp)-payoff(chosenIndex, opp).\n",
    "    \"\"\"\n",
    "    chosenStrat = allStrats[chosenIndex]\n",
    "    scChosen,_= payoff_zero_sum(chosenStrat, oppStrat)\n",
    "    for j, s_j in enumerate(allStrats):\n",
    "        sc_j,_= payoff_zero_sum(s_j, oppStrat)\n",
    "        diff= sc_j-scChosen\n",
    "        regretSum[j]+=diff\n",
    "    return regretSum\n",
    "\n",
    "def get_mixed_strategy(regretSum):\n",
    "    \"\"\"\n",
    "    p[j]=max(0, regret_j)/ sumOfPos\n",
    "    如果全<=0=>均匀\n",
    "    \"\"\"\n",
    "    pos=[max(0,r) for r in regretSum]\n",
    "    s=sum(pos)\n",
    "    n=len(regretSum)\n",
    "    if s>1e-15:\n",
    "        return [p/s for p in pos]\n",
    "    else:\n",
    "        return [1.0/n]*n\n",
    "\n",
    "def sample_from_distribution(dist):\n",
    "    r= random.random()\n",
    "    cum=0.0\n",
    "    for i, p in enumerate(dist):\n",
    "        cum+=p\n",
    "        if r<cum:\n",
    "            return i\n",
    "    return len(dist)-1\n",
    "\n",
    "def single_run(allStrats, T, seed=None):\n",
    "    \"\"\"\n",
    "    对 A,B 都用后悔匹配, 运行 T 步, 返回每个step的 averageRegretA, averageRegretB\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    m=len(allStrats)\n",
    "    # A\n",
    "    regretSumA=[0.0]*m\n",
    "    cumPayoffA_strat=[0.0]*m  # 对每个j,若一直用它,累积收益\n",
    "    cumPayoffA_chosen=0.0     # 实际所选策略累积收益\n",
    "    # B\n",
    "    regretSumB=[0.0]*m\n",
    "    cumPayoffB_strat=[0.0]*m\n",
    "    cumPayoffB_chosen=0.0\n",
    "\n",
    "    # init\n",
    "    pA=[1.0/m]*m\n",
    "    pB=[1.0/m]*m\n",
    "\n",
    "    regretsA=[]\n",
    "    regretsB=[]\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        iA= sample_from_distribution(pA)\n",
    "        iB= sample_from_distribution(pB)\n",
    "\n",
    "        scA, scB= payoff_zero_sum(allStrats[iA], allStrats[iB])\n",
    "        # update cum payoff chosen\n",
    "        cumPayoffA_chosen+=scA\n",
    "        cumPayoffB_chosen+=scB\n",
    "\n",
    "        # update each pure payoff\n",
    "        for j in range(m):\n",
    "            scAj,_= payoff_zero_sum(allStrats[j], allStrats[iB])\n",
    "            cumPayoffA_strat[j]+=scAj\n",
    "        for k in range(m):\n",
    "            _, scBk= payoff_zero_sum(allStrats[iA], allStrats[k])\n",
    "            cumPayoffB_strat[k]+=scBk\n",
    "\n",
    "        # update regrets\n",
    "        regretSumA= regret_matching_update(regretSumA, allStrats, iA, allStrats[iB])\n",
    "        regretSumB= regret_matching_update(regretSumB, allStrats, iB, allStrats[iA])\n",
    "\n",
    "        # get new strategy\n",
    "        pA= get_mixed_strategy(regretSumA)\n",
    "        pB= get_mixed_strategy(regretSumB)\n",
    "\n",
    "        # external regret: max_j(cumPayoffA_strat[j]) - cumPayoffA_chosen\n",
    "        extA= max(cumPayoffA_strat)- cumPayoffA_chosen\n",
    "        if extA<0: \n",
    "            extA=0\n",
    "        avgRegretA= extA/t\n",
    "\n",
    "        extB= max(cumPayoffB_strat)- cumPayoffB_chosen\n",
    "        if extB<0:\n",
    "            extB=0\n",
    "        avgRegretB= extB/t\n",
    "\n",
    "        regretsA.append(avgRegretA)\n",
    "        regretsB.append(avgRegretB)\n",
    "\n",
    "    return regretsA, regretsB\n",
    "\n",
    "def multiple_runs(allStrats, T, runs=20):\n",
    "    \"\"\"\n",
    "    多次独立运行, 取每个 step 的 average(或median) 以减少随机波动.\n",
    "    \"\"\"\n",
    "    all_regA=[]\n",
    "    all_regB=[]\n",
    "    for r in range(runs):\n",
    "        regA, regB= single_run(allStrats, T, seed=1000+r) \n",
    "        all_regA.append(regA)\n",
    "        all_regB.append(regB)\n",
    "\n",
    "    # 统计: 对于每个t in [1..T], 求 regA[t], regB[t] 的平均\n",
    "    avgRegA=[0.0]*T\n",
    "    avgRegB=[0.0]*T\n",
    "    for t in range(T):\n",
    "        valsA=[ all_regA[r][t] for r in range(runs)]\n",
    "        valsB=[ all_regB[r][t] for r in range(runs)]\n",
    "        avgRegA[t]= statistics.mean(valsA)\n",
    "        avgRegB[t]= statistics.mean(valsB)\n",
    "    return avgRegA, avgRegB\n",
    "\n",
    "def main():\n",
    "    N=3\n",
    "    S=5\n",
    "    random.seed()\n",
    "    allStrats= get_all_strategies(S,N)  # 3兵,5场 => C(3+5-1,5-1)=C(7,4)=35\n",
    "    print(f\"Number of pure strategies = {len(allStrats)} (S={S},N={N}).\")\n",
    "\n",
    "    # 分别测试 T=200, T=1000, 甚至更多\n",
    "    # 同时多次重复 runs=20 或更多, 取平均\n",
    "    T1=200\n",
    "    T2=1000\n",
    "    runs=20\n",
    "\n",
    "    avgRegA_T1, avgRegB_T1= multiple_runs(allStrats, T1, runs=runs)\n",
    "    avgRegA_T2, avgRegB_T2= multiple_runs(allStrats, T2, runs=runs)\n",
    "\n",
    "    # 打印最终时刻(平均后悔)\n",
    "    print(f\"=== T={T1}, after {runs} runs, final avg regret A,B= {avgRegA_T1[-1]:.4f},{avgRegB_T1[-1]:.4f}\")\n",
    "    print(f\"=== T={T2}, after {runs} runs, final avg regret A,B= {avgRegA_T2[-1]:.4f},{avgRegB_T2[-1]:.4f}\")\n",
    "\n",
    "    # 也可以做一个log-log图,看看 slope ~ -0.5\n",
    "    # 这里先做简单可视化:\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    # 1) plot T=200\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(1,T1+1), avgRegA_T1, label='A avg regret (T=200)')\n",
    "    plt.plot(range(1,T1+1), avgRegB_T1, label='B avg regret (T=200)')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('average regret')\n",
    "    plt.title('T=200 case')\n",
    "    plt.legend()\n",
    "\n",
    "    # 2) plot T=1000\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(1,T2+1), avgRegA_T2, label='A avg regret (T=1000)')\n",
    "    plt.plot(range(1,T2+1), avgRegB_T2, label='B avg regret (T=1000)')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('average regret')\n",
    "    plt.title('T=1000 case')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "577fbe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pure strategies= 21 for (S=5,N=3).\n",
      "\n",
      "=== Observing average regrets at specified time steps ===\n",
      "\n",
      "  t=2,  avg_regA=0.75000, avg_regB=0.80000\n",
      "  t=4,  avg_regA=0.51250, avg_regB=0.61250\n",
      "  t=8,  avg_regA=0.39375, avg_regB=0.48125\n",
      "  t=16,  avg_regA=0.26250, avg_regB=0.35938\n",
      "  t=32,  avg_regA=0.20625, avg_regB=0.25781\n",
      "  t=64,  avg_regA=0.17578, avg_regB=0.18984\n",
      "  t=128,  avg_regA=0.12227, avg_regB=0.12148\n",
      "  t=256,  avg_regA=0.08105, avg_regB=0.10000\n",
      "  t=512,  avg_regA=0.05752, avg_regB=0.05742\n",
      "  t=1024,  avg_regA=0.03672, avg_regB=0.03872\n",
      "  t=2048,  avg_regA=0.02473, avg_regB=0.02688\n",
      "  t=4096,  avg_regA=0.01793, avg_regB=0.01731\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#展示后悔值验证收敛\n",
    "import random, math, statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_all_strategies(S, N):\n",
    "    \"\"\"\n",
    "    枚举所有将 S 个兵力分配到 N 个战场的纯策略列表。\n",
    "    (x1, x2, ..., xN)，xi >= 0, sum(xi)=S。\n",
    "    对 (S=5, N=3), 大小= C(5+3-1, 3-1)= C(7,2)=21.\n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    def back(now, left, slots):\n",
    "        if slots==1:\n",
    "            results.append(tuple(now+[left]))\n",
    "            return\n",
    "        for i in range(left+1):\n",
    "            back(now+[i], left-i, slots-1)\n",
    "    back([], S, N)\n",
    "    return results\n",
    "\n",
    "def payoff_zero_sum(strategyA, strategyB):\n",
    "    \"\"\"\n",
    "    零和计分:\n",
    "      if a>b => (A+=1,B-=1)\n",
    "      if a<b => (A-=1,B+=1)\n",
    "      if a=b => (0,0)\n",
    "    3个战场加和 => (scoreA, scoreB).\n",
    "    \"\"\"\n",
    "    scoreA=0\n",
    "    scoreB=0\n",
    "    for a,b in zip(strategyA, strategyB):\n",
    "        if a>b:\n",
    "            scoreA+=1\n",
    "            scoreB-=1\n",
    "        elif a<b:\n",
    "            scoreA-=1\n",
    "            scoreB+=1\n",
    "    return scoreA, scoreB\n",
    "\n",
    "def regret_update(regretSum, allStrats, chosenIndex, oppStrat):\n",
    "    \"\"\"\n",
    "    外部后悔: regretSum[j]+= payoff(j,opp)- payoff(chosenIndex, opp)\n",
    "    \"\"\"\n",
    "    chosenStrat= allStrats[chosenIndex]\n",
    "    scChosen,_= payoff_zero_sum(chosenStrat, oppStrat)\n",
    "    for j, s_j in enumerate(allStrats):\n",
    "        sc_j,_= payoff_zero_sum(s_j, oppStrat)\n",
    "        diff= sc_j- scChosen\n",
    "        regretSum[j]+= diff\n",
    "    return regretSum\n",
    "\n",
    "def get_mixed_strategy(regretSum):\n",
    "    \"\"\"\n",
    "    p[j] = max(0, regret_j)/ sum_of_positives\n",
    "    若 sum_of_positives=0 => 均匀\n",
    "    \"\"\"\n",
    "    pos= [max(0.0,r) for r in regretSum]\n",
    "    s= sum(pos)\n",
    "    n= len(regretSum)\n",
    "    if s>1e-15:\n",
    "        return [p/s for p in pos]\n",
    "    else:\n",
    "        return [1.0/n]*n\n",
    "\n",
    "def sample_from_distribution(dist):\n",
    "    r= random.random()\n",
    "    cum=0.0\n",
    "    for i,p in enumerate(dist):\n",
    "        cum+=p\n",
    "        if r<cum:\n",
    "            return i\n",
    "    return len(dist)-1\n",
    "\n",
    "def single_run(allStrats, T, seed=None):\n",
    "    \"\"\"\n",
    "    进行一次完整的后悔匹配对战(A,B都学习), 到 T 步:\n",
    "    返回 regretsA[t], regretsB[t], 其中 regretsA[t] 是时刻 t 的 平均后悔(0-based)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    m= len(allStrats)\n",
    "\n",
    "    random.seed()\n",
    "    # 对 A\n",
    "    regretSumA= [0.0]*m\n",
    "    cumPayoffA_strat= [0.0]*m\n",
    "    cumPayoffA_chosen=0.0\n",
    "\n",
    "    # 对 B\n",
    "    regretSumB= [0.0]*m\n",
    "    cumPayoffB_strat= [0.0]*m\n",
    "    cumPayoffB_chosen=0.0\n",
    "\n",
    "    # 初始化混合策略\n",
    "    pA= [1.0/m]*m\n",
    "    pB= [1.0/m]*m\n",
    "\n",
    "    regretsA= []\n",
    "    regretsB= []\n",
    "\n",
    "    for t in range(1,T+1):\n",
    "        iA= sample_from_distribution(pA)\n",
    "        iB= sample_from_distribution(pB)\n",
    "\n",
    "        scA, scB= payoff_zero_sum(allStrats[iA], allStrats[iB])\n",
    "        # 累积实际收益\n",
    "        cumPayoffA_chosen+= scA\n",
    "        cumPayoffB_chosen+= scB\n",
    "\n",
    "        # 更新 \"若一直用 j\" 收益\n",
    "        for j in range(m):\n",
    "            scAj,_= payoff_zero_sum(allStrats[j], allStrats[iB])\n",
    "            cumPayoffA_strat[j]+= scAj\n",
    "        for k in range(m):\n",
    "            _, scBk= payoff_zero_sum(allStrats[iA], allStrats[k])\n",
    "            cumPayoffB_strat[k]+= scBk\n",
    "\n",
    "        # 后悔更新\n",
    "        regretSumA= regret_update(regretSumA, allStrats, iA, allStrats[iB])\n",
    "        regretSumB= regret_update(regretSumB, allStrats, iB, allStrats[iA])\n",
    "        # 得到新混合策略\n",
    "        pA= get_mixed_strategy(regretSumA)\n",
    "        pB= get_mixed_strategy(regretSumB)\n",
    "\n",
    "        # 计算外部后悔\n",
    "        bestA= max(cumPayoffA_strat)\n",
    "        extA= bestA - cumPayoffA_chosen\n",
    "        if extA<0: extA=0\n",
    "        avgA= extA/t\n",
    "\n",
    "        bestB= max(cumPayoffB_strat)\n",
    "        extB= bestB - cumPayoffB_chosen\n",
    "        if extB<0: extB=0\n",
    "        avgB= extB/t\n",
    "\n",
    "        regretsA.append(avgA)\n",
    "        regretsB.append(avgB)\n",
    "\n",
    "    return regretsA, regretsB\n",
    "\n",
    "def multi_runs_observe(allStrats, T, runs=20, seeds=None):\n",
    "    \"\"\"\n",
    "    多次独立运行, 返回:\n",
    "      avgRegA[t], avgRegB[t], 其中每个 t 的外部后悔取所有 runs 的平均\n",
    "    \"\"\"\n",
    "    if seeds is None:\n",
    "        seeds= [1000+i for i in range(runs)]\n",
    "    all_regA= []\n",
    "    all_regB= []\n",
    "    for r in range(runs):\n",
    "        regA, regB= single_run(allStrats, T, seed=seeds[r])\n",
    "        all_regA.append(regA)\n",
    "        all_regB.append(regB)\n",
    "\n",
    "    # 对每个 t, 取 average\n",
    "    avgRegA= []\n",
    "    avgRegB= []\n",
    "    for t in range(T):\n",
    "        valsA= [all_regA[r][t] for r in range(runs)]\n",
    "        valsB= [all_regB[r][t] for r in range(runs)]\n",
    "        avgRegA.append( statistics.mean(valsA) )\n",
    "        avgRegB.append( statistics.mean(valsB) )\n",
    "    return avgRegA, avgRegB\n",
    "\n",
    "def main():\n",
    "    S, N= 5, 3\n",
    "    allStrats= get_all_strategies(S,N)\n",
    "    print(f\"Number of pure strategies= {len(allStrats)} for (S={S},N={N}).\")\n",
    "\n",
    "    # 要观测在 t=10,100,500,1000,5000,10000 的平均后悔:\n",
    "    # 先把 maxT=10000 全都跑完, 每个 step 记录. 再在指定点输出\n",
    "    Tmax=10000\n",
    "    runs= 20   # 多次重复\n",
    "    avgRegA, avgRegB= multi_runs_observe(allStrats, Tmax, runs=runs)\n",
    "\n",
    "    # 观察点\n",
    "    check_points= [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    print(\"\\n=== Observing average regrets at specified time steps ===\\n\")\n",
    "    for cp in check_points:\n",
    "        rA= avgRegA[cp-1]   # 0-based index\n",
    "        rB= avgRegB[cp-1]\n",
    "        print(f\"  t={cp},  avg_regA={rA:.5f}, avg_regB={rB:.5f}\")\n",
    "    print()\n",
    "\n",
    "    # 如果想画图，也可以一并展示\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(range(1,Tmax+1), avgRegA, label='A average regret')\n",
    "    plt.plot(range(1,Tmax+1), avgRegB, label='B average regret')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('average regret')\n",
    "    plt.title(f'Colonel Blotto (S=5,N=3) - average regrets over {runs} runs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea86c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
